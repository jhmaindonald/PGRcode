[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Read more about Quarto blogs here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2024\n\n\nJohn Maindonald, W John Braun, Jeffrey Andrews\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "A brief summary of what the text covers",
    "section": "",
    "text": "Chapter and appendix headings are:\n\nChapter 1: Learning from data, and tools for the task\nChapter 2: Generalizing from models\nChapter 3: Multiple linear regression\nChapter 4: Exploiting the linear model framework\nChapter 5: Generalized linear models and survival analysis\nChapter 6: Time series models\nChapter 7: Multilevel models, and repeated measures\nChapter 8: Tree-based Classification and Regression\nChapter 9: Multivariate data exploration and discrimination\nAppendix A: The R System – A Brief Overview\n\nChanges from the earlier text include:\n\nChapter 1 gives a broad overview of the questions, approaches, and tools that arise in statistical analysis. Where judged necessary, these are filled out in more detail in later chapters. Notes are included on reproducible reporting using R Markdown, and on project management.\nP-values get much more critical attention than in the earlier text. They are contrasted, in a classical hypothesis testing context, with Bayes Factors, calculated assuming a standard family of ‘uninformative’ priors used in the BayesFactor package that allows use of a numerical approximation. As the calculations do not involve simulation, it is straightforward to make comparisons with \\(p\\)-values for a range of sample sizes, effect sizes, and scale parameters for the prior.\nInformation statistics – primarily AIC, AICc, and BIC – are a further focus. Associated relative preference measures, with a role similar to that of Bayes Factors, are noted.\nThere is extended commentary on the insight that studies where a substantial number of published experimental results have been independently replicated offer on what p-values mean in practice. Selection effects that result from the use of a \\(p\\) &lt;= 0.05 criterion for publication have been a major contributor to effect size estimates that may on average be too large by a factor that may be 2.0 or more. A case is made for the publication in some form of all studies that meet minimum design and execution standards. Stricter experimental design criteria are called for, perhaps designing for \\(p \\leq 0.005\\) rather than the common \\(p \\leq 0.05\\),\nSimulation and resampling approaches get more extended use – as sources of insight, as devices for building intuition, and as mechanisms for obtaining sampling distributions when theoretical results are not available.\nAn important addition is the treatment of gene expression and other contexts where there may be hundreds or thousands of p-values.\nThe discussion on choosing models and checking model fits has been revised and extended.\nThe treatment of Generalized Additive Models has been rewritten and extended. There is new content on quantile regression with automatic choice of smoothing parameter, and on fitting monotonic increasing or decreasing response curves as specific forms of shape constrained additive response.\nThe treatment of models that allow for extra-binomial or extra-Poisson variation has been substantially extended.\nExponential time series (ETS) get greater attention, especially for their use in forecasting. Modeling of seasonal terms now gets attention.\nChanges in the lme4 package for fitting mixed-effects models, and the implementation of the Kenward-Roger approach that is now available in the afex package, have required substantial rewrites. There is a new section on “A mixed model with a betabinomial error.” The calculation of lethal time estimates and confidence intervals (primarily targeted at plant quarantine work) uses the first author’s qra (quantal response analysis) package.\nTree diagrams from tree-based regression have been finessed. There is now more attentio n to the handling of prior probabilities. The discussion introduces issues and ideas that are important for machine learning approaches more generally. The absence of coverage of machine learning methods more generally is an important omission.\nPrincipal component calculations now use the function prcomp(), which uses a singular value decomposition approach and is preferred to princomp(). A new section on “High dimensional data – RNA-Seq gene expression” demonstrates approaches now available for analysing data of this general type.\nA new section treats hierarchical and other forms of clustering.\nThe treatment of causal inference from observational data has been greatly extended, with extensive commentary on relevant R packages, and discussion of examples from the literature. Approaches to matching are a particular focus, with extensive references given. The use of directed acyclic graphs as a mechanism for making clear causal pathway assumptions is noted and references given, but not further discussed.\nThere is some limited attention to the use of multiple imputation to fill in missing values in data where some observations are incomplete, allowing use of those observations in a regression or other further analysis.\nAn appendix gives a brief overview of key features of the R system and notes technical issues that have particular relevance for users of the text.\nIn Chapter 2 and on, code is given only for those figures that are specifically targeted at the methodology under discussion. This site will be used as a first point of reference for R markdown scripts that have all the code from the book, and other supplementary materials.\nThe CRAN (Comprehensive R Archive Network) repository contains, at the time of writing, close to 20,000 packages. Further packages are available on other repositories, with Bioconductor perhaps the most important. Several others are listed upon typing setRepositories() at the command line. The 20,000 contrasts with the around 2,000 packages that were on CRAN prior to 2010 when the third edition of “Data Analysis and Graphics Using R” was in preparation. We have tried to keep up to date with new packages that supplement or extend what was available, but some will undoubtedly have been missed.\n\nReflections, looking back on the text in its published form, appear in “Afterword”."
  },
  {
    "objectID": "RunningCode.html#code-markup",
    "href": "RunningCode.html#code-markup",
    "title": "Use of Code Files",
    "section": "Code markup",
    "text": "Code markup\nThe following shows an approach to placing graphs side by side\n\nz &lt;- seq(-4,4,length=101)\nplot(z, dnorm(z), type=\"l\", ylab=\"Normal density\")\nplot(z, dt(z, df=5), type=\"l\", ylab=\"t-statistic density, 5 df\")"
  },
  {
    "objectID": "RunningCode.html#using-the-hmiscknitrset-function",
    "href": "RunningCode.html#using-the-hmiscknitrset-function",
    "title": "Use of Code Files",
    "section": "Using the Hmisc::knitrSet() function",
    "text": "Using the Hmisc::knitrSet() function\nThis function creates a setup where\n\nSpace around the graph can be adjusted by setting the chunk options bot, left, top, and rt;\nCommonly used base graphics parameters can be supplied as chunk options. These include lwd, mgp, las, tcl, axes, xpd, mfg.\nDefault settings can be supplied for a number of chunk options.\n\nSee ?Hmisc::knitrSet for details. With R markdown files, the setting fig.align=\"default\" is required. Either do not change from the default globally, or accompany fig.show=\"hold\" whenever used with fig.align=\"default\", thus:\n\nHmisc::knitrSet(basename=\"tmp\", lang='markdown', fig.path=\"figs/g\", w=6, h=6)\nz &lt;- seq(-4,4,length=101)\nplot(z, dnorm(z), type=\"l\", ylab=\"Normal density\")\nplot(z, dt(z, df=5), type=\"l\", ylab=\"t-statistic density, 5 df\")\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/UsingCode.R\")\n}"
  },
  {
    "objectID": "xtras.html",
    "href": "xtras.html",
    "title": "xtras",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "xtras.html#quarto",
    "href": "xtras.html#quarto",
    "title": "xtras",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "corrections.html",
    "href": "corrections.html",
    "title": "Corrections",
    "section": "",
    "text": "Page 63, lines -8 to -6\nThe statement “which applies for a wide class of priors . . . with densities that tail off in much the same manner as for the normal” is a mis-characterization. The assumptions on which the inequality depends do not, for commonly used families of priors, hold in general. The ‘bound’ is best treated as giving a useful rough ballpark indication of what to expect when degrees of freedom are ‘small’. As degrees of freedom increase, a much smaller Bayes Factor can be expected.\nIn line -6, “. . . \\(p\\)-value equal to 0” should be “. . . \\(p\\)-value equal to 0.00283”.\n\n\nPage 132, first line in Subsection 2.9.2\n“The statement”. . . applying to a wide class of priors” misses the point. Refer back to the page 63 correction.\n\n\nPage 316, Exercise 6.10\n_datasets_ should of course be datasets.\n\n\nPage 396, Exercise 8.3\nThe second sentence refers to a non-existent Chapter 3 model fit. The following is offered as a replacement for the complete exercise:\n\n8.3. Use qqnorm() to check differences from normality in nsw74psid1::re78. What do you notice? Use tree-based regression to predict re78, and check differences from normality in the distribution of residuals.\nWhat do you notice about the tails of the distribution?\n\n\nUse the function car::powerTransform() with family='bcnPower' to search for a transformation that will bring the distribution of re78 closer to normality. Run summary on the output to get values (suitably rounded values are usually preferred) of lambda and gamma that you can then supply as arguments to car::bcnPower() to obtain transformed values tran78 of re78. Use qqnorm() with the transformed data to compare its distribution with a normal distribution. The distribution should now be much closer to normal, making the choice of splits that maximize the between-groups sum-of-squares sums of squares about the mean a more optimal procedure.\nUse tree-based regression to predict tran78, and check differences from normality in the distribution of residuals. What do you now notice about the tails of the distribution? What are the variable importance ranks i) if the tree is chosen that gives the minimum cross-validated error; ii) if the tree is chosen following the one standard error criterion? In each case, calculate the cross-validated relative error.\nDo a random forest fit to the transformed data, and compare the bootstrap error with the cross-validated error from the rpart fits."
  },
  {
    "objectID": "afterword.html",
    "href": "afterword.html",
    "title": "Afterword",
    "section": "",
    "text": "The writing of a text that has aimed to provide a reasonably well rounded account of modern statistical methodology, albeit with very limited attention to machine learning, has been a huge challenge. Comments now follow on several areas where, more than elsewhere, our text remains a work in progress. A warning is that some technical terms will be used that assume a fair level of prior statistical understanding."
  },
  {
    "objectID": "afterword.html#inference-remains-a-hotly-contested-area.",
    "href": "afterword.html#inference-remains-a-hotly-contested-area.",
    "title": "Afterword",
    "section": "Inference remains a hotly contested area.",
    "text": "Inference remains a hotly contested area.\nWe have used Bayes Factors, calculated assuming the family of ‘uninformative’ priors used in the BayesFactor package, as a way to make a connection from the hypothesis testing framework of frequentist statistics into the Bayesian world. As the calculations use a numerical approximation that avoids the need for the extensive chain of simulations required for the Markov Chain Monte Carlo approach, it is straightforward to make comparisons with \\(p\\)-values for a range of sample sizes, effect sizes, and scale parameter for the prior.\nAs Kahneman1 argues in his book on human judgment and decision making, humans are not good intuitive statisticians. This surely applies as much or more to the choice of Bayesian priors as to the judgments that are required in more classical contexts.\nHow does the Bayes Factor change with changes in the effect size, sample size, and number of model parameters? What is the effect of varying the scale parameter for the prior distribution? What circumstances create a case for centering the prior away from the null? The start that we have made at working with the prior families used in the BayesFactor package to provide graphs that can help answer such questions could usefully be extended much further. What difference does it make if a Cauchy prior is replaced by a normal prior, with roughly matched ranges of scale factors?\nMore attention to Bayesian credible intervals would have been made sense. Arguably, these make better sense than Bayes Factors if the interest is in finding a replacement for \\(p\\)-values and associated confidence intervals.\nInformation statistics – primarily AIC, AICc, and BIC – are a further focus. Associated relative preference measures, with a role similar to that of Bayes Factors, are noted. The BIC relative preference measure can be regarded as arising from the Bayes Factor obtained when a Jeffreys Unit Information prior is used that is centered away from the null.2\nNote especially the Subsection 2.9.2 comparison between Bayes Factors and the BIC statistic, for the one-sample \\(t\\)-test case. At the largest sample sizes (\\(n\\) = 80 and \\(n\\)=160) the Bayes Factor, while always smaller than the BIC ‘relative preference’ statistic, comes close to it in value. Larger sample sizes will be required to obtain a similar rough equivalence when the comparison is between two models that have one or more explanatory variables in common.\nThese various statistics are tools, to be used with appropriate caution, and having regard to what is known about the studies that generated the data."
  },
  {
    "objectID": "afterword.html#what-can-be-learned-from-reproducibilityreplication-studies",
    "href": "afterword.html#what-can-be-learned-from-reproducibilityreplication-studies",
    "title": "Afterword",
    "section": "What can be learned from reproducibility/replication studies?",
    "text": "What can be learned from reproducibility/replication studies?\nThere is extended commentary on the insight that studies where a substantial number of published experimental results have been independently replicated offer on what p-values mean in practice. Effect sizes for the replications have mostly been found to be on average much lower than for the original experiment. A major part of the difference is no doubt caused by selection effects, from publishing mainly or only those results that fall under a \\(p\\) &lt;= 0.05 or similar criterion.\nThere is a strong case for the publication in some form of all studies that effmeet minimum design and execution standards. Stricter experimental design criteria are called for, perhaps designing for \\(p \\leq 0.005\\) rather than the common \\(p \\leq 0.05\\)."
  },
  {
    "objectID": "afterword.html#simulation-has-many-uses",
    "href": "afterword.html#simulation-has-many-uses",
    "title": "Afterword",
    "section": "Simulation has many uses",
    "text": "Simulation has many uses\nThe model that is fitted is just one of the models that might have been fitted. Simulation can be used to repeatedly generate new data from the fitted model, then refitting the model to each set of new data. Overall, the different refits give an indication of how different another model fit, from data generated in the same way as the data presented for analysis, might have been. Do indications of departures from model assumptions for diagnostic plots for the fitted model lie within the range observed in the simulations? What is the extent of variation of \\(p\\)-values or other statistics that are of interest?\nSimulation can provide important insights when experiments are planned. Thus, where two treatments will be compared, it is insightful to simulate results for one or more effect sizes that are of interest. If sample sizes are overly small, statistics from the simulations (e.g., effect sizes, \\(p\\)-values, or other statistics) will show large variation from one simulation to another. There would be merit in requiring reports of results from experimental trials to show plots of relevant statistics that were examined at the study planning stage. Experimenters should have as clear as possible an understanding, before proceeding, of the ability of the experiment to discriminate between treatments. Steps taken to obtain this understanding should be reported."
  },
  {
    "objectID": "afterword.html#the-big-wide-world-of-r",
    "href": "afterword.html#the-big-wide-world-of-r",
    "title": "Afterword",
    "section": "The Big Wide World of R",
    "text": "The Big Wide World of R\nThe CRAN (Comprehensive R Archive Network) repository contains, at the time of writing, close to 20,000 packages. The 20,000 contrasts with the around 2,000 packages that were on CRAN prior to 2010 when the third edition of “Data Analysis and Graphics Using R” was in preparation.\nFurther packages are available on other repositories, with Bioconductor perhaps the most important. Type setRepositories() at the R command line to see the names of several further repositories. We have tried to keep up to date with new packages that supplement or extend what was available in 2010, but some will undoubtedly have been missed."
  },
  {
    "objectID": "afterword.html#footnotes",
    "href": "afterword.html#footnotes",
    "title": "Afterword",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKahneman, Daniel. Thinking, fast and slow. Macmillan, 2011.↩︎\nSee http://www.stat.washington.edu/research/reports/1999/tr347.pdf↩︎"
  },
  {
    "objectID": "figFuns.html",
    "href": "figFuns.html",
    "title": "Code for Selected Figures",
    "section": "",
    "text": "options(rmarkdown.html_vignette.check_title = FALSE)\n## xtras=TRUE    ## Set to TRUE to execute code 'extras'\nxtras &lt;- FALSE\nlibrary(knitr)\n## opts_chunk[['set']](results=\"asis\")\n## opts_chunk[['set']](eval=FALSE)   ## Set to TRUE to execute main part of code\nopts_chunk[['set']](eval=FALSE)\nFigures for which code appears here may in due course be made available for execution as functions."
  },
  {
    "objectID": "figFuns.html#figure-1.20",
    "href": "figFuns.html#figure-1.20",
    "title": "Code for Selected Figures",
    "section": "Figure 1.20",
    "text": "Figure 1.20\n\neff2stat &lt;- function(eff=c(.2,.4,.8,1.2), n=c(10,40), numreps=100,\n                     FUN=function(x,N)pt(sqrt(N)*mean(x)/sd(x), df=N-1, \n                                         lower.tail=FALSE)){\n  simStat &lt;- function(eff=c(.2,.4,.8,1.2), N=10, nrep=100, FUN){\n    num &lt;- N*nrep*length(eff)\n    array(rnorm(num, mean=eff), dim=c(length(eff),nrep,N)) |&gt;\n      apply(2:1, FUN, N=N) \n  }\n  mat &lt;- matrix(nrow=numreps*length(eff),ncol=length(n))\n  for(j in 1:length(n)) mat[,j] &lt;- \n    as.vector(simStat(eff, N=n[j], numreps, FUN=FUN))  ## length(eff)*numep\n  data.frame(effsize=rep(rep(eff, each=numreps), length(n)),\n             N=rep(n, each=numreps*length(eff)), stat=as.vector(mat))\n}\n\n\nset.seed(31)\ndf200 &lt;- eff2stat(eff=c(.2,.4,.8,1.2), n=c(10, 40), numreps=200)\nlabx &lt;- c(0.001,0.01,0.05,0.2,0.4,0.8)\ngph &lt;- bwplot(factor(effsize) ~ I(stat^0.25) | factor(N), data=df200, \n              layout=c(2,1), xlab=\"P-value\", ylab=\"Effect size\", \n              scales=list(x=list(at=labx^0.25, labels =labx)))\nupdate(gph+latticeExtra::layer(panel.abline(v=labx[1:3]^0.25, col='lightgray')),\n       strip=strip.custom(factor.levels=paste0(\"n=\",c(10,40))),\n       par.settings=DAAG::DAAGtheme(color=F, col.points=\"gray50\"))"
  },
  {
    "objectID": "figFuns.html#figure-1.24",
    "href": "figFuns.html#figure-1.24",
    "title": "Code for Selected Figures",
    "section": "Figure 1.24",
    "text": "Figure 1.24\n\nt2bfInterval &lt;- function(t, n=10, rscale=\"medium\", mu=c(-.1,.1)){\n     null0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu,\n                                       rscale=rscale,simple=TRUE)\nalt0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu, rscale=rscale, \n                                 complement=TRUE, simple=TRUE)\nalt0/null0\n}\n\n\npval &lt;- c(0.05,0.01,0.001); nval &lt;- c(4,6,10,20,40,80,160)\nbfDF &lt;- expand.grid(p=pval, n=nval)\npcol &lt;- 1; ncol &lt;- 2; tcol &lt;- 3\nbfDF[,'t'] &lt;- apply(bfDF,1,function(x){qt(x[pcol]/2, df=x[ncol]-1,                                  lower.tail=FALSE)})\nother &lt;- apply(bfDF,1,function(x)\n    c(BayesFactor::ttest.tstat(t=x[tcol], n1=x[ncol], rscale=\"medium\",\n                               simple=TRUE),\n      BayesFactor::ttest.tstat(t=x[tcol], n1=x[ncol], rscale=\"wide\",\n                               simple=TRUE),\n## Now specify a null interval\n    t2bfInterval(t=x[tcol], n=x[ncol], mu=c(-0.1,0.1),rscale=\"medium\"),\n    t2bfInterval(t=x[tcol], n=x[ncol], mu=c(-0.1,0.1),rscale=\"wide\")\n  ))\nbfDF &lt;- setNames(cbind(bfDF, t(other)),\n    c('p','n','t','bf','bfInterval'))\n\nAn alternative way to do the calculations for Exercise 31 in Chapter 1 is:\n\ndoBF &lt;- function(pval=c(0.05,0.01,0.002), nval=c(10,40,160)){\nbfDF &lt;- cbind(expand.grid(p=pval, n=nval),\n              matrix(nrow=nrow(bfDF), ncol=5))\nnames(bfDF)[3:7] &lt;- c(\"t\",\"bf\",\"bfw\",\"bfInterval\",\"bfIntervalw\")\nij=0\nfor(n in nval)for(p in pval){\n  # Here, `nval` (last specified in `expand.grid()`) is placed first \nij &lt;- ij+1\nt &lt;- bfDF[ij,'t'] &lt;- qt(p/2, df=n-1, lower.tail=FALSE)\nbfDF[ij,'bf'] &lt;- t2BF(t, n, mu=0, rscale=\"medium\")\nbfDF[ij,'bfw'] &lt;- t2BF(t, n, mu=0, rscale=\"wide\")\nbfDF[ij,'bfInterval'] &lt;- t2BF(t, n, mu=c(-0.1,0.1), rscale=\"medium\")\nbfDF[ij,'bfIntervalw'] &lt;- t2BF(t, n, mu=c(-0.1,0.1),rscale=\"wide\")\n}\nbfDF\n}\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/figFuns.R\")\n}"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "A Practical Guide . . . – Code and Supplements",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe text builds on “Data Analysis and Graphics Using R” (Maindonald and Braun, CUP, 3rd edn, 2010.)↩︎"
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "bf.html",
    "href": "bf.html",
    "title": "Bayes Factors and Information Statistics",
    "section": "",
    "text": "Bayes Factors give a data analysis perspective that is different to that given by \\(p\\)-values, one that is in most cases closer to what the analyst would like to know. The value obtained depends, inevitably, on the choice of prior. But, what choice of prior makes good sense? Intuition, if it is to help, requires training. How do Bayes Factors with common choices of prior, stack up against the use of the information statistics BIC (Bayes Information Criterion) and AIC (Akaike Information Criterion) for use in model choice? Intuition, if it is to be useful at all, requires training. The discussion that follows may be helpful to this end.\nFor purposes of making a connection to the BIC and AIC, the focus will be on Bayes Factors as returned by functions in the BayesFactor package and, by BPpack::BF(). The BIC, in the large sample context for which it was developed, has a direct connection to a form of Bayes Factor. For the AIC, or the AICc that should replace it for use when the ratio of sample size to number of parameters estimated is less than perhaps 40, a more tenuous connection can be made. See Bayarri et al (2012) for commentary on the rational for the Jeffreys-Zellner-Siow family of priors that is used by BayesFactor functions.\nThe AIC penalty term is designed so that, in the large sample limit, the statistic will select the model with the lowest prediction error. The BIC penalty term is designed, in the large sample limit, to select the correct model. In practical use, this distinction may be somewhat artificial.\n\nAll summary statistics are random variables\n\n\n\n\n\n\n\n\nFigure 1: Boxplots are for 200 simulated \\(p\\)-values for a one-sided - one-sample \\(t\\)-test, for the specified effect sizes eff and - sample sizes n. The \\(p^{0.25}\\) scale on the \\(x\\)-axis is used - to reduce the extreme asymmetry in the distributions.\n\n\n\n\n\nNote first that all these statistics, as well as \\(p\\)-values, are random variables. The randomness is a particular issue when sample sizes are small and/or effect sizes are small. Figure 1 highlights this point. Code is:\n\n\nCode\neff2stat &lt;- function(eff=c(.2,.4,.8,1.2), n=c(40,160), numreps=200,\n                     FUN=function(x,N)pt(sqrt(N)*mean(x)/sd(x), df=N-1,\n                                         lower.tail=FALSE)){\n  simStat &lt;- function(eff=c(.2,.4,.8,1.2), N=10, nrep=200, FUN){\n    num &lt;- N*nrep*length(eff)\n    array(rnorm(num, mean=eff), dim=c(length(eff),nrep,N)) |&gt;\n      apply(2:1, FUN, N=N)\n  }\n  mat &lt;- matrix(nrow=numreps*length(eff),ncol=length(n))\n  for(j in 1:length(n)) mat[,j] &lt;-\n    as.vector(simStat(eff, N=n[j], numreps, FUN=FUN))  ## length(eff)*numep\n  data.frame(effsize=rep(rep(eff, each=numreps), length(n)),\n             N=rep(n, each=numreps*length(eff)), stat=as.vector(mat))\n}\n\n\n\n\nCode\nlibrary(lattice)\nset.seed(31)\nn &lt;- c (40,80)\ndf200 &lt;- eff2stat(eff=c(.2,.4,.8,1.2), n=n, numreps=200)\nlabx &lt;- c(0.001,0.01,0.05,0.2,0.4,0.8)\ngph &lt;- bwplot(factor(effsize) ~ I(stat^0.25) | factor(N), data=df200,\n              layout=c(2,1), xlab=\"P-value\", ylab=\"Effect size\",\n              scales=list(x=list(at=labx^0.25, labels =labx)))\nupdate(gph+latticeExtra::layer(panel.abline(v=labx[1:3]^0.25, col='lightgray')),\n       strip=strip.custom(factor.levels=paste0(\"n=\", n)),\n       par.settings=DAAG::DAAGtheme(color=F, col.points=\"gray50\"))\n\n\n\n\nThe BIC and AIC connection to Bayes Factors\nAs a starting point, comparisons will be for a one-sample \\(t\\)-statistic.\nGiven two models with the same outcome variable, with respective BIC (Bayesian Information Criterion) statistics \\(m_1\\) and \\(m_2\\), the quantity \\[\nb_{12} = exp((m_1-m_2)/2)\n\\] can be used as a relative preference statistic for \\(m_2\\) as against \\(m_1\\). If model 1 is nested in model 2 this becomes, under what is known as a Jeffreys Unit Information (JUI) prior that is centered on the maximum likelihood of the difference under the alternative, a Bayes Factor giving the probability of model 2 relative to model 1. In the case of a one-sample \\(t\\)-statistic, the BIC-derived Bayes Factor is \\[\n\\mbox{exp}(N*\\log(1+\\frac{t^2}{N-1})-\\log(N))/2),\n\\mbox{  where }N \\mbox{ is the sample size}\n\\]\nHow does this compare with Bayes Factors that are obtained with other choices of prior? Specifically, because the calculations can then be handled without Markov Chain Monte Carlo simulation, we look at results from functions in the Bayesfactor and BFpack packages.\n\nComparison with results from BayesFactor::ttestBF()\nFunctions in the BayesFactor package assume a Jeffreys-Zellner-Siow (JSZ) prior, which has a reasonable claim to be used as a default prior. Numerical quadrature is used to calculate the Bayes Factor, avoiding the need for Markov Chain Monte Carlo simulation. A Cauchy prior is assumed for the effect size, with the argument rscale giving the scale factor. The Jeffreys distribution has a similar role for the variance of the normal distributions that are assumed both under the null and under the alternative.\n\n\nCode\n# Functions that calculate Bayes Factors or relative preferences  \nt2BF &lt;- function(p=0.05, N, xrs=1/sqrt(2)){\n  t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n  BayesFactor::ttest.tstat(t=t, n1=N, rscale=xrs, simple=TRUE)}\nt2BFbic &lt;- function(p,N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n  exp((N*log(1+t^2/(N-1))-log(N))/2)}\nt2AIC &lt;- function(p,N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n  exp((N*log(1+t^2/(N-1))-2)/2)}\nt2AICc &lt;- function(p,N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n  exp((N*log(1+t^2/(N-1))-12/(N-3)+4/(N-2)-2)/2)}  ## Requires N &gt; 6\nt2eff &lt;- function(p=0.05, N)\n  eff &lt;- qt(p/2, df=N-1, lower.tail=FALSE)/sqrt(N)\n\n\n\n\nCode\npval &lt;- c(.05,.01,.001); np &lt;- length(pval)\nNval &lt;- c(3,4,5,10,20,40,80,160,360); nlen &lt;- length(Nval)\n## Bayes Factors, using BayesFactor::ttest.tstat()\nrs &lt;- c(1/sqrt(2), sqrt(2))\nbf &lt;- matrix(nrow=length(rs)+2,ncol=length(Nval))\ndimnames(bf) &lt;-\n  list(c('rscale=1/sqrt(2)', '       sqrt(2)', 'BIC','Effect size'),\n       paste0(c(\"n=\",rep(\"\",length(Nval)-1)),Nval))\nbfVal &lt;- setNames(rep(list(bf),length(pval)),\n                  paste0('p', substring(paste(pval),2)))\nfor(k in 1:length(pval)){p &lt;- pval[k]\n  for(i in 1:length(rs))for(j in 1:nlen)\n    bfVal[[k]][i,j] &lt;- t2BF(p=p, N=Nval[j], xrs=rs[i])\n  bfVal[[k]][length(rs)+1,] &lt;- outer(p, Nval, t2BFbic)\n  bfVal[[k]][length(rs)+2,] &lt;- outer(p, Nval, t2eff)\n  }\nlapply(bfVal, function(x)signif(x,2))\n\n\n$p.05\n                  n=3   4   5   10   20   40   80  160  360\nrscale=1/sqrt(2)  2.6 2.4 2.2 1.80 1.40 1.10 0.80 0.59 0.40\n       sqrt(2)    3.0 2.5 2.1 1.30 0.89 0.62 0.43 0.31 0.20\nBIC              19.0 9.6 6.6 3.00 1.80 1.20 0.79 0.55 0.36\nEffect size       2.5 1.6 1.2 0.72 0.47 0.32 0.22 0.16 0.10\n\n$p.01\n                   n=3    4    5   10   20   40  80  160  360\nrscale=1/sqrt(2)   6.4  7.0  7.0  6.2 5.10 4.10 3.1 2.30 1.60\n       sqrt(2)     9.8  9.6  8.7  5.6 3.70 2.50 1.8 1.20 0.82\nBIC              210.0 77.0 45.0 15.0 8.00 5.00 3.3 2.30 1.50\nEffect size        5.7  2.9  2.1  1.0 0.64 0.43 0.3 0.21 0.14\n\n$p.001\n                  n=3      4     5    10    20    40    80   160   360\nrscale=1/sqrt(2)   21   32.0  38.0  41.0 37.00 31.00 24.00 18.00 13.00\n       sqrt(2)     39   56.0  60.0  47.0 31.00 21.00 15.00 10.00  6.70\nBIC              6500 1600.0 750.0 180.0 77.00 44.00 28.00 19.00 12.00\nEffect size        18    6.5   3.9   1.5  0.87  0.56  0.38  0.27  0.17\n\n\nNote two points:\n- Not until \\(n\\)=80 is the BIC-based Bayes Factor in much the same ballpark as the that generated by the BayesFactor function. For smaller values of \\(n\\), it is overly large. The BIC statistic really is designed for use in a “large sample” context.\n- As \\(n\\) increases, the estimated effect size to which the Bayes Factor corresponds becomes ever smaller.\nNote then that BayesFactor::ttestBF() with the default setting of rscale, and the BIC-based Bayes Factor, are both using a prior whose scale is large relative to an ever smaller effect size.\n\n\nMatching the setting of rscale to the effect size\nObserve then the result from matching the scale for the prior to the effect size. The following checks this for \\(p\\)=0.05, making at the same time a comparison with AIC-based and BIC-based relative ‘preferences’.\n\n\nCode\nrs &lt;- c(0.5,1,4,16)\npval &lt;- 0.05\nBFrs &lt;- matrix(nrow=length(rs)+3, ncol=nlen)\ndimnames(BFrs) &lt;-\n  list(c(paste0(c(\"rscale=\",rep(\"       \",3)),rs,\"/sqrt(n)\"),\n         \"rscale=1/sqrt(2)\",\"BIC-based\",\"AIC-based\"), \n       paste0(c(\"n=\",rep(\"\",nlen-1)),Nval))\nfor(j in 1:nlen){\n  for(i in 1:length(rs))\n     BFrs[i,j] &lt;- t2BF(p=pval, N=Nval[j], xrs=rs[i]/sqrt(Nval[j]))\n  BFrs[length(rs)+1, j] &lt;- t2BF(p=pval, N=Nval[j], xrs=1/sqrt(2))\n  BFrs[length(rs)+2, j] &lt;- t2BFbic(p=pval, N=Nval[j])\n  BFrs[length(rs)+3, j] &lt;- t2AIC(p=pval, N=Nval[j])\n}\nprint(setNames(\"p=0.05\",\"\"), quote=F)\n\n\n       \np=0.05 \n\n\nCode\nround(BFrs,2)\n\n\n                     n=3    4    5   10   20   40   80  160  360\nrscale=0.5/sqrt(n)  1.84 1.77 1.71 1.59 1.53 1.50 1.48 1.48 1.47\n       1/sqrt(n)    2.38 2.19 2.06 1.81 1.70 1.65 1.62 1.61 1.60\n       4/sqrt(n)    3.02 2.28 1.92 1.41 1.22 1.15 1.11 1.10 1.09\n       16/sqrt(n)   1.48 0.91 0.70 0.46 0.39 0.36 0.35 0.34 0.34\nrscale=1/sqrt(2)    2.56 2.38 2.22 1.76 1.39 1.07 0.80 0.59 0.40\nBIC-based          18.96 9.57 6.56 3.00 1.78 1.16 0.79 0.55 0.36\nAIC-based          12.08 7.04 5.39 3.49 2.93 2.71 2.60 2.56 2.53\n\n\nThe BIC is designed, in effect, to look for effect sizes that are around one. If a small effect size is expected in a large sample context, use of ttestBF() or ttest.tstat() with a setting of rscale that matches the expected effect size, makes better sense than use of BIC().\nThere is a choice of prior that allows the AIC-based preference measure to be interpreted as a Bayes Factor. See Burnham & Anderson (2004). Relative preference values that are larger than from the BayesFactor functions at all settings of rscale suggests a tendency to choose an overly complex model.\nFor \\(p\\)=0.01 we find:\n\n\nCode\nrs &lt;- c(0.5,1,4,16)\npval &lt;- 0.01\nBFrs &lt;- matrix(nrow=length(rs)+3, ncol=nlen)\ndimnames(BFrs) &lt;-\n  list(c(paste0(c(\"rscale=\",rep(\"       \",3)),rs,\"/sqrt(n)\"),\"rscale=1/sqrt(2)\",\"BIC-based\",\"AIC-based\"), paste0(c(\"n=\",rep(\"\",nlen-1)),Nval))\nfor(j in 1:nlen){\n  for(i in 1:length(rs))\n     BFrs[i,j] &lt;- t2BF(p=pval, N=Nval[j], xrs=rs[i]/sqrt(Nval[j]))\n  BFrs[length(rs)+1, j] &lt;- t2BF(p=pval, N=Nval[j], xrs=1/sqrt(2))\n  BFrs[length(rs)+2, j] &lt;- t2BFbic(p=pval, N=Nval[j])\n  BFrs[length(rs)+3, j] &lt;- t2AIC(p=pval, N=Nval[j])\n}\n\n\n\n\nCode\nprint(setNames(\"p=0.01\",\"\"), quote=F)\n\n\n       \np=0.01 \n\n\nCode\nround(BFrs,2)\n\n\n                      n=3     4     5    10    20    40    80   160   360\nrscale=0.5/sqrt(n)   3.49  3.65  3.62  3.38  3.22  3.13  3.09  3.07  3.06\n       1/sqrt(n)     5.56  5.71  5.54  4.87  4.48  4.28  4.19  4.14  4.12\n       4/sqrt(n)    12.32 10.39  8.77  5.85  4.75  4.30  4.09  3.99  3.94\n       16/sqrt(n)   12.09  6.54  4.51  2.31  1.73  1.51  1.42  1.38  1.35\nrscale=1/sqrt(2)     6.38  7.03  7.05  6.20  5.12  4.08  3.12  2.32  1.60\nBIC-based          205.66 76.53 44.54 15.34  8.04  4.96  3.29  2.25  1.47\nAIC-based          131.05 56.31 36.64 17.84 13.23 11.54 10.81 10.47 10.29\n\n\n\n\n\nAIC and BIC – What \\(P\\)-value Corresponds to a Zero Bifference?\nWe ask “For what \\(p\\)-value does the statistic, in either case, rate the simpler model and the more complex model equally?”\n\n\nCode\neqAIC2p &lt;- function(n)2*pt(sqrt((n-1)*(exp(2/n)-1)),lower.tail=F,df=n-1)\neqBIC2p &lt;- function(n)2*pt(sqrt((n-1)*(exp(log(n)/n)-1)),lower.tail=F,df=n-1)\neqAICc2p &lt;- function(n){penalty &lt;- 12/(n-3)-4/(n-2)+2;\n  2*pt(sqrt((n-1)*(exp(penalty/n)-1)),lower.tail=F,df=n-1)}\neq2p &lt;- rbind(BIC=setNames(eqBIC2p(Nval[-1]), paste(Nval[-1])), \n              AIC=eqAIC2p(Nval[-1]), AICc=eqAICc2p(Nval[-1])) \nsignif(eq2p,2)\n\n\n          4     5    10    20   40    80   160   360\nBIC  0.3500 0.290 0.160 0.096 0.06 0.038 0.025 0.015\nAIC  0.2600 0.230 0.190 0.170 0.17 0.160 0.160 0.160\nAICc 0.0048 0.029 0.098 0.130 0.14 0.150 0.150 0.160\n\n\n\n\nUse of functions from the BFpack package\nWe investigate the Bayes Factors that are calculated using the Fractional Bayes Factor Approach. The details are not easy to describe simply. However the effect is that allowance must be made for the use of a fraction of the information in the data to determine the null. See Mulder et al (2021).\nWe compare\n\nBayes Factor with Jeffreys-Zellner-Siow prior centered on NULL\nFractional Bayes Factor from BFpack (BF.type=1), i.e., the prior is centered on the NULL\nFractional Bayes Factor from BFpack (BF.type=2), i.e., the prior is centered on the maximum likelihood estimate under the alternative.\nAlternative versus NULL, based on Bayesian Information Criterion (BIC)\n\n\n\nCode\nsuppressPackageStartupMessages(library(BayesFactor))\nsuppressPackageStartupMessages(library(BFpack))\nsuppressPackageStartupMessages(library(metRology))\npval &lt;- c(.05,.01,.001); np &lt;- length(pval)\nNval &lt;- c(3:5,10,20,40,80,160,320); nlen &lt;- length(Nval)\nbicVal &lt;- outer(pval, Nval, t2BFbic)\n# t2BF &lt;- function(p, N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n#                       BayesFactor::ttest.tstat(t=t, n1=N, simple=TRUE)}\nBFval &lt;- packValNull &lt;- packValAlt &lt;- matrix(nrow=np,ncol=nlen)\ndimnames(packValNull) &lt;- dimnames(packValAlt) &lt;- dimnames(bicVal) &lt;- \n  dimnames(BFval) &lt;-\n  list(paste(pval), paste0(c(\"n=\",rep(\"\",nlen-1)),Nval))\nfor(i in 1:np)for(j in 1:nlen){\n  t &lt;- qt(pval[i]/2,Nval[j]-1,lower.tail=F)\n  d &lt;- rnorm(Nval[j])\n  d &lt;- d-mean(d)+t*sd(d)/sqrt(Nval[j])\n  tt &lt;- bain::t_test(d)\n  packValNull[i,j] &lt;- BF(tt,  hypothesis='mu=0',  \n    BF.type=1)[['BFmatrix_confirmatory']]['complement', 'mu=0']\n  packValAlt[i,j] &lt;- BF(tt,  hypothesis='mu=0',  \n    BF.type=2)[['BFmatrix_confirmatory']]['complement', 'mu=0']\n  BFval[i,j] &lt;- t2BF(pval[i], Nval[j])}\n\n\n\n\nCode\n## Fractional Bayes factor, center on point null\nprint(setNames(\"Fractional Bayes Factor, center prior on null\",\"\"), quote=F)\n\n\n                                              \nFractional Bayes Factor, center prior on null \n\n\nCode\nprint(packValNull, digits=3)\n\n\n        n=3     4     5    10    20     40     80   160   320\n0.05   2.04  2.19  2.13  1.66  1.20  0.856  0.607  0.43 0.304\n0.01   4.51  6.19  6.71  6.10  4.66  3.395  2.432  1.73 1.227\n0.001 14.24 28.34 36.64 42.93 35.65 26.849 19.520 13.98 9.951\n\n\n\n\nCode\n## Bayes Factor (Cauchy prior, `rscale=\"medium\")`\nprint(setNames(\"From `BayesFactor::ttestBF()`, center prior on null\",\"\"), quote=F)\n\n\n                                                    \nFrom `BayesFactor::ttestBF()`, center prior on null \n\n\nCode\nprint(BFval, digits=3)\n\n\n        n=3     4     5    10    20    40    80    160    320\n0.05   2.56  2.38  2.22  1.76  1.39  1.07  0.80  0.585  0.422\n0.01   6.38  7.03  7.05  6.20  5.12  4.08  3.12  2.321  1.688\n0.001 21.44 32.35 37.65 40.86 36.57 30.53 24.19 18.364 13.527\n\n\n\n\nCode\n## BIC-based to BFpack::BF() ratio\nprint(setNames(\"FBF, center prior on null: Ratio to BayesFactor result\",\"\"), quote=F)\n\n\n                                                       \nFBF, center prior on null: Ratio to BayesFactor result \n\n\nCode\nprint(packValNull/BFval, digits=3)\n\n\n        n=3     4     5    10    20    40    80   160   320\n0.05  0.797 0.919 0.958 0.939 0.864 0.800 0.759 0.734 0.721\n0.01  0.708 0.880 0.952 0.984 0.910 0.833 0.778 0.745 0.727\n0.001 0.664 0.876 0.973 1.050 0.975 0.879 0.807 0.761 0.736\n\n\n\nBFpack::BF() with BF.type=2 vs derived from BIC\n\n\nCode\n# Fractional Bayes factor, center on estimate under alternative\nprint(setNames(\"FBF, center on estimate under alternative\",\"\"), quote=F)\n\n\n                                          \nFBF, center on estimate under alternative \n\n\nCode\nprint(packValAlt, digits=3)\n\n\n         n=3       4      5    10    20     40     80    160    320\n0.05    20.9    9.57   6.22   2.6  1.48  0.946  0.638  0.441  0.308\n0.01   226.8   76.53  42.27  13.3  6.67  4.033  2.647  1.804  1.253\n0.001 7123.0 1606.11 715.79 151.9 63.95 35.565 22.407 14.973 10.295\n\n\n\n\nCode\n## From BIC\nprint(setNames(\"Derived from BIC\",\"\"), quote=F)\n\n\n                 \nDerived from BIC \n\n\nCode\nprint(bicVal, digits=3)\n\n\n       n=3       4      5    10    20    40     80   160    320\n0.05    19    9.57   6.56   3.0  1.78  1.16  0.792  0.55  0.385\n0.01   206   76.53  44.54  15.3  8.04  4.96  3.286  2.25  1.567\n0.001 6460 1606.11 754.24 175.7 77.10 43.73 27.819 18.68 12.872\n\n\n\n\nCode\n## BIC-based to BFpack::BF() ratio\nprint(setNames(\"FBF, center prior on alternative: Ratio to BIC\",\"\"), quote=F)\n\n\n                                               \nFBF, center prior on alternative: Ratio to BIC \n\n\nCode\nprint(packValAlt/bicVal, digits=3)\n\n\n      n=3 4     5    10    20    40    80   160 320\n0.05  1.1 1 0.949 0.865 0.829 0.813 0.805 0.802 0.8\n0.01  1.1 1 0.949 0.865 0.829 0.813 0.805 0.802 0.8\n0.001 1.1 1 0.949 0.865 0.829 0.813 0.805 0.802 0.8\n\n\nThe function BFpack::BF() is making allowance for the use of a fraction of the information in the data used to specify the prior distribution. The BIC based calculations do not make such an adjustment.\nAs for the use of the BIC to choose between a simpler and a more complex model, the calculated Bayes Factors are unreasonably large for small samples, while in larger samples the prior is tuned to detect effect sizes that are of similar (or larger) magnitude than the standard deviation.\nFigure 2 summarizes the comparisons made\n\n\n\n\n\n\n\n\nFigure 2: Results from different ways to calculate the Bayes Factor - for a result from a one-sample two-sided \\(t\\)-test where \\(p\\)=0.05\n\n\n\n\n\nCode is:\n\n\nCode\nlibrary(lattice)\nallVal &lt;- rbind(BFval, packValNull, bicVal, packValAlt)\nrownames(allVal) &lt;- paste0(\n  rep(c('BayesFactor', 'packValNull', 'BIC', 'packValAlt'), c(3,3,3,3)),\n  rep(c(\".05\",\".01\",\".001\"), 4))\ntdf &lt;- as.data.frame(t(allVal))\ntdf$n &lt;- Nval\nlabs &lt;- sort(c(2^(0:6),2^(0:6)*1.5))\nxyplot(BayesFactor.05+packValNull.05+BIC.05+packValAlt.05 ~ n,\n       data=tdf, type='l', auto.key=list(columns=2),\n       xlab=\"Sample size $n$\",\n       ylab=\"Bayes Factor (Choice of 4 possibilities)\",\n       scales=list(x=list(at=(0:8)*40),\n         y=list(log=T, at=labs, labels=paste(labs))),\n par.settings=simpleTheme(lty=c(1,1:3), lwd=2, col=rep(c('gray','black'), c(1,3))))\n\n\n\n\n\nBayes Factors for regression coefficients.\nWe will work with data simulated from a model of the form\n\\[\ny = a_1 x_1 + a_2 x_2 + \\epsilon,\n\\] where \\(\\epsilon\\) is normally distributed with mean 0 and variance \\(\\sigma^2\\).\nThe following function creates simulated data:\n\n\nCode\nsimReg &lt;- function(N=160, b1=1.2, b2=1.25, sd=40, num=20){\n    x1 &lt;- seq(from=1, to=min(num,N), length.out=N)\n    x2 &lt;- sample(x1)\n    df &lt;- data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(min(num,N),sd=sd))\n}\n\n\nIt can suitably be used thus:\n\n\nCode\nset.seed(19)\ndat &lt;- simReg(N = 160, b1 = 1.2, b2 = 1.25, sd = 40, num = 20)\ny.lm &lt;- lm(y ~ x1+x2, data=dat)\n## Check least squares fit\ncoef(summary(y.lm))\n\n\n             Estimate Std. Error    t value    Pr(&gt;|t|)\n(Intercept) -8.553311  9.8363109 -0.8695649 0.385865705\nx1           1.214061  0.6201538  1.9576765 0.052040933\nx2           1.863687  0.6201538  3.0052006 0.003090532\n\n\n\nFit BayesFactor model using lmBF()\nThis function returns the Bayes Factors for individual linear models against the intercept only model as the null. A comparison other than against the intercept only model require one call to lmBF() for each model, then using the ratio of the two Bayes Factors to compare the models. For obtaining multiple Bayes Factors that relate to the one model, the function regressionBF() that is demonstrated below may be preferred.\n\n\nCode\ny.lmBF12 &lt;- lmBF(y ~ x1+x2, data=dat, progress=FALSE)\ny.lmBF1 &lt;- lmBF(y ~ x1, data=dat, progress=FALSE)\ny.lmBF2 &lt;- lmBF(y ~ x2, data=dat, progress=FALSE)\nextractBF(y.lmBF1/y.lmBF12)  \n\n\n           bf        error                     time         code\nx1 0.07403776 3.405148e-05 Wed Jul 24 19:29:21 2024 63417705b50b\n\n\nCode\n  ## `extractBF(y.lmBF1/y.lmBF12)[1,1]` gives just the Bayes Factor.\n  ## `y.lmBF1/y.lmBF12` gives greater detail\n\n\nNote that in terms such as y.lmBF1/y.lmBF12, the ‘full’ model has to appear as the denominator. To obtain the Bayes Factors for y~x1+x2 against y~x1 (i.e. ‘Omit x2’), which is the Bayes Factor for x2 given x1, use the construction:\n\n\nCode\n1/extractBF(y.lmBF1/y.lmBF12)[1,1]  ## Or, `1/(y.lmBF1/y.lmBF12)` \n\n\n[1] 13.50662\n\n\nThe following shows the two Bayes Factors, for x2 given x1, and for x1 given x2, side by side:\n\n\nCode\nsapply(list('x2|x1'=y.lmBF1/y.lmBF12, 'x1|x2'=y.lmBF2/y.lmBF12),\n       function(x)1/extractBF(x)[1,1])\n\n\n    x2|x1     x1|x2 \n13.506622  1.286492 \n\n\n\n\nThe function regressionBF()\nThe function regressionBF() calculates Bayes Factors, either for all model terms (specify whichModels='all') or for all single term deletions (specify whichModels='top'), in either case against the intercept only model as the null. Again use extractBF() to get output in terse summary form:\n\n\nCode\ny.regBF &lt;- regressionBF(y ~ x1+x2, data=dat, progress=FALSE)\nextractBF(y.regBF)\n\n\n               bf        error                     time         code\nx1       0.896133 3.379620e-05 Wed Jul 24 19:29:21 2024 6341680d4ad0\nx2       9.408323 2.142530e-05 Wed Jul 24 19:29:21 2024 634140aad30e\nx1 + x2 12.103729 4.161742e-06 Wed Jul 24 19:29:21 2024  6341f065d4d\n\n\nNow use regressionBF() with the argument whichModels='top'. Use extractBF() to omit the final line that shows 1.0 as the Bayes Factor for the full model against itself:\n\n\nCode\ntop.regBF &lt;- regressionBF(y ~ x1+x2, data=dat, progress=FALSE, \n  whichModels='top')\n  ## Type `top.regBF` to get detailed output\nextractBF(top.regBF)  ## Summary output\n\n\n           bf        error                     time         code\nx1 0.07403776 3.405148e-05 Wed Jul 24 19:29:21 2024  63415aba3a5\nx2 0.77730783 2.182576e-05 Wed Jul 24 19:29:21 2024 63414784a88b\n\n\nIt may seem more natural to work for the inverses of the Bayes Factors that are shown. These can be obtained in either of the following ways:\n\n\nCode\n1/extractBF(top.regBF)[,1]  |&gt; round(2)\n\n\n[1] 14.285714  1.282051\n\n\nreturned, here obtaining the Bayes Factors for x2 given x1, and for x1 given x2, as against the model y~x1+x2. This does however give very large Bayes Factors when \\(p\\)-values are very small. The very small Bayes Factors that are their inverses may be easier to work with.\n\n\nDataset to dataset variation with the one data generation mechanism\nWith the default settings, the simulated data and statistics in the fitted model show, even in medium size datasets such as here with \\(n\\)=80, large variation from one simulation to the next:\nTen simulations give \\(p\\)-values, estimated coefficients (expected values are b1=1.2 and b2=1.5), and Bayes Factors against the model that includes both x1 and x2, thus:\n\n\nCode\nmultsim &lt;- function(N=80, b1=1.25, b2=1.2, sd=4, num=20, nsim=10){\n    x1 &lt;- seq(from=1, to=min(num,N), length.out=N)/5\n    x2 &lt;- sample(x1)\nstats &lt;- matrix(nrow=8, ncol=nsim)\nfor(i in 1:nsim){\n  n &lt;- length(x1)\n  dat &lt;- data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(n,sd=sd))\n  y.lm &lt;- lm(y~x1+x2, data=dat)\n  c1c4 &lt;- coef(summary(y.lm))[2:3, c(1,4)]\n  lm1 &lt;- lm(y~x1, data=dat)\n  lm2 &lt;- lm(y~x2, data=dat)\n  lm12 &lt;- lm(y~x1+x2, data=dat)\nstats[1:6, i] &lt;- c(c1c4[,1], c1c4[,2], \n    1/extractBF(regressionBF(y ~ x1+x2, data=dat, whichModels='top'))$bf[2:1])\n  stats[7:8,i] &lt;- c(exp((BIC(lm2)-BIC(lm12))/2), exp((BIC(lm1)-BIC(lm12))/2))\n}\nrownames(stats) &lt;- c('b1','b2','p1','p2', 'bf1','bf2', 'bf1-BIC','bf2-BIC')\nstats\n}\nset.seed(17)\nstats &lt;- multsim()\nstats |&gt; round(3)\n\n\n          [,1]   [,2]   [,3]   [,4]    [,5]   [,6]    [,7]  [,8]   [,9]   [,10]\nb1       1.321  1.455  1.485  1.614   0.823  0.858   1.819 1.138  1.151   1.685\nb2       0.575  1.096  0.893  1.220   1.593  1.397   0.338 1.103  1.046   0.340\np1       0.001  0.000  0.001  0.001   0.043  0.067   0.000 0.005  0.004   0.000\np2       0.150  0.006  0.043  0.014   0.000  0.003   0.450 0.006  0.009   0.409\nbf1     29.247 92.644 35.210 27.357   1.473  1.162 244.210 9.637 10.350 270.895\nbf2      0.632  7.514  1.541  3.764 181.749 13.297   0.297 7.753  5.506   0.311\nbf1-BIC 24.894 95.848 31.273 23.816   0.955  0.649 285.874 7.356  7.907 321.521\nbf2-BIC  0.330  5.945  0.942  2.608 202.431 10.120   0.151 5.770  3.905   0.160\n\n\nObserve that the BIC-based factors are in most cases substantially less favorable than the Bayes Factors from regressionBF() to the model that has both of the explanatory variables. This remains the case even for a much larger sample size. Thus, try:\n\n\nCode\nstats &lt;- multsim(N=360, sd=8)  # Increase sd to avoid overly small p-values\nstats |&gt; round(3)\n\n\n\n\nRelative preference statistics from AIC and AICc\nNow create a simulated dataset, and calculate Bayes Factors for the coefficients (1) using Bayesfactor functions, and (2) derived from BIC statistics:\n\n\nCode\nsimDat &lt;-\nfunction(x1=rep(1:20,4)/5, x2=sample(rep(1:20,4)/5), \n                   b1=1.2, b2=1.5, sd=8){\n    n &lt;- length(x1)\n    data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(n,sd=sd))\n}\nlibrary(AICcmodavg)\nset.seed(31)\ndat31 &lt;- simDat()\ny0.lm &lt;- lm(y~0, data=dat31)\ny.lm &lt;- lm(y~x1+x2, data=dat31); bf12 &lt;- lmBF(y ~ x1+x2, data=dat31)\ny2.lm &lt;- lm(y~x2, data=dat31); bf2 &lt;- lmBF(y ~ x2, data=dat31)\ny1.lm &lt;- lm(y~x1, data=dat31); bf1 &lt;- lmBF(y ~ x1, data=dat31)\n## Regression summary\ncoef(summary(y.lm)) |&gt; signif(2)\n\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     1.20        2.2    0.55    0.590\nx1              0.99        0.7    1.40    0.160\nx2              1.80        0.7    2.50    0.014\n\n\nCode\n## Bayes Factors for x1 and x2, using functions from _Bayesfactor_\nBF &lt;- c(extractBF(bf12/bf2)$bf, extractBF(bf12/bf1)$bf)\n## Bayes Factors for x1 and x2, derived from BIC statistics\nBICbf &lt;- c(exp((BIC(y2.lm)-BIC(y.lm))/2), exp((BIC(y1.lm)-BIC(y.lm))/2))\n## Bayes Factors for x1 and x2, derived from AIC statistics\nAICrp &lt;- c(exp((AIC(y2.lm)-AIC(y.lm))/2), exp((AIC(y1.lm)-AIC(y.lm))/2))\n## Bayes Factors for x1 and x2, derived from AICc statistics\nAICcRP &lt;- c(exp((AICc(y2.lm)-AICc(y.lm))/2), exp((AICc(y1.lm)-AICc(y.lm))/2)) |&gt; round(2)\n## Bayes Factors for x1 and x2, derived from BIC ratio of ratios to null model\nstats &lt;- cbind(BF=BF, BICbf=BICbf, AICrp=AICrp, AICcRP=AICcRP)\n\n\nRelative preference statistics in favor of including both x1 and x2, are:\n\n\nCode\nstats |&gt; round(2)\n\n\n       BF BICbf AICrp AICcRP\n[1,] 0.70  0.31  1.04   0.93\n[2,] 4.45  2.65  8.73   7.83\n\n\nThe BIC-based statistics are substantially smaller, and the AIC and AICc-based statistics substantially larger, than those derived from calculations using BayesFactor::lmBF().\n\n\nChange in Bayes Factors with varying rscaleCont\nThe following code will be used for the calculations:\n\n\nCode\nsimReg &lt;- function(N=160, b1=1.2, b2=1.25, sd=40, num=20){\n    x1 &lt;- seq(from=1, to=min(num,N), length.out=N)\n    x2 &lt;- sample(x1)\n    df &lt;- data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(min(num,N),sd=sd))\n}\ntuneReg &lt;- function(p=0.01, data){\n    reg &lt;- lm(y ~ x1+x2, data=data)\n    N &lt;- nrow(data)\n    tstats &lt;- coef(summary(reg))[,'t value']\n    tfix &lt;- qt(1-p/2, df=N-3, lower.tail=F)\n    y1 &lt;- fitted(reg)+resid(reg)*tstats[2]/tfix\n    y2 &lt;- fitted(reg)+resid(reg)*tstats[3]/tfix\n    cbind(dat, y1=y1, y2=y2)\n}\n\n\n\n\nCode\nrscaleCF &lt;- function(data=dat, rs=seq(from=0.04, to=0.16, by=0.05)){\n  colnam &lt;- as.character(substitute(rs))\n  if(colnam[1]==\"c\")colnam &lt;- colnam[-1] else colnam &lt;- paste(rs)\n  bfVal &lt;- matrix(nrow=3, ncol=length(colnam))\n  dimnames(bfVal) &lt;- list(\"BFvsInterceptOnly\"=c('x1','x2','x1+x2'), \n                          rscaleCont=colnam)\n  for(i in 1:length(rs)){\n  reg &lt;- regressionBF(y1~x1+x2, data=data, rscaleCont=rs[i])\n  bfVal[,i] &lt;- extractBF(reg)$bf\n  }\nbfVal\n}\n\n\n\n\nCode\nset.seed(53)\ndat &lt;- simReg(N=80, sd=28)\ndat80 &lt;- tuneReg(p=0.01, data=dat)\ncoef(summary(lm(y1~x1+x2, data=dat80)))[2:3,4, drop=F] |&gt; round(3)\nbfVal80a &lt;- rscaleCF(data=dat80, rs=c(.22, .31, sqrt(2)/4))\nbfVal80omit &lt;- apply(bfVal80a, 2, function(x)x[3]/x[-3])\ndimnames(bfVal80omit) &lt;- list('x1+x2 vs'=c('Omit x2','Omit x1'),\n  rscaleCont=dimnames(bfVal80a)[[2]])\nset.seed(53)\ndat &lt;- simReg(N=160, sd=40)\ndat160 &lt;- tuneReg(p=0.01, data=dat)\nbfVal160a&lt;- rscaleCF(data=dat160, rs=c(.26,.154,sqrt(2)/4))\nbfVal160omit &lt;- apply(bfVal160a, 2, function(x)x[3]/x[-3])\ndimnames(bfVal160omit) &lt;- list('x1+x2 vs'=c('Omit x2','Omit x1'),\n  rscaleCont=dimnames(bfVal160a)[[2]])\n\n\nIn the following, the first column shows the value of rscaleCont for the maximum Bayes Factor when x2 is omitted (i.e., only x1 left), and the second column when x1 is omitted.\n\n\nCode\nbfVal80omit |&gt; round(3)\n\n\n         rscaleCont\nx1+x2 vs   0.22  0.31 sqrt(2)/4\n  Omit x2 2.980 2.922     2.857\n  Omit x1 5.516 5.693     5.662\n\n\nFor `dat160’, we see\n\n\nCode\nbfVal160omit |&gt; round(3)\n\n\n         rscaleCont\nx1+x2 vs    0.26  0.154 sqrt(2)/4\n  Omit x2 24.741 22.994    24.242\n  Omit x1  5.136  5.329     4.772\n\n\nNotice that, in both cases, the Bayes Factors for the default setting of rscaleCont lie, for both variables, between that for the ‘Omit x2’ maximum and that for the ‘Omit x1’ maximum.\nIn this context, the smaller Bayes Factor is modestly increased, with the larger Bayes Factor slightly reduced. The full model (y1 ~ x1+x2) and the models obtained by leaving out x2 (x1 is retained) or x1 (x2 is retained), are both affected in much the same way by changes in rscaleConf.\n\n\nDifferent statistics offer different perspectives\nBayes Factors are at best a rough measure of model preference, to be used alongside other measures of model preference. The value obtained depends both on the choice of prior on where the prior is centered, and on the choice of scale for the prior. Different choices can lead to quite different Bayes Factors.\nNote also that when samples are small, different samples from the same population, if available, will give widely varying summary statistics. Refer back to Figure 1, which showed what could be expected for \\(p\\)-values.\nThe comparisons could usefully be extended to consider other choices of prior.\n\n\nNote – Finding rscaleConf value that makes Bayes Factor a maximum\n\n\nReferences\nBayarri, M. J., Berger, J. O., Forte, A., & García-Donato, G. (2012). Criteria for Bayesian model choice with application to variable selection.\nBurnham, K. P., & Anderson, D. R. (2004). Multimodel inference: understanding AIC and BIC in model selection. Sociological methods & research, 33(2), 261-304.\nMulder, J., Williams, D. R., Gu, X., Tomarken, A., Böing-Messing, F., Olsson-Collentine, A., Meijerink-Bosman, M., Menke, J., van Aert, R., Fox, J.-P., Hoijtink, H., Rosseel, Y., Wagenmakers, E.-J., & van Lissa, C. (2021). BFpack: Flexible Bayes Factor Testing of Scientific Theories in R. Journal of Statistical Software, 100(18), 1–63. https://doi.org/10.18637/jss.v100.i18"
  }
]