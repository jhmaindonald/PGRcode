---
title: "Code and Supplements -- A Practical Guide . . ."
---

![](images/frontcover.png) 

### Resources made available here primarily relate to:

**A Practical Guide to Data Analysis Using R -- An Example-Based Approach**,\
by John H Maindonald, W John Braun, and Jeffrey L Andrews[^1]    
published by Cambridge University Press in 2024.

[^1]: The text builds on "Data Analysis and Graphics Using R" (Maindonald and Braun, CUP, 3rd edn, 2010.)

### Scroll down to see [Corrections]

### [Solutions to selected exercises](https://jhmaindonald.github.io/PGRselected)
(These can also be accessed from the menu)

### [PDFs (drafts) for chapters 1, 2, and 3](https://jhmaindonald.github.io/PGRch123)

These are made available with the permission of Cambridge University Press.

# Items of Related interest

### [Updated version of early 2000s booklet on R](../usingR-Booklet/)

This is now to a large extent superseded by tutorials that are available on the web. Beginning R users may nonetheless find it useful as an introductory document for learning about R.

### [What does the data say? -- Traps to Avoid](https://bookdown.org/jhmaindonald/datathoughts/)

This collection of "examples that inform and educate", taken 
from the media and from the published literature, is used to illustrate some of the ready traps to which data analysts can readily fall. The booklet is in the style of what Kahneman (2013), in his book *Thinking Fast and Slow*, calls "educating gossip". Its examples may be used to supplement, or in a few cases offer a further perspective on, those in *A Practical Guide to R*. The source files are all available from

[Source files](../dataThoughts/)

### Corrections

# Corrections
### Page 63, lines -8 to -6

The statement "which applies for a wide class of priors
. . . with densities that tail off in much the same 
manner as for the normal" is a mis-characterization.
The assumptions on which the inequality depends do not,
for commonly used families of priors, hold in general.
The 'bound' is best treated as giving a useful rough 
ballpark indication of what to expect when degrees of 
freedom are 'small'.  As degrees of freedom increase, 
a much smaller Bayes Factor can be expected.

In line -6, ". . . $p$-value equal to 0" should be
". . . $p$-value equal to 0.00283".

### Page 132, first line in Subsection 2.9.2

"The statement ". . . applying to a wide class of priors" 
misses the point. Refer back to the page 63 correction.

### Page 316, Exercise 6.10

\_datasets\_ should of course be _datasets_.

### Page 396, Exercise 8.3

The second sentence refers to a non-existent Chapter 3 model
fit.  The following is offered as a replacement for the
complete exercise:


> 8.3. Use `qqnorm()` to check differences from normality in `nsw74psid1::re78`. What do you notice? Use tree-based regression to predict `re78`, and check differences from normality in the distribution of residuals.\
> What do you notice about the tails of the distribution?

a.  Use the function `car::powerTransform()` with `family='bcnPower'` to search for a transformation that will bring the distribution of `re78` closer to normality. Run summary on the output to get values (suitably rounded values are usually preferred) of `lambda` and `gamma` that you can then supply as arguments to `car::bcnPower()` to obtain transformed values `tran78` of `re78`. Use `qqnorm()` with the transformed data to compare its distribution with a normal distribution. The distribution should now be much closer to normal, making the choice of splits that maximize the between-groups sum-of-squares sums of squares about the mean a more optimal procedure.

b.  Use tree-based regression to predict `tran78`, and check differences from normality in the distribution of residuals. What do you now notice about the tails of the distribution? What are the variable importance ranks i) if the tree is chosen that gives the minimum cross-validated error; ii) if the tree is chosen following the one standard error criterion? In each case, calculate the cross-validated relative error.

c.  Do a random forest fit to the transformed data, and compare the bootstrap error with then cross-validated error from the `rpart` fits.



