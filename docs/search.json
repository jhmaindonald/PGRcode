[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Read more about Quarto blogs here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2024\n\n\nJohn Maindonald, W John Braun, Jeffrey Andrews\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "A Practical Guide – Supplementary Notes",
    "section": "",
    "text": "A brief summary of what the text covers\nChapter and appendix headings are:\n\nChapter 1: Learning from data, and tools for the task\nChapter 2: Generalizing from models\nChapter 3: Multiple linear regression\nChapter 4: Exploiting the linear model framework\nChapter 5: Generalized linear models and survival analysis\nChapter 6: Time series models\nChapter 7: Multilevel models, and repeated measures\nChapter 8: Tree-based Classification and Regression\nChapter 9: Multivariate data exploration and discrimination\nAppendix A: The R System – A Brief Overview\n\nChanges from the earlier text include:\n\nChapter 1 gives a broad overview of the questions, approaches, and tools that arise in statistical analysis. Where judged necessary, these are filled out in more detail in later chapters. Notes are included on reproducible reporting using R Markdown, and on project management.\nP-values get much more critical attention than in the earlier text. They are contrasted, in a classical hypothesis testing context, with Bayes Factors, calculated assuming a standard family of ‘uninformative’ priors used in the BayesFactor package that allows use of a numerical approximation. As the calculations do not involve simulation, it is straightforward to make comparisons with \\(p\\)-values for a range of sample sizes, effect sizes, and scale parameters for the prior.\nInformation statistics – primarily AIC, AICc, and BIC – are a further focus. Associated relative preference measures, with a role similar to that of Bayes Factors, are noted.\nThere is extended commentary on the insight that studies where a substantial number of published experimental results have been independently replicated offer on what p-values mean in practice. Selection effects that result from the use of a \\(p\\) &lt;= 0.05 criterion for publication have been a major contributor to effect size estimates that may on average be too large by a factor that may be 2.0 or more. A case is made for the publication in some form of all studies that meet minimum design and execution standards. Stricter experimental design criteria are called for, perhaps designing for \\(p \\leq 0.005\\) rather than the common \\(p \\leq 0.05\\),\nSimulation and resampling approaches get more extended use – as sources of insight, as devices for building intuition, and as mechanisms for obtaining sampling distributions when theoretical results are not available.\nAn important addition is the treatment of gene expression and other contexts where there may be hundreds or thousands of p-values.\nThe discussion on choosing models and checking model fits has been revised and extended.\nThe treatment of Generalized Additive Models has been rewritten and extended. There is new content on quantile regression with automatic choice of smoothing parameter, and on fitting monotonic increasing or decreasing response curves as specific forms of shape constrained additive response.\nThe treatment of models that allow for extra-binomial or extra-Poisson variation has been substantially extended.\nExponential time series (ETS) get greater attention, especially for their use in forecasting. Modeling of seasonal terms now gets attention.\nChanges in the lme4 package for fitting mixed-effects models, and the implementation of the Kenward-Roger approach that is now available in the afex package, have required substantial rewrites. There is a new section on “A mixed model with a betabinomial error.” The calculation of lethal time estimates and confidence intervals (primarily targeted at plant quarantine work) uses the first author’s qra (quantal response analysis) package.\nTree diagrams from tree-based regression have been finessed. There is now more attentio n to the handling of prior probabilities. The discussion introduces issues and ideas that are important for machine learning approaches more generally. The absence of coverage of machine learning methods more generally is an important omission.\nPrincipal component calculations now use the function prcomp(), which uses a singular value decomposition approach and is preferred to princomp(). A new section on “High dimensional data – RNA-Seq gene expression” demonstrates approaches now available for analysing data of this general type.\nA new section treats hierarchical and other forms of clustering.\nThe treatment of causal inference from observational data has been greatly extended, with extensive commentary on relevant R packages, and discussion of examples from the literature. Approaches to matching are a particular focus, with extensive references given. The use of directed acyclic graphs as a mechanism for making clear causal pathway assumptions is noted and references given, but not further discussed.\nThere is some limited attention to the use of multiple imputation to fill in missing values in data where some observations are incomplete, allowing use of those observations in a regression or other further analysis.\nAn appendix gives a brief overview of key features of the R system and notes technical issues that have particular relevance for users of the text.\nIn Chapter 2 and on, code is given only for those figures that are specifically targeted at the methodology under discussion. This site will be used as a first point of reference for R markdown scripts that have all the code from the book, and other supplementary materials.\nThe CRAN (Comprehensive R Archive Network) repository contains, at the time of writing, close to 20,000 packages. Further packages are available on other repositories, with Bioconductor perhaps the most important. Several others are listed upon typing setRepositories() at the command line. The 20,000 contrasts with the around 2,000 packages that were on CRAN prior to 2010 when the third edition of “Data Analysis and Graphics Using R” was in preparation. We have tried to keep up to date with new packages that supplement or extend what was available, but some will undoubtedly have been missed.\n\nReflections, looking back on the text in its published form, appear in “Afterword”."
  },
  {
    "objectID": "RunningCode.html#code-markup",
    "href": "RunningCode.html#code-markup",
    "title": "Use of Code Files",
    "section": "Code markup",
    "text": "Code markup\nThe following shows an approach to placing graphs side by side\n\nz &lt;- seq(-4,4,length=101)\nplot(z, dnorm(z), type=\"l\", ylab=\"Normal density\")\nplot(z, dt(z, df=5), type=\"l\", ylab=\"t-statistic density, 5 df\")"
  },
  {
    "objectID": "RunningCode.html#using-the-hmiscknitrset-function",
    "href": "RunningCode.html#using-the-hmiscknitrset-function",
    "title": "Use of Code Files",
    "section": "Using the Hmisc::knitrSet() function",
    "text": "Using the Hmisc::knitrSet() function\nThis function creates a setup where\n\nSpace around the graph can be adjusted by setting the chunk options bot, left, top, and rt;\nCommonly used base graphics parameters can be supplied as chunk options. These include lwd, mgp, las, tcl, axes, xpd, mfg.\nDefault settings can be supplied for a number of chunk options.\n\nSee ?Hmisc::knitrSet for details. With R markdown files, the setting fig.align=\"default\" is required. Either do not change from the default globally, or accompany fig.show=\"hold\" whenever used with fig.align=\"default\", thus:\n\nHmisc::knitrSet(basename=\"tmp\", lang='markdown', fig.path=\"figs/g\", w=6, h=6)\nz &lt;- seq(-4,4,length=101)\nplot(z, dnorm(z), type=\"l\", ylab=\"Normal density\")\nplot(z, dt(z, df=5), type=\"l\", ylab=\"t-statistic density, 5 df\")\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/UsingCode.R\")\n}"
  },
  {
    "objectID": "xtras.html",
    "href": "xtras.html",
    "title": "xtras",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "xtras.html#quarto",
    "href": "xtras.html#quarto",
    "title": "xtras",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "afterword.html",
    "href": "afterword.html",
    "title": "Afterword",
    "section": "",
    "text": "The writing of a text that has aimed to provide a reasonably well rounded account of modern statistical methodology, albeit with very limited attention to machine learning, has been a huge challenge. Comments now follow on several areas where, more than elsewhere, our text remains a work in progress. A warning is that some technical terms will be used that assume a fair level of prior statistical understanding."
  },
  {
    "objectID": "afterword.html#inference-remains-a-hotly-contested-area.",
    "href": "afterword.html#inference-remains-a-hotly-contested-area.",
    "title": "Afterword",
    "section": "Inference remains a hotly contested area.",
    "text": "Inference remains a hotly contested area.\nWe have used Bayes Factors, calculated assuming the family of ‘uninformative’ priors used in the BayesFactor package, as a way to make a connection from the hypothesis testing framework of frequentist statistics into the Bayesian world. As the calculations use a numerical approximation that avoids the need for the extensive chain of simulations required for the Markov Chain Monte Carlo approach, it is straightforward to make comparisons with \\(p\\)-values for a range of sample sizes, effect sizes, and scale parameter for the prior.\nAs Kahneman1 argues in his book on human judgment and decision making, humans are not good intuitive statisticians. This surely applies as much or more to the choice of Bayesian priors as to the judgments that are required in more classical contexts.\nHow does the Bayes Factor change with changes in the effect size, sample size, and number of model parameters? What is the effect of varying the scale parameter for the prior distribution? What circumstances create a case for centering the prior away from the null? The start that we have made at working with the prior families used in the BayesFactor package to provide graphs that can help answer such questions could usefully be extended much further. What difference does it make if a Cauchy prior is replaced by a normal prior, with roughly matched ranges of scale factors?\nMore attention to Bayesian credible intervals would have been made sense. Arguably, these make better sense than Bayes Factors if the interest is in finding a replacement for \\(p\\)-values and associated confidence intervals.\nInformation statistics – primarily AIC, AICc, and BIC – are a further focus. Associated relative preference measures, with a role similar to that of Bayes Factors, are noted. The BIC relative preference measure can be regarded as arising from the Bayes Factor obtained when a Jeffreys Unit Information prior is used that is centered away from the null.2\nNote especially the Subsection 2.9.2 comparison between Bayes Factors and the BIC statistic, for the one-sample \\(t\\)-test case. At the largest sample sizes (\\(n\\) = 80 and \\(n\\)=160) the Bayes Factor, while always smaller than the BIC ‘relative preference’ statistic, comes close to it in value. Larger sample sizes will be required to obtain a similar rough equivalence when the comparison is between two models that have one or more explanatory variables in common.\nThese various statistics are tools, to be used with appropriate caution, and having regard to what is known about the studies that generated the data."
  },
  {
    "objectID": "afterword.html#what-can-be-learned-from-reproducibilityreplication-studies",
    "href": "afterword.html#what-can-be-learned-from-reproducibilityreplication-studies",
    "title": "Afterword",
    "section": "What can be learned from reproducibility/replication studies?",
    "text": "What can be learned from reproducibility/replication studies?\nThere is extended commentary on the insight that studies where a substantial number of published experimental results have been independently replicated offer on what p-values mean in practice. Effect sizes for the replications have mostly been found to be on average much lower than for the original experiment. A major part of the difference is no doubt caused by selection effects, from publishing mainly or only those results that fall under a \\(p\\) &lt;= 0.05 or similar criterion.\nThere is a strong case for the publication in some form of all studies that effmeet minimum design and execution standards. Stricter experimental design criteria are called for, perhaps designing for \\(p \\leq 0.005\\) rather than the common \\(p \\leq 0.05\\)."
  },
  {
    "objectID": "afterword.html#simulation-has-many-uses",
    "href": "afterword.html#simulation-has-many-uses",
    "title": "Afterword",
    "section": "Simulation has many uses",
    "text": "Simulation has many uses\nThe model that is fitted is just one of the models that might have been fitted. Simulation can be used to repeatedly generate new data from the fitted model, then refitting the model to each set of new data. Overall, the different refits give an indication of how different another model fit, from data generated in the same way as the data presented for analysis, might have been. Do indications of departures from model assumptions for diagnostic plots for the fitted model lie within the range observed in the simulations? What is the extent of variation of \\(p\\)-values or other statistics that are of interest?\nSimulation can provide important insights when experiments are planned. Thus, where two treatments will be compared, it is insightful to simulate results for one or more effect sizes that are of interest. If sample sizes are overly small, statistics from the simulations (e.g., effect sizes, \\(p\\)-values, or other statistics) will show large variation from one simulation to another. There would be merit in requiring reports of results from experimental trials to show plots of relevant statistics that were examined at the study planning stage. Experimenters should have as clear as possible an understanding, before proceeding, of the ability of the experiment to discriminate between treatments. Steps taken to obtain this understanding should be reported."
  },
  {
    "objectID": "afterword.html#the-big-wide-world-of-r",
    "href": "afterword.html#the-big-wide-world-of-r",
    "title": "Afterword",
    "section": "The Big Wide World of R",
    "text": "The Big Wide World of R\nThe CRAN (Comprehensive R Archive Network) repository contains, at the time of writing, close to 20,000 packages. The 20,000 contrasts with the around 2,000 packages that were on CRAN prior to 2010 when the third edition of “Data Analysis and Graphics Using R” was in preparation.\nFurther packages are available on other repositories, with Bioconductor perhaps the most important. Type setRepositories() at the R command line to see the names of several further repositories. We have tried to keep up to date with new packages that supplement or extend what was available in 2010, but some will undoubtedly have been missed."
  },
  {
    "objectID": "afterword.html#footnotes",
    "href": "afterword.html#footnotes",
    "title": "Afterword",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKahneman, Daniel. Thinking, fast and slow. Macmillan, 2011.↩︎\nSee http://www.stat.washington.edu/research/reports/1999/tr347.pdf↩︎"
  },
  {
    "objectID": "figFuns.html",
    "href": "figFuns.html",
    "title": "Code for Selected Figures",
    "section": "",
    "text": "options(rmarkdown.html_vignette.check_title = FALSE)\n## xtras=TRUE    ## Set to TRUE to execute code 'extras'\nxtras &lt;- FALSE\nlibrary(knitr)\n## opts_chunk[['set']](results=\"asis\")\n## opts_chunk[['set']](eval=FALSE)   ## Set to TRUE to execute main part of code\nopts_chunk[['set']](eval=FALSE)\nFigures for which code appears here may in due course be made available for execution as functions."
  },
  {
    "objectID": "figFuns.html#figure-1.20",
    "href": "figFuns.html#figure-1.20",
    "title": "Code for Selected Figures",
    "section": "Figure 1.20",
    "text": "Figure 1.20\n\neff2stat &lt;- function(eff=c(.2,.4,.8,1.2), n=c(10,40), numreps=100,\n                     FUN=function(x,N)pt(sqrt(N)*mean(x)/sd(x), df=N-1, \n                                         lower.tail=FALSE)){\n  simStat &lt;- function(eff=c(.2,.4,.8,1.2), N=10, nrep=100, FUN){\n    num &lt;- N*nrep*length(eff)\n    array(rnorm(num, mean=eff), dim=c(length(eff),nrep,N)) |&gt;\n      apply(2:1, FUN, N=N) \n  }\n  mat &lt;- matrix(nrow=numreps*length(eff),ncol=length(n))\n  for(j in 1:length(n)) mat[,j] &lt;- \n    as.vector(simStat(eff, N=n[j], numreps, FUN=FUN))  ## length(eff)*numep\n  data.frame(effsize=rep(rep(eff, each=numreps), length(n)),\n             N=rep(n, each=numreps*length(eff)), stat=as.vector(mat))\n}\n\n\nset.seed(31)\ndf200 &lt;- eff2stat(eff=c(.2,.4,.8,1.2), n=c(10, 40), numreps=200)\nlabx &lt;- c(0.001,0.01,0.05,0.2,0.4,0.8)\ngph &lt;- bwplot(factor(effsize) ~ I(stat^0.25) | factor(N), data=df200, \n              layout=c(2,1), xlab=\"P-value\", ylab=\"Effect size\", \n              scales=list(x=list(at=labx^0.25, labels =labx)))\nupdate(gph+latticeExtra::layer(panel.abline(v=labx[1:3]^0.25, col='lightgray')),\n       strip=strip.custom(factor.levels=paste0(\"n=\",c(10,40))),\n       par.settings=DAAG::DAAGtheme(color=F, col.points=\"gray50\"))"
  },
  {
    "objectID": "figFuns.html#figure-1.24",
    "href": "figFuns.html#figure-1.24",
    "title": "Code for Selected Figures",
    "section": "Figure 1.24",
    "text": "Figure 1.24\n\nt2bfInterval &lt;- function(t, n=10, rscale=\"medium\", mu=c(-.1,.1)){\n     null0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu,\n                                       rscale=rscale,simple=TRUE)\nalt0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu, rscale=rscale, \n                                 complement=TRUE, simple=TRUE)\nalt0/null0\n}\n\n\npval &lt;- c(0.05,0.01,0.001); nval &lt;- c(4,6,10,20,40,80,160)\nbfDF &lt;- expand.grid(p=pval, n=nval)\npcol &lt;- 1; ncol &lt;- 2; tcol &lt;- 3\nbfDF[,'t'] &lt;- apply(bfDF,1,function(x){qt(x[pcol]/2, df=x[ncol]-1,                                  lower.tail=FALSE)})\nother &lt;- apply(bfDF,1,function(x)\n    c(BayesFactor::ttest.tstat(t=x[tcol], n1=x[ncol], rscale=\"medium\",\n                               simple=TRUE),\n      BayesFactor::ttest.tstat(t=x[tcol], n1=x[ncol], rscale=\"wide\",\n                               simple=TRUE),\n## Now specify a null interval\n    t2bfInterval(t=x[tcol], n=x[ncol], mu=c(-0.1,0.1),rscale=\"medium\"),\n    t2bfInterval(t=x[tcol], n=x[ncol], mu=c(-0.1,0.1),rscale=\"wide\")\n  ))\nbfDF &lt;- setNames(cbind(bfDF, t(other)),\n    c('p','n','t','bf','bfInterval'))\n\nAn alternative way to do the calculations for Exercise 31 in Chapter 1 is:\n\ndoBF &lt;- function(pval=c(0.05,0.01,0.002), nval=c(10,40,160)){\nbfDF &lt;- cbind(expand.grid(p=pval, n=nval),\n              matrix(nrow=nrow(bfDF), ncol=5))\nnames(bfDF)[3:7] &lt;- c(\"t\",\"bf\",\"bfw\",\"bfInterval\",\"bfIntervalw\")\nij=0\nfor(n in nval)for(p in pval){\n  # Here, `nval` (last specified in `expand.grid()`) is placed first \nij &lt;- ij+1\nt &lt;- bfDF[ij,'t'] &lt;- qt(p/2, df=n-1, lower.tail=FALSE)\nbfDF[ij,'bf'] &lt;- t2BF(t, n, mu=0, rscale=\"medium\")\nbfDF[ij,'bfw'] &lt;- t2BF(t, n, mu=0, rscale=\"wide\")\nbfDF[ij,'bfInterval'] &lt;- t2BF(t, n, mu=c(-0.1,0.1), rscale=\"medium\")\nbfDF[ij,'bfIntervalw'] &lt;- t2BF(t, n, mu=c(-0.1,0.1),rscale=\"wide\")\n}\nbfDF\n}\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/figFuns.R\")\n}"
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Code and Supplements – A Practical Guide . . .",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe text builds on “Data Analysis and Graphics Using R” (Maindonald and Braun, CUP, 3rd edn, 2010.)↩︎"
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "bf.html",
    "href": "bf.html",
    "title": "Bayes Factors & the BIC Statistic",
    "section": "",
    "text": "Kahneman (2011) makes the point that humans are poor intuitive statisticians. This is an especially serious issue for understanding and using \\(p\\)-values, for the choice of priors for Bayesian analyses, and for the use of information statistics. Observing how these various statistics compare for simulated data, and how they relate to one another, is helpful for the development of intuition.\nHere, the focus will be on the Bayesian Information Criterion (BIC) and the Akaike Information Criterion (AIC), and on the connection that can be made, more directly for the BIC than for the AIC, to a form of Bayes Factor.\nFor purposes of making a connection to the BIC and AIC, the focus will be on Bayes Factors as returned by functions in the Bayesfactor package and, by BPpack::BF().\n\nAll summary statistics are random variables\n\n\n\n\n\n\n\n\nFigure 1: Boxplots are for 200 simulated \\(p\\)-values for a one-sided - one-sample \\(t\\)-test, for the specified effect sizes eff and - sample sizes n. The \\(p^{0.25}\\) scale on the \\(x\\)-axis is used - to reduce the extreme asymmetry in the distributions.\n\n\n\n\n\nNote first that all these statistics, as well as \\(p\\)-values, are random variables. The randomness is a particular issue when sample sizes are small and/or effect sizes are small. Figure 1 highlights this point. Code is:\n\n\nCode\neff2stat &lt;- function(eff=c(.2,.4,.8,1.2), n=c(40,160), numreps=200,\n                     FUN=function(x,N)pt(sqrt(N)*mean(x)/sd(x), df=N-1,\n                                         lower.tail=FALSE)){\n  simStat &lt;- function(eff=c(.2,.4,.8,1.2), N=10, nrep=200, FUN){\n    num &lt;- N*nrep*length(eff)\n    array(rnorm(num, mean=eff), dim=c(length(eff),nrep,N)) |&gt;\n      apply(2:1, FUN, N=N)\n  }\n  mat &lt;- matrix(nrow=numreps*length(eff),ncol=length(n))\n  for(j in 1:length(n)) mat[,j] &lt;-\n    as.vector(simStat(eff, N=n[j], numreps, FUN=FUN))  ## length(eff)*numep\n  data.frame(effsize=rep(rep(eff, each=numreps), length(n)),\n             N=rep(n, each=numreps*length(eff)), stat=as.vector(mat))\n}\n\n\n\n\nCode\nlibrary(lattice)\nset.seed(31)\nn &lt;- c (40,80)\ndf200 &lt;- eff2stat(eff=c(.2,.4,.8,1.2), n=n, numreps=200)\nlabx &lt;- c(0.001,0.01,0.05,0.2,0.4,0.8)\ngph &lt;- bwplot(factor(effsize) ~ I(stat^0.25) | factor(N), data=df200,\n              layout=c(2,1), xlab=\"P-value\", ylab=\"Effect size\",\n              scales=list(x=list(at=labx^0.25, labels =labx)))\nupdate(gph+latticeExtra::layer(panel.abline(v=labx[1:3]^0.25, col='lightgray')),\n       strip=strip.custom(factor.levels=paste0(\"n=\", n)),\n       par.settings=DAAG::DAAGtheme(color=F, col.points=\"gray50\"))\n\n\n\n\nThe BIC and AIC connection to Bayes Factors\nAs a starting point, comparisons will be for a one-sample \\(t\\)-statistic.\nGiven two models with the same outcome variable, with respective BIC (Bayesian Information Criterion) statistics \\(m_1\\) and \\(m_2\\), the quantity \\[\nb_{12} = exp((m_1-m_2)/2)\n\\] can be used as a relative preference statistic for \\(m_2\\) as against \\(m_1\\). If model 1 is nested in model 2 this becomes, under the prior that has the name Jeffreys Unit Information (JUI) prior, with the prior centered on the maximum likelihood of the difference under the alternative, a Bayes Factor giving the probability of model 2 relative to model 1. In the case of a one-sample \\(t\\)-statistic, the BIC-derived Bayes Factor is \\[\n\\mbox{exp}(N*\\log(1+\\frac{t^2}{N-1})-\\log(N))/2),\n\\mbox{  where }N \\mbox{ is the sample size}\n\\]\nHow does this compare with Bayes Factors that are obtained with other choices of prior? Specifically, because the calculations can then be handled without Markov Chain Monte Carlo simulation, we look at results from functions in the Bayesfactor and BFpack packages.\n\nComparison with results from BayesFactor::ttestBF()\nFunctions in the BayesFactor package assume a Jeffreys-Zellner-Siow (JSZ) prior, which has a reasonable claim to be used as a default prior. Numerical quadrature is used to calculate the Bayes Factor, avoiding the need for Markov Chain Monte Carlo simulation. A Cauchy prior is assumed for the effect size, with the argument rscale giving the scale factor. The Jeffreys distribution has a similar role for the variance of the normal distributions that are assumed both under the null and under the alternative.\n\n\nCode\n# Functions that calculate Bayes Factors or relative preferences  \nt2BF &lt;- function(p=0.05, N, xrs=1/sqrt(2)){\n  t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n  BayesFactor::ttest.tstat(t=t, n1=N, rscale=xrs, simple=TRUE)}\nt2BFbic &lt;- function(p,N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n  exp((N*log(1+t^2/(N-1))-log(N))/2)}\nt2AIC &lt;- function(p,N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n  exp((N*log(1+t^2/(N-1))-2)/2)}\nt2AICc &lt;- function(p,N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n  exp((N*log(1+t^2/(N-1))-12/(N-3)+4/(N-2)-2)/2)}  ## Requires N &gt; 6\nt2eff &lt;- function(p=0.05, N)\n  eff &lt;- qt(p/2, df=N-1, lower.tail=FALSE)/sqrt(N)\n\n\n\n\nCode\npval &lt;- c(.05,.01,.001); np &lt;- length(pval)\nNval &lt;- c(3,4,5,10,20,40,80,160,360); nlen &lt;- length(Nval)\n## Bayes Factors, using BayesFactor::ttest.tstat()\nrs &lt;- c(1/sqrt(2), sqrt(2))\nbf &lt;- matrix(nrow=length(rs)+2,ncol=length(Nval))\ndimnames(bf) &lt;-\n  list(c('rscale=1/sqrt(2)', '       sqrt(2)', 'BIC','Effect size'),\n       paste0(c(\"n=\",rep(\"\",length(Nval)-1)),Nval))\nbfVal &lt;- setNames(rep(list(bf),length(pval)),\n                  paste0('p', substring(paste(pval),2)))\nfor(k in 1:length(pval)){\n  p &lt;- pval[k]\nfor(i in 1:length(rs))for(j in 1:nlen)\n  bfVal[[k]][i,j] &lt;- t2BF(p=p, N=Nval[j], xrs=rs[i])\nbfVal[[k]][length(rs)+1,] &lt;- outer(p, Nval, t2BFbic)\nbfVal[[k]][length(rs)+2,] &lt;- outer(p, Nval, t2eff)\n}\nlapply(bfVal, function(x)signif(x,2))\n\n\n$p.05\n                  n=3   4   5   10   20   40   80  160  360\nrscale=1/sqrt(2)  2.6 2.4 2.2 1.80 1.40 1.10 0.80 0.59 0.40\n       sqrt(2)    3.0 2.5 2.1 1.30 0.89 0.62 0.43 0.31 0.20\nBIC              19.0 9.6 6.6 3.00 1.80 1.20 0.79 0.55 0.36\nEffect size       2.5 1.6 1.2 0.72 0.47 0.32 0.22 0.16 0.10\n\n$p.01\n                   n=3    4    5   10   20   40  80  160  360\nrscale=1/sqrt(2)   6.4  7.0  7.0  6.2 5.10 4.10 3.1 2.30 1.60\n       sqrt(2)     9.8  9.6  8.7  5.6 3.70 2.50 1.8 1.20 0.82\nBIC              210.0 77.0 45.0 15.0 8.00 5.00 3.3 2.30 1.50\nEffect size        5.7  2.9  2.1  1.0 0.64 0.43 0.3 0.21 0.14\n\n$p.001\n                  n=3      4     5    10    20    40    80   160   360\nrscale=1/sqrt(2)   21   32.0  38.0  41.0 37.00 31.00 24.00 18.00 13.00\n       sqrt(2)     39   56.0  60.0  47.0 31.00 21.00 15.00 10.00  6.70\nBIC              6500 1600.0 750.0 180.0 77.00 44.00 28.00 19.00 12.00\nEffect size        18    6.5   3.9   1.5  0.87  0.56  0.38  0.27  0.17\n\n\nNote several points:\n- The BIC-based ‘Bayes Factor’ gives unreasonably large factors for small values of \\(n\\). Not until \\(n\\)=80 is the value in much the same ballpark as the Bayes Factor generated by the BayesFactor function. The BIC statistic really is designed for use in a “large sample” context.\n- For large enough values of \\(n\\), the BIC-based values lie between the Bayes Factor with for an rscale of \\(1/\\sqrt{2}\\) and that for an rscale of \\(\\sqrt{2}\\).\n- As \\(n\\) increases, the estimated effect size to which the Bayes Factor corresponds becomes ever smaller.\nNote then that BayesFactor::ttestBF() with the default setting of rscale, and the BIC-based Bayes Factor, are both using a prior whose scale is large relative to an ever smaller effect size.\n\n\nMatching the setting of rscale to the effect size\nObserve then the result from matching the scale for the prior to the effect size. The following checks this for \\(p\\)=0.05, making at the same time a comparison with AIC-based and BIC-based relative ‘preferences’.\n\n\nCode\nrs &lt;- c(0.5,1,4,16)\npval &lt;- 0.05\nBFrs &lt;- matrix(nrow=length(rs)+3, ncol=nlen)\ndimnames(BFrs) &lt;-\n  list(c(paste0(c(\"rscale=\",rep(\"       \",3)),rs,\"/sqrt(n)\"),\n         \"rscale=1/sqrt(2)\",\"BIC-based\",\"AIC-based\"), \n       paste0(c(\"n=\",rep(\"\",nlen-1)),Nval))\nfor(j in 1:nlen){\n  for(i in 1:length(rs))\n     BFrs[i,j] &lt;- t2BF(p=pval, N=Nval[j], xrs=rs[i]/sqrt(Nval[j]))\n  BFrs[length(rs)+1, j] &lt;- t2BF(p=pval, N=Nval[j], xrs=1/sqrt(2))\n  BFrs[length(rs)+2, j] &lt;- t2BFbic(p=pval, N=Nval[j])\n  BFrs[length(rs)+3, j] &lt;- t2AIC(p=pval, N=Nval[j])\n}\nprint(setNames(\"p=0.05\",\"\"), quote=F)\n\n\n       \np=0.05 \n\n\nCode\nround(BFrs,2)\n\n\n                     n=3    4    5   10   20   40   80  160  360\nrscale=0.5/sqrt(n)  1.84 1.77 1.71 1.59 1.53 1.50 1.48 1.48 1.47\n       1/sqrt(n)    2.38 2.19 2.06 1.81 1.70 1.65 1.62 1.61 1.60\n       4/sqrt(n)    3.02 2.28 1.92 1.41 1.22 1.15 1.11 1.10 1.09\n       16/sqrt(n)   1.48 0.91 0.70 0.46 0.39 0.36 0.35 0.34 0.34\nrscale=1/sqrt(2)    2.56 2.38 2.22 1.76 1.39 1.07 0.80 0.59 0.40\nBIC-based          18.96 9.57 6.56 3.00 1.78 1.16 0.79 0.55 0.36\nAIC-based          12.08 7.04 5.39 3.49 2.93 2.71 2.60 2.56 2.53\n\n\nThus, the BIC is designed to look for effect sizes that are around one. If a small effect size is expected in a large sample context, use of ttestBF() or ttest.tstat() with a setting of rscale that matches the expected effect size, makes better sense than use of BIC().\nThere is a choice of prior that allows the AIC-based preference measure to be interpreted as a Bayes Factor. See Burnham & Anderson (2004). Relative preference values that are larger than from the BayesFactor functions at all settings of rscale suggests a tendency to choose an overly complex model.\nFor \\(p\\)=0.01 we find:\n\n\nCode\nrs &lt;- c(0.5,1,4,16)\npval &lt;- 0.01\nBFrs &lt;- matrix(nrow=length(rs)+3, ncol=nlen)\ndimnames(BFrs) &lt;-\n  list(c(paste0(c(\"rscale=\",rep(\"       \",3)),rs,\"/sqrt(n)\"),\"rscale=1/sqrt(2)\",\"BIC-based\",\"AIC-based\"), paste0(c(\"n=\",rep(\"\",nlen-1)),Nval))\nfor(j in 1:nlen){\n  for(i in 1:length(rs))\n     BFrs[i,j] &lt;- t2BF(p=pval, N=Nval[j], xrs=rs[i]/sqrt(Nval[j]))\n  BFrs[length(rs)+1, j] &lt;- t2BF(p=pval, N=Nval[j], xrs=1/sqrt(2))\n  BFrs[length(rs)+2, j] &lt;- t2BFbic(p=pval, N=Nval[j])\n  BFrs[length(rs)+3, j] &lt;- t2AIC(p=pval, N=Nval[j])\n}\n\n\n\n\nCode\nprint(setNames(\"p=0.01\",\"\"), quote=F)\n\n\n       \np=0.01 \n\n\nCode\nround(BFrs,2)\n\n\n                      n=3     4     5    10    20    40    80   160   360\nrscale=0.5/sqrt(n)   3.49  3.65  3.62  3.38  3.22  3.13  3.09  3.07  3.06\n       1/sqrt(n)     5.56  5.71  5.54  4.87  4.48  4.28  4.19  4.14  4.12\n       4/sqrt(n)    12.32 10.39  8.77  5.85  4.75  4.30  4.09  3.99  3.94\n       16/sqrt(n)   12.09  6.54  4.51  2.31  1.73  1.51  1.42  1.38  1.35\nrscale=1/sqrt(2)     6.38  7.03  7.05  6.20  5.12  4.08  3.12  2.32  1.60\nBIC-based          205.66 76.53 44.54 15.34  8.04  4.96  3.29  2.25  1.47\nAIC-based          131.05 56.31 36.64 17.84 13.23 11.54 10.81 10.47 10.29\n\n\n\n\n\nUse of functions from the BFpack package\nWe investigate the Bayes Factors that are calculated using the Fractional Bayes Factor Approach. The details are not easy to describe simply. However the effect is that allowance must be made for the use of a fraction of the information in the data to determine the null. See Mulder et al (2021).\nWe compare\n\nBayes Factor with Jeffreys-Zellner-Siow prior centered on NULL\nFractional Bayes Factor from BFpack (BF.type=1), i.e., the prior is centered on the NULL\nFractional Bayes Factor from BFpack (BF.type=2), i.e., the prior is centered on the maximum likelihood estimate under the alternative.\nAlternative versus NULL, based on Bayesian Information Criterion (BIC)\n\n\n\nCode\nsuppressPackageStartupMessages(library(BayesFactor))\nsuppressPackageStartupMessages(library(BFpack))\nsuppressPackageStartupMessages(library(metRology))\npval &lt;- c(.05,.01,.001); np &lt;- length(pval)\nNval &lt;- c(3:5,10,20,40,80,160,320); nlen &lt;- length(Nval)\nbicVal &lt;- outer(pval, Nval, t2BFbic)\n# t2BF &lt;- function(p, N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n#                       BayesFactor::ttest.tstat(t=t, n1=N, simple=TRUE)}\nBFval &lt;- packValNull &lt;- packValAlt &lt;- matrix(nrow=np,ncol=nlen)\ndimnames(packValNull) &lt;- dimnames(packValAlt) &lt;- dimnames(bicVal) &lt;- \n  dimnames(BFval) &lt;-\n  list(paste(pval), paste0(c(\"n=\",rep(\"\",nlen-1)),Nval))\nfor(i in 1:np)for(j in 1:nlen){\n  t &lt;- qt(pval[i]/2,Nval[j]-1,lower.tail=F)\n  d &lt;- rnorm(Nval[j])\n  d &lt;- d-mean(d)+t*sd(d)/sqrt(Nval[j])\n  tt &lt;- bain::t_test(d)\n  packValNull[i,j] &lt;- BF(tt,  hypothesis='mu=0',  \n    BF.type=1)[['BFmatrix_confirmatory']]['complement', 'mu=0']\n  packValAlt[i,j] &lt;- BF(tt,  hypothesis='mu=0',  \n    BF.type=2)[['BFmatrix_confirmatory']]['complement', 'mu=0']\n  BFval[i,j] &lt;- t2BF(pval[i], Nval[j])}\n\n\n\n\nCode\n## Fractional Bayes factor, center on point null\nprint(setNames(\"Fractional Bayes Factor, center prior on null\",\"\"), quote=F)\n\n\n                                              \nFractional Bayes Factor, center prior on null \n\n\nCode\nprint(packValNull, digits=3)\n\n\n        n=3     4     5    10    20     40     80   160   320\n0.05   2.04  2.19  2.13  1.66  1.20  0.856  0.607  0.43 0.304\n0.01   4.51  6.19  6.71  6.10  4.66  3.395  2.432  1.73 1.227\n0.001 14.24 28.34 36.64 42.93 35.65 26.849 19.520 13.98 9.951\n\n\n\n\nCode\n## Bayes Factor (Cauchy prior, `rscale=\"medium\")`\nprint(setNames(\"From `BayesFactor::ttestBF()`, center prior on null\",\"\"), quote=F)\n\n\n                                                    \nFrom `BayesFactor::ttestBF()`, center prior on null \n\n\nCode\nprint(BFval, digits=3)\n\n\n        n=3     4     5    10    20    40    80    160    320\n0.05   2.56  2.38  2.22  1.76  1.39  1.07  0.80  0.585  0.422\n0.01   6.38  7.03  7.05  6.20  5.12  4.08  3.12  2.321  1.688\n0.001 21.44 32.35 37.65 40.86 36.57 30.53 24.19 18.364 13.527\n\n\n\n\nCode\n## BIC-based to BFpack::BF() ratio\nprint(setNames(\"FBF, center prior on null: Ratio to BayesFactor result\",\"\"), quote=F)\n\n\n                                                       \nFBF, center prior on null: Ratio to BayesFactor result \n\n\nCode\nprint(packValNull/BFval, digits=3)\n\n\n        n=3     4     5    10    20    40    80   160   320\n0.05  0.797 0.919 0.958 0.939 0.864 0.800 0.759 0.734 0.721\n0.01  0.708 0.880 0.952 0.984 0.910 0.833 0.778 0.745 0.727\n0.001 0.664 0.876 0.973 1.050 0.975 0.879 0.807 0.761 0.736\n\n\n\nBFpack::BF() with BF.type=2 vs derived from BIC\n\n\nCode\n# Fractional Bayes factor, center on estimate under alternative\nprint(setNames(\"FBF, center on estimate under alternative\",\"\"), quote=F)\n\n\n                                          \nFBF, center on estimate under alternative \n\n\nCode\nprint(packValAlt, digits=3)\n\n\n         n=3       4      5    10    20     40     80    160    320\n0.05    20.9    9.57   6.22   2.6  1.48  0.946  0.638  0.441  0.308\n0.01   226.8   76.53  42.27  13.3  6.67  4.033  2.647  1.804  1.253\n0.001 7123.0 1606.11 715.79 151.9 63.95 35.565 22.407 14.973 10.295\n\n\n\n\nCode\n## From BIC\nprint(setNames(\"Derived from BIC\",\"\"), quote=F)\n\n\n                 \nDerived from BIC \n\n\nCode\nprint(bicVal, digits=3)\n\n\n       n=3       4      5    10    20    40     80   160    320\n0.05    19    9.57   6.56   3.0  1.78  1.16  0.792  0.55  0.385\n0.01   206   76.53  44.54  15.3  8.04  4.96  3.286  2.25  1.567\n0.001 6460 1606.11 754.24 175.7 77.10 43.73 27.819 18.68 12.872\n\n\n\n\nCode\n## BIC-based to BFpack::BF() ratio\nprint(setNames(\"FBF, center prior on alternative: Ratio to BIC\",\"\"), quote=F)\n\n\n                                               \nFBF, center prior on alternative: Ratio to BIC \n\n\nCode\nprint(packValAlt/bicVal, digits=3)\n\n\n      n=3 4     5    10    20    40    80   160 320\n0.05  1.1 1 0.949 0.865 0.829 0.813 0.805 0.802 0.8\n0.01  1.1 1 0.949 0.865 0.829 0.813 0.805 0.802 0.8\n0.001 1.1 1 0.949 0.865 0.829 0.813 0.805 0.802 0.8\n\n\nThe function BFpack::BF() is making allowance for the use of a fraction of the information in the data used to specify the prior distribution. The BIC based calculations do not make such an adjustment.\nAs for the use of the BIC to choose between a simpler and a more complex model, the calculated Bayes Factors are unreasonably large for small samples, while in larger samples the prior is tuned to detect effect sizes that are of similar (or larger) magnitude than the standard deviation.\nFigure 2 summarizes the comparisons made\n\n\n\n\n\n\n\n\nFigure 2: Results from different ways to calculate the Bayes Factor - for a result from a one-sample two-sided \\(t\\)-test where \\(p\\)=0.05\n\n\n\n\n\nCode is:\n\n\nCode\nlibrary(lattice)\nallVal &lt;- rbind(BFval, packValNull, bicVal, packValAlt)\nrownames(allVal) &lt;- paste0(\n  rep(c('BayesFactor', 'packValNull', 'BIC', 'packValAlt'), c(3,3,3,3)),\n  rep(c(\".05\",\".01\",\".001\"), 4))\ntdf &lt;- as.data.frame(t(allVal))\ntdf$n &lt;- Nval\nlabs &lt;- sort(c(2^(0:6),2^(0:6)*1.5))\nxyplot(BayesFactor.05+packValNull.05+BIC.05+packValAlt.05 ~ n,\n       data=tdf, type='l', auto.key=list(columns=2),\n       xlab=\"Sample size $n$\",\n       ylab=\"Bayes Factor (Choice of 4 possibilities)\",\n       scales=list(x=list(at=(0:8)*40),\n         y=list(log=T, at=labs, labels=paste(labs))),\n par.settings=simpleTheme(lty=c(1,1:3), lwd=2, col=rep(c('gray','black'), c(1,3))))\n\n\n\n\nBayes Factors for regression coefficients.\nWe will use the following function to simulate data, with two explanatory variables, for use in regression calculations:\n\n\nCode\nsimDat &lt;- function(x1=rep(1:20,4)/5, x2=sample(rep(1:20,4)/5), \n                   b1=1.2, b2=1.5, sd=8){\n  n &lt;- length(x1)\n  data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(n,sd=sd))\n}\n\n\n\nOne data generating mechanism – large dataset to dataset variation\nNote first that with the default settings, the simulated data and \\(p\\)-values in the fitted model show large variation from one simulation to the next. The following shows relatively extreme differences:\n\n\nCode\nset.seed(17)\ndat &lt;- simDat()\ny.lm &lt;- lm(y~x1+x2, data=dat); bf12 &lt;- lmBF(y ~ x1+x2, data=dat)\n## Repeat simulation of data\ndat &lt;- simDat()\nyy.lm &lt;- lm(y~x1+x2, data=dat); bf12 &lt;- lmBF(y ~ x1+x2, data=dat)\ncbind(coef(summary(y.lm))[,-2], coef(summary(yy.lm))[,-2]) |&gt; signif(2)\n\n\n            Estimate t value Pr(&gt;|t|) Estimate t value Pr(&gt;|t|)\n(Intercept)     -2.4    -1.0   0.3200     1.40    0.63    0.530\nx1               1.7     2.3   0.0250     1.80    2.40    0.018\nx2               2.0     2.7   0.0076     0.74    1.00    0.310\n\n\nP-values from nine simulations are:\n\n\nCode\nmultSims &lt;- function(sd, nsims=9, rnam=c(\"(Intercept)\",\"x1\",\"x2\")){\npvals &lt;- matrix(nrow=3, ncol=nsims, dimnames=list(rnam,\n                paste0('pval', 1:nsims)))\nfor(i in 1:nsims){dat &lt;- simDat(sd=sd)\n  dat.lm &lt;- lm(y~x1+x2, data=dat); bf12 &lt;- lmBF(y ~ x1+x2, data=dat)\n  pvals[,i] &lt;- coef(summary(dat.lm))[,4]\n}\npvals\n}\nmultSims(sd=8) |&gt; signif(2)\n\n\n            pval1 pval2 pval3  pval4  pval5 pval6  pval7 pval8 pval9\n(Intercept)  0.40 0.690 0.590 0.2000 0.3500 0.850 0.7800 0.790 0.910\nx1           0.49 0.081 0.250 0.0460 0.0380 0.089 0.2500 0.390 0.025\nx2           0.44 0.020 0.037 0.0012 0.0064 0.190 0.0024 0.022 0.380\n\n\nNow try with sd=5:\n\n\nCode\nmultSims(sd=5) |&gt; signif(2)\n\n\n              pval1  pval2  pval3   pval4   pval5   pval6  pval7   pval8  pval9\n(Intercept) 5.7e-01 0.8600 0.6000 0.77000 0.73000 2.2e-01 0.1800 0.09900 0.4200\nx1          1.6e-01 0.0510 0.0015 0.16000 0.01700 7.4e-05 0.4800 0.00089 0.0042\nx2          3.5e-06 0.0023 0.0150 0.00033 0.00025 6.2e-03 0.0001 0.00004 0.0460\n\n\nThe \\(p\\)-values are more consistently small.\n\n\nBayes Factors and BIC statistics\nNow create a simulated dataset, and calculate Bayes Factors for the coefficients (1) using Bayesfactor functions, and (2) derived from BIC statistics:\n\n\nCode\nset.seed(31)\ndat31 &lt;- simDat()\ny.lm &lt;- lm(y~x1+x2, data=dat31); bf12 &lt;- lmBF(y ~ x1+x2, data=dat31)\ny2.lm &lt;- lm(y~x2, data=dat31); bf2 &lt;- lmBF(y ~ x2, data=dat31)\ny1.lm &lt;- lm(y~x1, data=dat31); bf1 &lt;- lmBF(y ~ x1, data=dat31)\n## Regression summary\ncoef(summary(y.lm)) |&gt; signif(2)\n\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     1.20        2.2    0.55    0.590\nx1              0.99        0.7    1.40    0.160\nx2              1.80        0.7    2.50    0.014\n\n\nCode\n## Bayes Factors for x1 and x2, using functions from _Bayesfactor_\nc(extractBF(bf12/bf2)$bf, extractBF(bf12/bf1)$bf) |&gt; round(2)\n\n\n[1] 0.70 4.45\n\n\nCode\n## Bayes Factors for x1 and x2, derived from BIC statistics\nc(exp((BIC(y2.lm)-BIC(y.lm))/2), exp((BIC(y1.lm)-BIC(y.lm))/2)) |&gt; round(2)\n\n\n[1] 0.31 2.65\n\n\nBoth are substantially smaller than those derived from calculations using BayesFactor::lmBF().\n\n\nMatching the Bayes Factor to the SEs of the coefficients\nCheck also the difference made to the Bayes Factors by setting the scale parameter (here rscaleCont) to a value that is matched to the standard error of the coefficient estimates. A standard error that is just under 0.7 is the same for both coefficients. We standardise this by dividing by a standard deviation of just under 7.2, multiply by the default rscale that equals \\(\\sqrt(2)/4\\), and use this as the value for rscaleCont.\n\n\nCode\nrs &lt;- 2*0.75/7.2\n  bf12 &lt;- lmBF(y ~ x1+x2, data=dat31, rscaleCont=rs)\n  bf2 &lt;- lmBF(y ~ x2, data=dat31, rscaleCont=rs)\n  bf1 &lt;- lmBF(y ~ x1, data=dat31, rscaleCont=rs)\n  c(extractBF(bf12/bf2)$bf, extractBF(bf12/bf1)$bf) |&gt; round(2)\n\n\n[1] 0.84 4.33\n\n\nIn this context, the smaller Bayes Factor is modestly increased, with the larger Bayes Factor slightly reduced.\n\n\n\nDifferent statistics offer different perspectives\nIn addition to the choice between different prior families, one has to choose a scale for the prior, if this is not done automatically. Different choices can lead to quite different Bayes Factors. Be aware that Bayes Factors are at best a rough measure of model preference. Use them along with other measures of model preference. Keep in mind that when samples are small, different samples from the same population, if available, would give widely varying results. Refer back to Figure 1, which showed what could be expected for \\(p\\)-values.\nThe comparisons could usefully be extended to consider other choices of prior.\n\n\nReferences\nBurnham, K. P., & Anderson, D. R. (2004). Multimodel inference: understanding AIC and BIC in model selection. Sociological methods & research, 33(2), 261-304.\nKahneman, D. (2011). Thinking, fast and slow. Macmillan.\nMulder, J., Williams, D. R., Gu, X., Tomarken, A., Böing-Messing, F., Olsson-Collentine, A., Meijerink-Bosman, M., Menke, J., van Aert, R., Fox, J.-P., Hoijtink, H., Rosseel, Y., Wagenmakers, E.-J., & van Lissa, C. (2021). BFpack: Flexible Bayes Factor Testing of Scientific Theories in R. Journal of Statistical Software, 100(18), 1–63. https://doi.org/10.18637/jss.v100.i18"
  }
]