---
title: "Bayes Factors and Information Statistics"
subtitle: ""
author: "John Maindonald"
date: "2024-07-15"
execute:
  echo: true
format:
  html:
    code-fold: true
    shift-heading-level-by: 2
  pdf:
    number-sections: false
# editor-options: 
#   markdown: 
#     wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk[["set"]](cache=FALSE, comment=NA)
```

Bayes Factors give a data analysis perspective that is different to that given by $p$-values, one that is in most cases closer to what the analyst would like to know. The value obtained depends, inevitably, on the choice of prior. But, what choice of prior makes good sense? Intuition, if it is to help, requires training. How do Bayes Factors with common choices of prior, stack up against the use of the information statistics BIC (Bayes Information Criterion) and AIC (Akaike Information Criterion) for use in model choice? Intuition, if it is to be useful at all, requires training. The discussion that follows may be helpful to this end.

For purposes of making a connection to the BIC and AIC, the focus will be on Bayes Factors as returned by functions in the *BayesFactor* package and, by `BPpack::BF()`. The BIC, in the large sample context for which it was developed, has a direct connection to a form of Bayes Factor. For the AIC, or the AICc that should replace it for use when the ratio of sample size to number of parameters estimated is less than perhaps 40, a more tenuous connection can be made.
See Bayarri et al (2012) for commentary on the rational for the
Jeffreys-Zellner-Siow family of priors that is used by *BayesFactor*
functions.

The AIC penalty term is designed so that, in the large sample limit, the statistic will select the model with the lowest prediction error. The BIC penalty term is designed, in the large sample limit, to select the correct model. In practical use, this distinction may be somewhat artificial.

# All summary statistics are random variables

```{r}
#| label: eff2stat
#| echo: false
eff2stat <- function(eff=c(.2,.4,.8,1.2), n=c(40,160), numreps=200,
                     FUN=function(x,N)pt(sqrt(N)*mean(x)/sd(x), df=N-1,
                                         lower.tail=FALSE)){
  simStat <- function(eff=c(.2,.4,.8,1.2), N=10, nrep=200, FUN){
    num <- N*nrep*length(eff)
    array(rnorm(num, mean=eff), dim=c(length(eff),nrep,N)) |>
      apply(2:1, FUN, N=N)
  }
  mat <- matrix(nrow=numreps*length(eff),ncol=length(n))
  for(j in 1:length(n)) mat[,j] <-
    as.vector(simStat(eff, N=n[j], numreps, FUN=FUN))  ## length(eff)*numep
  data.frame(effsize=rep(rep(eff, each=numreps), length(n)),
             N=rep(n, each=numreps*length(eff)), stat=as.vector(mat))
}
```

```{r, fig.width=5.2, fig.height=2.5, echo=F, out.width="\\textwidth"}
#| fig-cap: "Boxplots are for 200 simulated $p$-values for a one-sided
#|  - one-sample $t$-test, for the specified effect sizes `eff` and 
#|  - sample sizes `n`.  The $p^{0.25}$ scale on the $x$-axis is used 
#|  - to reduce the extreme asymmetry in the distributions."
#| label: fig-pval
#| echo: false
library(lattice)
set.seed(31)
n <- c (40,80)
df200 <- eff2stat(eff=c(.2,.4,.8,1.2), n=n, numreps=200)
labx <- c(0.001,0.01,0.05,0.2,0.4,0.8)
gph <- bwplot(factor(effsize) ~ I(stat^0.25) | factor(N), data=df200,
              layout=c(2,1), xlab="P-value", ylab="Effect size",
              scales=list(x=list(at=labx^0.25, labels =labx)))
update(gph+latticeExtra::layer(panel.abline(v=labx[1:3]^0.25, col='lightgray')),
       strip=strip.custom(factor.levels=paste0("n=", n)),
       par.settings=DAAG::DAAGtheme(color=F, col.points="gray50"))
```

Note first that all these statistics, as well as $p$-values, are random variables. The randomness is a particular issue when sample sizes are small and/or effect sizes are small. @fig-pval highlights this point. Code is:

```{r, eval=FALSE}
#| label: eff2stat
```

```{r, eval=FALSE}
#| label: fig-pval
```

# The BIC and AIC connection to Bayes Factors

As a starting point, comparisons will be for a one-sample $t$-statistic.

Given two models with the same outcome variable, with respective BIC (Bayesian Information Criterion) statistics $m_1$ and $m_2$, the quantity 
$$
b_{12} = exp((m_1-m_2)/2)
$$ 
can be used as a relative preference statistic for $m_2$ as against $m_1$. If model 1 is nested in model 2 this becomes, under what is known as a Jeffreys Unit Information (JUI) prior that is centered on the maximum likelihood of the difference under the alternative, a Bayes Factor giving the probability of model 2 relative to model 1. In the case of a one-sample $t$-statistic, the BIC-derived Bayes Factor is $$
\mbox{exp}(N*\log(1+\frac{t^2}{N-1})-\log(N))/2), 
\mbox{  where }N \mbox{ is the sample size}
$$

How does this compare with Bayes Factors that are obtained with other choices of prior? Specifically, because the calculations can then be handled without Markov Chain Monte Carlo simulation, we look at results from functions in the *Bayesfactor* and *BFpack* packages.

## Comparison with results from `BayesFactor::ttestBF()`

Functions in the *BayesFactor* package assume a Jeffreys-Zellner-Siow (JSZ) prior, which has a reasonable claim to be used as a default prior. Numerical quadrature is used to calculate the Bayes Factor, avoiding the need for Markov Chain Monte Carlo simulation. A Cauchy prior is assumed for the effect size, with the argument `rscale` giving the scale factor. The Jeffreys distribution has a similar role for the variance of the normal distributions that are assumed both under the null and under the alternative.

```{r}
# Functions that calculate Bayes Factors or relative preferences  
t2BF <- function(p=0.05, N, xrs=1/sqrt(2)){
  t <- qt(p/2, df=N-1, lower.tail=FALSE)
  BayesFactor::ttest.tstat(t=t, n1=N, rscale=xrs, simple=TRUE)}
t2BFbic <- function(p,N){t <- qt(p/2, df=N-1, lower.tail=FALSE)
  exp((N*log(1+t^2/(N-1))-log(N))/2)}
t2AIC <- function(p,N){t <- qt(p/2, df=N-1, lower.tail=FALSE)
  exp((N*log(1+t^2/(N-1))-2)/2)}
t2AICc <- function(p,N){t <- qt(p/2, df=N-1, lower.tail=FALSE)
  exp((N*log(1+t^2/(N-1))-12/(N-3)+4/(N-2)-2)/2)}  ## Requires N > 6
t2eff <- function(p=0.05, N)
  eff <- qt(p/2, df=N-1, lower.tail=FALSE)/sqrt(N)
```

```{r}
pval <- c(.05,.01,.001); np <- length(pval)
Nval <- c(3,4,5,10,20,40,80,160,360); nlen <- length(Nval)
## Bayes Factors, using BayesFactor::ttest.tstat()
rs <- c(1/sqrt(2), sqrt(2))
bf <- matrix(nrow=length(rs)+2,ncol=length(Nval))
dimnames(bf) <-
  list(c('rscale=1/sqrt(2)', '       sqrt(2)', 'BIC','Effect size'),
       paste0(c("n=",rep("",length(Nval)-1)),Nval))
bfVal <- setNames(rep(list(bf),length(pval)),
                  paste0('p', substring(paste(pval),2)))
for(k in 1:length(pval)){p <- pval[k]
  for(i in 1:length(rs))for(j in 1:nlen)
    bfVal[[k]][i,j] <- t2BF(p=p, N=Nval[j], xrs=rs[i])
  bfVal[[k]][length(rs)+1,] <- outer(p, Nval, t2BFbic)
  bfVal[[k]][length(rs)+2,] <- outer(p, Nval, t2eff)
  }
lapply(bfVal, function(x)signif(x,2))
```

Note two points:\
- Not until $n$=80 is the BIC-based Bayes Factor in much the same ballpark as the that generated by the *BayesFactor* function. For smaller values of $n$, it is overly large. The BIC statistic really is designed for use in a "large sample" context.\
- As $n$ increases, the estimated effect size to which the Bayes Factor corresponds becomes ever smaller.

Note then that `BayesFactor::ttestBF()` with the default setting of `rscale`, and the BIC-based Bayes Factor, are both using a prior whose scale is large relative to an ever smaller effect size.

## Matching the setting of `rscale` to the effect size

Observe then the result from matching the scale for the prior to the effect size. The following checks this for $p$=0.05, making at the same time a comparison with AIC-based and BIC-based relative 'preferences'.

```{r}
rs <- c(0.5,1,4,16)
pval <- 0.05
BFrs <- matrix(nrow=length(rs)+3, ncol=nlen)
dimnames(BFrs) <-
  list(c(paste0(c("rscale=",rep("       ",3)),rs,"/sqrt(n)"),
         "rscale=1/sqrt(2)","BIC-based","AIC-based"), 
       paste0(c("n=",rep("",nlen-1)),Nval))
for(j in 1:nlen){
  for(i in 1:length(rs))
     BFrs[i,j] <- t2BF(p=pval, N=Nval[j], xrs=rs[i]/sqrt(Nval[j]))
  BFrs[length(rs)+1, j] <- t2BF(p=pval, N=Nval[j], xrs=1/sqrt(2))
  BFrs[length(rs)+2, j] <- t2BFbic(p=pval, N=Nval[j])
  BFrs[length(rs)+3, j] <- t2AIC(p=pval, N=Nval[j])
}
print(setNames("p=0.05",""), quote=F)
round(BFrs,2)
```

The BIC is designed, in effect, to look for effect sizes that are around one. If a small effect size is expected in a large sample context, use of `ttestBF()` or `ttest.tstat()` with a setting of `rscale` that matches the expected effect size, makes better sense than use of `BIC()`.

There is a choice of prior that allows the AIC-based preference measure to be interpreted as a Bayes Factor. See Burnham & Anderson (2004). Relative preference values that are larger than from the *BayesFactor* functions at all settings of `rscale` suggests a tendency to choose an overly complex model.

For $p$=0.01 we find:

```{r, echo=T}
rs <- c(0.5,1,4,16)
pval <- 0.01
BFrs <- matrix(nrow=length(rs)+3, ncol=nlen)
dimnames(BFrs) <-
  list(c(paste0(c("rscale=",rep("       ",3)),rs,"/sqrt(n)"),"rscale=1/sqrt(2)","BIC-based","AIC-based"), paste0(c("n=",rep("",nlen-1)),Nval))
for(j in 1:nlen){
  for(i in 1:length(rs))
     BFrs[i,j] <- t2BF(p=pval, N=Nval[j], xrs=rs[i]/sqrt(Nval[j]))
  BFrs[length(rs)+1, j] <- t2BF(p=pval, N=Nval[j], xrs=1/sqrt(2))
  BFrs[length(rs)+2, j] <- t2BFbic(p=pval, N=Nval[j])
  BFrs[length(rs)+3, j] <- t2AIC(p=pval, N=Nval[j])
}
```

```{r, echo=T}
print(setNames("p=0.01",""), quote=F)
round(BFrs,2)
```

# AIC and BIC -- What $P$-value Corresponds to a Zero Bifference?

We ask "For what $p$-value does the statistic, in either case, rate the simpler model and the more complex model equally?"

```{r}
eqAIC2p <- function(n)2*pt(sqrt((n-1)*(exp(2/n)-1)),lower.tail=F,df=n-1)
eqBIC2p <- function(n)2*pt(sqrt((n-1)*(exp(log(n)/n)-1)),lower.tail=F,df=n-1)
eqAICc2p <- function(n){penalty <- 12/(n-3)-4/(n-2)+2;
  2*pt(sqrt((n-1)*(exp(penalty/n)-1)),lower.tail=F,df=n-1)}
eq2p <- rbind(BIC=setNames(eqBIC2p(Nval[-1]), paste(Nval[-1])), 
              AIC=eqAIC2p(Nval[-1]), AICc=eqAICc2p(Nval[-1])) 
signif(eq2p,2)
```

# Use of functions from the *BFpack* package

We investigate the Bayes Factors that are calculated using the Fractional Bayes Factor Approach. The details are not easy to describe simply. However the effect is that allowance must be made for the use of a fraction of the information in the data to determine the null. See Mulder et al (2021).

We compare

-   Bayes Factor with Jeffreys-Zellner-Siow prior centered on NULL
-   Fractional Bayes Factor from *BFpack* (`BF.type=1`), i.e., the prior is centered on the NULL
-   Fractional Bayes Factor from *BFpack* (`BF.type=2`), i.e., the prior is centered on the maximum likelihood estimate under the alternative.
-   Alternative versus NULL, based on Bayesian Information Criterion (BIC)

```{r BF}
suppressPackageStartupMessages(library(BayesFactor))
suppressPackageStartupMessages(library(BFpack))
suppressPackageStartupMessages(library(metRology))
pval <- c(.05,.01,.001); np <- length(pval)
Nval <- c(3:5,10,20,40,80,160,320); nlen <- length(Nval)
bicVal <- outer(pval, Nval, t2BFbic)
# t2BF <- function(p, N){t <- qt(p/2, df=N-1, lower.tail=FALSE)
#                       BayesFactor::ttest.tstat(t=t, n1=N, simple=TRUE)}
BFval <- packValNull <- packValAlt <- matrix(nrow=np,ncol=nlen)
dimnames(packValNull) <- dimnames(packValAlt) <- dimnames(bicVal) <- 
  dimnames(BFval) <-
  list(paste(pval), paste0(c("n=",rep("",nlen-1)),Nval))
for(i in 1:np)for(j in 1:nlen){
  t <- qt(pval[i]/2,Nval[j]-1,lower.tail=F)
  d <- rnorm(Nval[j])
  d <- d-mean(d)+t*sd(d)/sqrt(Nval[j])
  tt <- bain::t_test(d)
  packValNull[i,j] <- BF(tt,  hypothesis='mu=0',  
    BF.type=1)[['BFmatrix_confirmatory']]['complement', 'mu=0']
  packValAlt[i,j] <- BF(tt,  hypothesis='mu=0',  
    BF.type=2)[['BFmatrix_confirmatory']]['complement', 'mu=0']
  BFval[i,j] <- t2BF(pval[i], Nval[j])}
```

```{r print1, echo=T}
## Fractional Bayes factor, center on point null
print(setNames("Fractional Bayes Factor, center prior on null",""), quote=F)
print(packValNull, digits=3)
```

```{r, echo=T}
## Bayes Factor (Cauchy prior, `rscale="medium")`
print(setNames("From `BayesFactor::ttestBF()`, center prior on null",""), quote=F)
print(BFval, digits=3)
```

```{r, echo=T}
## BIC-based to BFpack::BF() ratio
print(setNames("FBF, center prior on null: Ratio to BayesFactor result",""), quote=F)
print(packValNull/BFval, digits=3)
```

## `BFpack::BF()` with BF.type=2 vs derived from BIC

```{r print2, echo=T}
# Fractional Bayes factor, center on estimate under alternative
print(setNames("FBF, center on estimate under alternative",""), quote=F)
print(packValAlt, digits=3)
```

```{r, echo=T}
## From BIC
print(setNames("Derived from BIC",""), quote=F)
print(bicVal, digits=3)
```

```{r, echo=T}
## BIC-based to BFpack::BF() ratio
print(setNames("FBF, center prior on alternative: Ratio to BIC",""), quote=F)
print(packValAlt/bicVal, digits=3)
```

The function `BFpack::BF()` is making allowance for the use of a fraction of the information in the data used to specify the prior distribution. The BIC based calculations do not make such an adjustment.

As for the use of the BIC to choose between a simpler and a more complex model, the calculated Bayes Factors are unreasonably large for small samples, while in larger samples the prior is tuned to detect effect sizes that are of similar (or larger) magnitude than the standard deviation.

@fig-compare summarizes the comparisons made

```{r, fig.width=8, fig.height=5, echo=T, out.width="100%"}
#| label: fig-compare
#| echo: false
#| fig-cap: "Results from different ways to calculate the Bayes Factor
#|   - for a result from a one-sample two-sided $t$-test where $p$=0.05"
library(lattice)
allVal <- rbind(BFval, packValNull, bicVal, packValAlt)
rownames(allVal) <- paste0(
  rep(c('BayesFactor', 'packValNull', 'BIC', 'packValAlt'), c(3,3,3,3)),
  rep(c(".05",".01",".001"), 4))
tdf <- as.data.frame(t(allVal))
tdf$n <- Nval
labs <- sort(c(2^(0:6),2^(0:6)*1.5))
xyplot(BayesFactor.05+packValNull.05+BIC.05+packValAlt.05 ~ n,
       data=tdf, type='l', auto.key=list(columns=2),
       xlab="Sample size $n$",
       ylab="Bayes Factor (Choice of 4 possibilities)",
       scales=list(x=list(at=(0:8)*40),
         y=list(log=T, at=labs, labels=paste(labs))),
 par.settings=simpleTheme(lty=c(1,1:3), lwd=2, col=rep(c('gray','black'), c(1,3))))
```

Code is:

```{r, eval=FALSE}
#| label: fig-compare
```

# Bayes Factors for regression coefficients.

We will work with data simulated from a model of the form

$$
y = a_1 x_1 + a_2 x_2 + \epsilon,
$$ where $\epsilon$ is normally distributed with mean 0 and variance $\sigma^2$.

The following function creates simulated data:
```{r}
simReg <- function(N=160, b1=1.2, b2=1.25, sd=40, num=20){
    x1 <- seq(from=1, to=min(num,N), length.out=N)
    x2 <- sample(x1)
    df <- data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(min(num,N),sd=sd))
}
```

It can suitably be used thus:
```{r}
set.seed(19)
dat <- simReg(N = 160, b1 = 1.2, b2 = 1.25, sd = 40, num = 20)
y.lm <- lm(y ~ x1+x2, data=dat)
## Check least squares fit
coef(summary(y.lm))
```

### Fit *BayesFactor* model using `lmBF()`

This function returns the Bayes Factors for individual
linear models against the intercept only model as the null.
A comparison other than against the intercept only model
require one call to `lmBF()` for each model, then using
the ratio of the two Bayes Factors to compare the models.
For obtaining multiple Bayes Factors that relate to the
one model, the function `regressionBF()` that is 
demonstrated below may be preferred.

```{r}
y.lmBF12 <- lmBF(y ~ x1+x2, data=dat, progress=FALSE)
y.lmBF1 <- lmBF(y ~ x1, data=dat, progress=FALSE)
y.lmBF2 <- lmBF(y ~ x2, data=dat, progress=FALSE)
extractBF(y.lmBF1/y.lmBF12)  
  ## `extractBF(y.lmBF1/y.lmBF12)[1,1]` gives just the Bayes Factor.
  ## `y.lmBF1/y.lmBF12` gives greater detail
```

Note that in terms such as `y.lmBF1/y.lmBF12`, the 'full' model
has to appear as the denominator.  To obtain the Bayes Factors
for `y~x1+x2` against `y~x1` (i.e. 'Omit `x2`'), which is the 
Bayes Factor for `x2` given `x1`, use the construction:
```{r}
1/extractBF(y.lmBF1/y.lmBF12)[1,1]  ## Or, `1/(y.lmBF1/y.lmBF12)` 
```

The following shows the two Bayes Factors, for `x2` given `x1`,
and for `x1` given `x2`, side by side:
```{r}
sapply(list('x2|x1'=y.lmBF1/y.lmBF12, 'x1|x2'=y.lmBF2/y.lmBF12),
       function(x)1/extractBF(x)[1,1])
```

### The function `regressionBF()` 

The function `regressionBF()` calculates Bayes Factors, either for all
model terms (specify `whichModels='all'`) or for all single term
deletions (specify `whichModels='top'`), in either case against the 
intercept only model as the null.
Again use `extractBF()` to get output in terse summary form:
```{r}
y.regBF <- regressionBF(y ~ x1+x2, data=dat, progress=FALSE)
extractBF(y.regBF)
```

Now use `regressionBF()` with the argument `whichModels='top'`.
Use `extractBF()` to omit the final line that shows 1.0 as the 
Bayes Factor for the full model against itself:
```{r}
top.regBF <- regressionBF(y ~ x1+x2, data=dat, progress=FALSE, 
  whichModels='top')
  ## Type `top.regBF` to get detailed output
extractBF(top.regBF)  ## Summary output
```

It may seem more natural to work for the inverses of the Bayes Factors
that are shown.  These can be obtained in either of the following
ways:
```{r}
1/extractBF(top.regBF)[,1]  |> round(2)

```
returned, here obtaining the Bayes Factors for `x2` given `x1`,
and for `x1` given `x2`, as against the model `y~x1+x2`.  This does
however give very large Bayes Factors when $p$-values are very small.
The very small Bayes Factors that are their inverses may be easier 
to work with.

## Dataset to dataset variation with the one data generation mechanism

With the default settings, the simulated data and statistics in the 
fitted model show, even in medium size datasets such as here with 
$n$=80, large variation from one simulation to the next:

Ten simulations give $p$-values, estimated coefficients (expected values are `b1`=1.2 and `b2`=1.5), and Bayes Factors against the model that includes both `x1` and `x2`, thus:

```{r}
multsim <- function(N=80, b1=1.25, b2=1.2, sd=4, num=20, nsim=10){
    x1 <- seq(from=1, to=min(num,N), length.out=N)/5
    x2 <- sample(x1)
stats <- matrix(nrow=8, ncol=nsim)
for(i in 1:nsim){
  n <- length(x1)
  dat <- data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(n,sd=sd))
  y.lm <- lm(y~x1+x2, data=dat)
  c1c4 <- coef(summary(y.lm))[2:3, c(1,4)]
  lm1 <- lm(y~x1, data=dat)
  lm2 <- lm(y~x2, data=dat)
  lm12 <- lm(y~x1+x2, data=dat)
stats[1:6, i] <- c(c1c4[,1], c1c4[,2], 
    1/extractBF(regressionBF(y ~ x1+x2, data=dat, whichModels='top'))$bf[2:1])
  stats[7:8,i] <- c(exp((BIC(lm2)-BIC(lm12))/2), exp((BIC(lm1)-BIC(lm12))/2))
}
rownames(stats) <- c('b1','b2','p1','p2', 'bf1','bf2', 'bf1-BIC','bf2-BIC')
stats
}
set.seed(17)
stats <- multsim()
stats |> round(3)
```
Observe that the BIC-based factors are in most cases substantially less favorable than the Bayes Factors from `regressionBF()` to the model that has both of the explanatory variables. This remains the case even for a much larger sample size. Thus, try:

```{r, eval=F}
stats <- multsim(N=360, sd=8)  # Increase sd to avoid overly small p-values
stats |> round(3)
```

## Relative preference statistics from AIC and AICc

Now create a simulated dataset, and calculate Bayes Factors for the coefficients (1) using *Bayesfactor* functions, and (2) derived from BIC statistics:

```{r}
simDat <-
function(x1=rep(1:20,4)/5, x2=sample(rep(1:20,4)/5), 
                   b1=1.2, b2=1.5, sd=8){
    n <- length(x1)
    data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(n,sd=sd))
}
library(AICcmodavg)
set.seed(31)
dat31 <- simDat()
y0.lm <- lm(y~0, data=dat31)
y.lm <- lm(y~x1+x2, data=dat31); bf12 <- lmBF(y ~ x1+x2, data=dat31)
y2.lm <- lm(y~x2, data=dat31); bf2 <- lmBF(y ~ x2, data=dat31)
y1.lm <- lm(y~x1, data=dat31); bf1 <- lmBF(y ~ x1, data=dat31)
## Regression summary
coef(summary(y.lm)) |> signif(2)
## Bayes Factors for x1 and x2, using functions from _Bayesfactor_
BF <- c(extractBF(bf12/bf2)$bf, extractBF(bf12/bf1)$bf)
## Bayes Factors for x1 and x2, derived from BIC statistics
BICbf <- c(exp((BIC(y2.lm)-BIC(y.lm))/2), exp((BIC(y1.lm)-BIC(y.lm))/2))
## Bayes Factors for x1 and x2, derived from AIC statistics
AICrp <- c(exp((AIC(y2.lm)-AIC(y.lm))/2), exp((AIC(y1.lm)-AIC(y.lm))/2))
## Bayes Factors for x1 and x2, derived from AICc statistics
AICcRP <- c(exp((AICc(y2.lm)-AICc(y.lm))/2), exp((AICc(y1.lm)-AICc(y.lm))/2)) |> round(2)
## Bayes Factors for x1 and x2, derived from BIC ratio of ratios to null model
stats <- cbind(BF=BF, BICbf=BICbf, AICrp=AICrp, AICcRP=AICcRP)
```

Relative preference statistics in favor of including both `x1` and `x2`, are:

```{r}
stats |> round(2)
```

The BIC-based statistics are substantially smaller, and the AIC and AICc-based statistics substantially larger, than those derived from calculations using `BayesFactor::lmBF()`.

## Change in Bayes Factors with varying `rscaleCont`

The following code will be used for the calculations:

```{r}
simReg <- function(N=160, b1=1.2, b2=1.25, sd=40, num=20){
    x1 <- seq(from=1, to=min(num,N), length.out=N)
    x2 <- sample(x1)
    df <- data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(min(num,N),sd=sd))
}
tuneReg <- function(p=0.01, data){
    reg <- lm(y ~ x1+x2, data=data)
    N <- nrow(data)
    tstats <- coef(summary(reg))[,'t value']
    tfix <- qt(1-p/2, df=N-3, lower.tail=F)
    y1 <- fitted(reg)+resid(reg)*tstats[2]/tfix
    y2 <- fitted(reg)+resid(reg)*tstats[3]/tfix
    cbind(dat, y1=y1, y2=y2)
}
```

```{r}
rscaleCF <- function(data=dat, rs=seq(from=0.04, to=0.16, by=0.05)){
  colnam <- as.character(substitute(rs))
  if(colnam[1]=="c")colnam <- colnam[-1] else colnam <- paste(rs)
  bfVal <- matrix(nrow=3, ncol=length(colnam))
  dimnames(bfVal) <- list("BFvsInterceptOnly"=c('x1','x2','x1+x2'), 
                          rscaleCont=colnam)
  for(i in 1:length(rs)){
  reg <- regressionBF(y1~x1+x2, data=data, rscaleCont=rs[i])
  bfVal[,i] <- extractBF(reg)$bf
  }
bfVal
}
```

```{r, results='hide'}
set.seed(53)
dat <- simReg(N=80, sd=28)
dat80 <- tuneReg(p=0.01, data=dat)
coef(summary(lm(y1~x1+x2, data=dat80)))[2:3,4, drop=F] |> round(3)
bfVal80a <- rscaleCF(data=dat80, rs=c(.22, .31, sqrt(2)/4))
bfVal80omit <- apply(bfVal80a, 2, function(x)x[3]/x[-3])
dimnames(bfVal80omit) <- list('x1+x2 vs'=c('Omit x2','Omit x1'),
  rscaleCont=dimnames(bfVal80a)[[2]])
set.seed(53)
dat <- simReg(N=160, sd=40)
dat160 <- tuneReg(p=0.01, data=dat)
bfVal160a<- rscaleCF(data=dat160, rs=c(.26,.154,sqrt(2)/4))
bfVal160omit <- apply(bfVal160a, 2, function(x)x[3]/x[-3])
dimnames(bfVal160omit) <- list('x1+x2 vs'=c('Omit x2','Omit x1'),
  rscaleCont=dimnames(bfVal160a)[[2]])
```

In the following, the first column shows the value of `rscaleCont` for the maximum Bayes Factor when `x2` is omitted (i.e., only `x1` left), and the second column when `x1` is omitted.

```{r}
bfVal80omit |> round(3)
```

For \`dat160', we see

```{r}
bfVal160omit |> round(3)
```

Notice that, in both cases, the Bayes Factors for the default setting of `rscaleCont` lie, for both variables, between that for the 'Omit x2' maximum and that for the 'Omit x1' maximum.

In this context, the smaller Bayes Factor is modestly increased, with the larger Bayes Factor slightly reduced.  The full model (`y1 ~ x1+x2`) and the models
obtained by leaving out `x2` (`x1` is retained) or `x1` (`x2` is retained),
are both affected in much the same way by changes in `rscaleConf`.

## Different statistics offer different perspectives

Bayes Factors are at best a rough measure of model preference, to be used alongside other measures of model preference. The value obtained depends both on the choice of prior on where the prior is centered, and on the choice of scale for the prior. Different choices can lead to quite different Bayes Factors.

Note also that when samples are small, different samples from the same population, if available, will give widely varying summary statistics. Refer back to @fig-pval, which showed what could be expected for $p$-values.

The comparisons could usefully be extended to consider other choices of prior.

## Note -- Finding `rscaleConf` value that makes Bayes Factor a maximum

```{r, echo=FALSE, eval=FALSE}
opt <- function(mat=bfVal80, row=1, maximum=F){
x <- as.numeric(colnames(mat))
y <- mat[row,]
interval <- range(x)
minmax <- c('minimum','maximum')[as.numeric(maximum)+1]
opt.gam <- mgcv::gam(y ~ s(x))
out <- optimize(function(x)predict(opt.gam, data.frame(x=x)), 
  interval=interval, maximum=maximum)
setNames(c(out[[minmax]], out$objective, interval[1], interval[2]),
c(minmax, "Value", "range from","range to"))
}
## Examples of use
opt(mat=bfVal80, row=1, maximum=T)
opt(mat=bfVal80, row=2, maximum=T)
```

## References

Bayarri, M. J., Berger, J. O., Forte, A., & García-Donato, G. (2012). Criteria for Bayesian model choice with application to variable selection.

Burnham, K. P., & Anderson, D. R. (2004). Multimodel inference: understanding AIC and BIC in model selection. Sociological methods & research, 33(2), 261-304.

Mulder, J., Williams, D. R., Gu, X., Tomarken, A., Böing-Messing, F., Olsson-Collentine, A., Meijerink-Bosman, M., Menke, J., van Aert, R., Fox, J.-P., Hoijtink, H., Rosseel, Y., Wagenmakers, E.-J., & van Lissa, C. (2021). BFpack: Flexible Bayes Factor Testing of Scientific Theories in R. Journal of Statistical Software, 100(18), 1–63. https://doi.org/10.18637/jss.v100.i18
