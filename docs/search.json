[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Read more about Quarto blogs here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\n\n\n\n\nJun 8, 2024\n\n\nJohn Maindonald, W John Braun, Jeffrey Andrews\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "A brief summary of what the text covers",
    "section": "",
    "text": "Chapter and appendix headings are:\n\nChapter 1: Learning from data, and tools for the task\nChapter 2: Generalizing from models\nChapter 3: Multiple linear regression\nChapter 4: Exploiting the linear model framework\nChapter 5: Generalized linear models and survival analysis\nChapter 6: Time series models\nChapter 7: Multilevel models, and repeated measures\nChapter 8: Tree-based Classification and Regression\nChapter 9: Multivariate data exploration and discrimination\nAppendix A: The R System – A Brief Overview\n\nChanges from the earlier text include:\n\nChapter 1 gives a broad overview of the questions, approaches, and tools that arise in statistical analysis. Where judged necessary, these are filled out in more detail in later chapters. Notes are included on reproducible reporting using R Markdown, and on project management.\nP-values get much more critical attention than in the earlier text. They are contrasted, in a classical hypothesis testing context, with Bayes Factors, calculated assuming a standard family of ‘uninformative’ priors used in the BayesFactor package that allows use of a numerical approximation. As the calculations do not involve simulation, it is straightforward to make comparisons with \\(p\\)-values for a range of sample sizes, effect sizes, and scale parameters for the prior.\nInformation statistics – primarily AIC, AICc, and BIC – are a further focus. Associated relative preference measures, with a role similar to that of Bayes Factors, are noted.\nThere is extended commentary on the insight that studies where a substantial number of published experimental results have been independently replicated offer on what p-values mean in practice. Selection effects that result from the use of a \\(p\\) &lt;= 0.05 criterion for publication have been a major contributor to effect size estimates that may on average be too large by a factor that may be 2.0 or more. A case is made for the publication in some form of all studies that meet minimum design and execution standards. Stricter experimental design criteria are called for, perhaps designing for \\(p \\leq 0.005\\) rather than the common \\(p \\leq 0.05\\),\nSimulation and resampling approaches get more extended use – as sources of insight, as devices for building intuition, and as mechanisms for obtaining sampling distributions when theoretical results are not available.\nAn important addition is the treatment of gene expression and other contexts where there may be hundreds or thousands of p-values.\nThe discussion on choosing models and checking model fits has been revised and extended.\nThe treatment of Generalized Additive Models has been rewritten and extended. There is new content on quantile regression with automatic choice of smoothing parameter, and on fitting monotonic increasing or decreasing response curves as specific forms of shape constrained additive response.\nThe treatment of models that allow for extra-binomial or extra-Poisson variation has been substantially extended.\nExponential time series (ETS) get greater attention, especially for their use in forecasting. Modeling of seasonal terms now gets attention.\nChanges in the lme4 package for fitting mixed-effects models, and the implementation of the Kenward-Roger approach that is now available in the afex package, have required substantial rewrites. There is a new section on “A mixed model with a betabinomial error.” The calculation of lethal time estimates and confidence intervals (primarily targeted at plant quarantine work) uses the first author’s qra (quantal response analysis) package.\nTree diagrams from tree-based regression have been finessed. There is now more attentio n to the handling of prior probabilities. The discussion introduces issues and ideas that are important for machine learning approaches more generally. The absence of coverage of machine learning methods more generally is an important omission.\nPrincipal component calculations now use the function prcomp(), which uses a singular value decomposition approach and is preferred to princomp(). A new section on “High dimensional data – RNA-Seq gene expression” demonstrates approaches now available for analysing data of this general type.\nA new section treats hierarchical and other forms of clustering.\nThe treatment of causal inference from observational data has been greatly extended, with extensive commentary on relevant R packages, and discussion of examples from the literature. Approaches to matching are a particular focus, with extensive references given. The use of directed acyclic graphs as a mechanism for making clear causal pathway assumptions is noted and references given, but not further discussed.\nThere is some limited attention to the use of multiple imputation to fill in missing values in data where some observations are incomplete, allowing use of those observations in a regression or other further analysis.\nAn appendix gives a brief overview of key features of the R system and notes technical issues that have particular relevance for users of the text.\nIn Chapter 2 and on, code is given only for those figures that are specifically targeted at the methodology under discussion. This site will be used as a first point of reference for R markdown scripts that have all the code from the book, and other supplementary materials.\nThe CRAN (Comprehensive R Archive Network) repository contains, at the time of writing, close to 20,000 packages. Further packages are available on other repositories, with Bioconductor perhaps the most important. Several others are listed upon typing setRepositories() at the command line. The 20,000 contrasts with the around 2,000 packages that were on CRAN prior to 2010 when the third edition of “Data Analysis and Graphics Using R” was in preparation. We have tried to keep up to date with new packages that supplement or extend what was available, but some will undoubtedly have been missed.\n\nReflections, looking back on the text in its published form, appear in “Afterword”."
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "A Practical Guide . . . – Code and Supplements",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe text builds on “Data Analysis and Graphics Using R” (Maindonald and Braun, CUP, 3rd edn, 2010.)↩︎"
  },
  {
    "objectID": "figFuns.html",
    "href": "figFuns.html",
    "title": "Code for Selected Figures",
    "section": "",
    "text": "options(rmarkdown.html_vignette.check_title = FALSE)\n## xtras=TRUE    ## Set to TRUE to execute code 'extras'\nxtras &lt;- FALSE\nlibrary(knitr)\n## opts_chunk[['set']](results=\"asis\")\n## opts_chunk[['set']](eval=FALSE)   ## Set to TRUE to execute main part of code\nopts_chunk[['set']](eval=FALSE)\nFigures for which code appears here may in due course be made available for execution as functions."
  },
  {
    "objectID": "figFuns.html#figure-1.20",
    "href": "figFuns.html#figure-1.20",
    "title": "Code for Selected Figures",
    "section": "Figure 1.20",
    "text": "Figure 1.20\n\neff2stat &lt;- function(eff=c(.2,.4,.8,1.2), n=c(10,40), numreps=100,\n                     FUN=function(x,N)pt(sqrt(N)*mean(x)/sd(x), df=N-1, \n                                         lower.tail=FALSE)){\n  simStat &lt;- function(eff=c(.2,.4,.8,1.2), N=10, nrep=100, FUN){\n    num &lt;- N*nrep*length(eff)\n    array(rnorm(num, mean=eff), dim=c(length(eff),nrep,N)) |&gt;\n      apply(2:1, FUN, N=N) \n  }\n  mat &lt;- matrix(nrow=numreps*length(eff),ncol=length(n))\n  for(j in 1:length(n)) mat[,j] &lt;- \n    as.vector(simStat(eff, N=n[j], numreps, FUN=FUN))  ## length(eff)*numep\n  data.frame(effsize=rep(rep(eff, each=numreps), length(n)),\n             N=rep(n, each=numreps*length(eff)), stat=as.vector(mat))\n}\n\n\nset.seed(31)\ndf200 &lt;- eff2stat(eff=c(.2,.4,.8,1.2), n=c(10, 40), numreps=200)\nlabx &lt;- c(0.001,0.01,0.05,0.2,0.4,0.8)\ngph &lt;- bwplot(factor(effsize) ~ I(stat^0.25) | factor(N), data=df200, \n              layout=c(2,1), xlab=\"P-value\", ylab=\"Effect size\", \n              scales=list(x=list(at=labx^0.25, labels =labx)))\nupdate(gph+latticeExtra::layer(panel.abline(v=labx[1:3]^0.25, col='lightgray')),\n       strip=strip.custom(factor.levels=paste0(\"n=\",c(10,40))),\n       par.settings=DAAG::DAAGtheme(color=F, col.points=\"gray50\"))"
  },
  {
    "objectID": "figFuns.html#figure-1.24",
    "href": "figFuns.html#figure-1.24",
    "title": "Code for Selected Figures",
    "section": "Figure 1.24",
    "text": "Figure 1.24\n\nt2bfInterval &lt;- function(t, n=10, rscale=\"medium\", mu=c(-.1,.1)){\n     null0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu,\n                                       rscale=rscale,simple=TRUE)\nalt0 &lt;- BayesFactor::ttest.tstat(t=t, n1=n, nullInterval=mu, rscale=rscale, \n                                 complement=TRUE, simple=TRUE)\nalt0/null0\n}\n\n\npval &lt;- c(0.05,0.01,0.001); nval &lt;- c(4,6,10,20,40,80,160)\nbfDF &lt;- expand.grid(p=pval, n=nval)\npcol &lt;- 1; ncol &lt;- 2; tcol &lt;- 3\nbfDF[,'t'] &lt;- apply(bfDF,1,function(x){qt(x[pcol]/2, df=x[ncol]-1,                                  lower.tail=FALSE)})\nother &lt;- apply(bfDF,1,function(x)\n    c(BayesFactor::ttest.tstat(t=x[tcol], n1=x[ncol], rscale=\"medium\",\n                               simple=TRUE),\n      BayesFactor::ttest.tstat(t=x[tcol], n1=x[ncol], rscale=\"wide\",\n                               simple=TRUE),\n## Now specify a null interval\n    t2bfInterval(t=x[tcol], n=x[ncol], mu=c(-0.1,0.1),rscale=\"medium\"),\n    t2bfInterval(t=x[tcol], n=x[ncol], mu=c(-0.1,0.1),rscale=\"wide\")\n  ))\nbfDF &lt;- setNames(cbind(bfDF, t(other)),\n    c('p','n','t','bf','bfInterval'))\n\nAn alternative way to do the calculations for Exercise 31 in Chapter 1 is:\n\ndoBF &lt;- function(pval=c(0.05,0.01,0.002), nval=c(10,40,160)){\nbfDF &lt;- cbind(expand.grid(p=pval, n=nval),\n              matrix(nrow=nrow(bfDF), ncol=5))\nnames(bfDF)[3:7] &lt;- c(\"t\",\"bf\",\"bfw\",\"bfInterval\",\"bfIntervalw\")\nij=0\nfor(n in nval)for(p in pval){\n  # Here, `nval` (last specified in `expand.grid()`) is placed first \nij &lt;- ij+1\nt &lt;- bfDF[ij,'t'] &lt;- qt(p/2, df=n-1, lower.tail=FALSE)\nbfDF[ij,'bf'] &lt;- t2BF(t, n, mu=0, rscale=\"medium\")\nbfDF[ij,'bfw'] &lt;- t2BF(t, n, mu=0, rscale=\"wide\")\nbfDF[ij,'bfInterval'] &lt;- t2BF(t, n, mu=c(-0.1,0.1), rscale=\"medium\")\nbfDF[ij,'bfIntervalw'] &lt;- t2BF(t, n, mu=c(-0.1,0.1),rscale=\"wide\")\n}\nbfDF\n}\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/figFuns.R\")\n}"
  },
  {
    "objectID": "afterword.html",
    "href": "afterword.html",
    "title": "Afterword",
    "section": "",
    "text": "The writing of a text that has aimed to provide a reasonably well rounded account of modern statistical methodology, albeit with very limited attention to machine learning, has been a huge challenge. Comments now follow on several areas where, more than elsewhere, our text remains a work in progress. A warning is that some technical terms will be used that assume a fair level of prior statistical understanding."
  },
  {
    "objectID": "afterword.html#inference-remains-a-hotly-contested-area.",
    "href": "afterword.html#inference-remains-a-hotly-contested-area.",
    "title": "Afterword",
    "section": "Inference remains a hotly contested area.",
    "text": "Inference remains a hotly contested area.\nWe have used Bayes Factors, calculated assuming the family of ‘uninformative’ priors used in the BayesFactor package, as a way to make a connection from the hypothesis testing framework of frequentist statistics into the Bayesian world. As the calculations use a numerical approximation that avoids the need for the extensive chain of simulations required for the Markov Chain Monte Carlo approach, it is straightforward to make comparisons with \\(p\\)-values for a range of sample sizes, effect sizes, and scale parameter for the prior.\nAs Kahneman1 argues in his book on human judgment and decision making, humans are not good intuitive statisticians. This surely applies as much or more to the choice of Bayesian priors as to the judgments that are required in more classical contexts.\nHow does the Bayes Factor change with changes in the effect size, sample size, and number of model parameters? What is the effect of varying the scale parameter for the prior distribution? What circumstances create a case for centering the prior away from the null? The start that we have made at working with the prior families used in the BayesFactor package to provide graphs that can help answer such questions could usefully be extended much further. What difference does it make if a Cauchy prior is replaced by a normal prior, with roughly matched ranges of scale factors?\nMore attention to Bayesian credible intervals would have been made sense. Arguably, these make better sense than Bayes Factors if the interest is in finding a replacement for \\(p\\)-values and associated confidence intervals.\nInformation statistics – primarily AIC, AICc, and BIC – are a further focus. Associated relative preference measures, with a role similar to that of Bayes Factors, are noted. The BIC relative preference measure can be regarded as arising from the Bayes Factor obtained when a Jeffreys Unit Information prior is used that is centered away from the null.2\nNote especially the Subsection 2.9.2 comparison between Bayes Factors and the BIC statistic, for the one-sample \\(t\\)-test case. At the largest sample sizes (\\(n\\) = 80 and \\(n\\)=160) the Bayes Factor, while always smaller than the BIC ‘relative preference’ statistic, comes close to it in value. Larger sample sizes will be required to obtain a similar rough equivalence when the comparison is between two models that have one or more explanatory variables in common.\nThese various statistics are tools, to be used with appropriate caution, and having regard to what is known about the studies that generated the data."
  },
  {
    "objectID": "afterword.html#what-can-be-learned-from-reproducibilityreplication-studies",
    "href": "afterword.html#what-can-be-learned-from-reproducibilityreplication-studies",
    "title": "Afterword",
    "section": "What can be learned from reproducibility/replication studies?",
    "text": "What can be learned from reproducibility/replication studies?\nThere is extended commentary on the insight that studies where a substantial number of published experimental results have been independently replicated offer on what p-values mean in practice. Effect sizes for the replications have mostly been found to be on average much lower than for the original experiment. A major part of the difference is no doubt caused by selection effects, from publishing mainly or only those results that fall under a \\(p\\) &lt;= 0.05 or similar criterion.\nThere is a strong case for the publication in some form of all studies that effmeet minimum design and execution standards. Stricter experimental design criteria are called for, perhaps designing for \\(p \\leq 0.005\\) rather than the common \\(p \\leq 0.05\\)."
  },
  {
    "objectID": "afterword.html#simulation-has-many-uses",
    "href": "afterword.html#simulation-has-many-uses",
    "title": "Afterword",
    "section": "Simulation has many uses",
    "text": "Simulation has many uses\nThe model that is fitted is just one of the models that might have been fitted. Simulation can be used to repeatedly generate new data from the fitted model, then refitting the model to each set of new data. Overall, the different refits give an indication of how different another model fit, from data generated in the same way as the data presented for analysis, might have been. Do indications of departures from model assumptions for diagnostic plots for the fitted model lie within the range observed in the simulations? What is the extent of variation of \\(p\\)-values or other statistics that are of interest?\nSimulation can provide important insights when experiments are planned. Thus, where two treatments will be compared, it is insightful to simulate results for one or more effect sizes that are of interest. If sample sizes are overly small, statistics from the simulations (e.g., effect sizes, \\(p\\)-values, or other statistics) will show large variation from one simulation to another. There would be merit in requiring reports of results from experimental trials to show plots of relevant statistics that were examined at the study planning stage. Experimenters should have as clear as possible an understanding, before proceeding, of the ability of the experiment to discriminate between treatments. Steps taken to obtain this understanding should be reported."
  },
  {
    "objectID": "afterword.html#the-big-wide-world-of-r",
    "href": "afterword.html#the-big-wide-world-of-r",
    "title": "Afterword",
    "section": "The Big Wide World of R",
    "text": "The Big Wide World of R\nThe CRAN (Comprehensive R Archive Network) repository contains, at the time of writing, close to 20,000 packages. The 20,000 contrasts with the around 2,000 packages that were on CRAN prior to 2010 when the third edition of “Data Analysis and Graphics Using R” was in preparation.\nFurther packages are available on other repositories, with Bioconductor perhaps the most important. Type setRepositories() at the R command line to see the names of several further repositories. We have tried to keep up to date with new packages that supplement or extend what was available in 2010, but some will undoubtedly have been missed."
  },
  {
    "objectID": "afterword.html#footnotes",
    "href": "afterword.html#footnotes",
    "title": "Afterword",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nKahneman, Daniel. Thinking, fast and slow. Macmillan, 2011.↩︎\nSee http://www.stat.washington.edu/research/reports/1999/tr347.pdf↩︎"
  },
  {
    "objectID": "corrections.html",
    "href": "corrections.html",
    "title": "Corrections and Updates",
    "section": "",
    "text": "Page 63, lines -8 to -6\nThe statement that the bound “applies for a wide class of priors . . .”with densities that tail off in much the same manner as for the normal does of course leave open the question of what is meant by tailing off “in much the same manner as for the normal.” See however Subsection 2.9.2. In practical use the bound is best treated as giving a ballpark indication of what might be a reasonable guess at a Bayes Factor when degrees of freedom are ‘small’. As degrees of freedom increase, a smaller Bayes Factor can be expected.\nIn line -6, “. . . \\(p\\)-value equal to 0” should be “. . . \\(p\\)-value equal to 0.00283”.\n\n\nPage 132, first line in Subsection 2.9.2\n“The statement”. . . applying to a wide class of priors” misses the point. Refer back to the page 63 correction.\n\n\nPage 316, Exercise 6.10\n_datasets_ should of course be datasets.\n\n\nPages 390-395 on random forests, Sections 8.4 and 8.5\nThe ranger package handles calculations with large datasets much more efficiently than randomForest, and is a preferred alternative. Thus, on an early 2015 Macbook Pro with 8MB of onboard memory, the ranger() function in the the ranger package takes around 20 seconds of elapsed time with the ggplot2::diamonds dataset (53940 rows by 10 columns), while calculations with randomForest() appear to hang.\n\n\nPage 396, Exercise 8.3\nThe second sentence refers to a non-existent Chapter 3 model fit. The following is offered as a replacement for the complete exercise:\n\n8.3. Use qqnorm() to check differences from normality in nsw74psid1::re78. What do you notice? Use tree-based regression to predict re78, and check differences from normality in the distribution of residuals.\nWhat do you notice about the tails of the distribution?\n\n\nUse the function car::powerTransform() with family='bcnPower' to search for a transformation that will bring the distribution of re78 closer to normality. Run summary on the output to get values (suitably rounded values are usually preferred) of lambda and gamma that you can then supply as arguments to car::bcnPower() to obtain transformed values tran78 of re78. Use qqnorm() with the transformed data to compare its distribution with a normal distribution. The distribution should now be much closer to normal, making the choice of splits that maximize the between-groups sum-of-squares sums of squares about the mean a more optimal procedure.\nUse tree-based regression to predict tran78, and check differences from normality in the distribution of residuals. What do you now notice about the tails of the distribution? What are the variable importance ranks i) if the tree is chosen that gives the minimum cross-validated error; ii) if the tree is chosen following the one standard error criterion? In each case, calculate the cross-validated relative error.\nDo a random forest fit to the transformed data, and compare the bootstrap error with the cross-validated error from the rpart fits."
  },
  {
    "objectID": "xtras.html",
    "href": "xtras.html",
    "title": "xtras",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "xtras.html#quarto",
    "href": "xtras.html#quarto",
    "title": "xtras",
    "section": "",
    "text": "Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see https://quarto.org.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "RunningCode.html#code-markup",
    "href": "RunningCode.html#code-markup",
    "title": "Use of Code Files",
    "section": "Code markup",
    "text": "Code markup\nThe following shows an approach to placing graphs side by side\n\nz &lt;- seq(-4,4,length=101)\nplot(z, dnorm(z), type=\"l\", ylab=\"Normal density\")\nplot(z, dt(z, df=5), type=\"l\", ylab=\"t-statistic density, 5 df\")"
  },
  {
    "objectID": "RunningCode.html#using-the-hmiscknitrset-function",
    "href": "RunningCode.html#using-the-hmiscknitrset-function",
    "title": "Use of Code Files",
    "section": "Using the Hmisc::knitrSet() function",
    "text": "Using the Hmisc::knitrSet() function\nThis function creates a setup where\n\nSpace around the graph can be adjusted by setting the chunk options bot, left, top, and rt;\nCommonly used base graphics parameters can be supplied as chunk options. These include lwd, mgp, las, tcl, axes, xpd, mfg.\nDefault settings can be supplied for a number of chunk options.\n\nSee ?Hmisc::knitrSet for details. With R markdown files, the setting fig.align=\"default\" is required. Either do not change from the default globally, or accompany fig.show=\"hold\" whenever used with fig.align=\"default\", thus:\n\nHmisc::knitrSet(basename=\"tmp\", lang='markdown', fig.path=\"figs/g\", w=6, h=6)\nz &lt;- seq(-4,4,length=101)\nplot(z, dnorm(z), type=\"l\", ylab=\"Normal density\")\nplot(z, dt(z, df=5), type=\"l\", ylab=\"t-statistic density, 5 df\")\n\n\nif(file.exists(\"/Users/johnm1/pkgs/PGRcode/inst/doc/\")){\ncode &lt;- knitr::knit_code$get()\ntxt &lt;- paste0(\"\\n## \", names(code),\"\\n\", sapply(code, paste, collapse='\\n'))\nwriteLines(txt, con=\"/Users/johnm1/pkgs/PGRcode/inst/doc/UsingCode.R\")\n}"
  },
  {
    "objectID": "boostVSbag.html",
    "href": "boostVSbag.html",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "",
    "text": "The random forest bagging approach takes bootstrap samples from the data, creates a tree for each bootstrap sample (‘bag’), and uses votes across all trees to determine predicted values. Among R packages that implement this approach, note in particular randomForest and ranger. The ranger package handles calculations much more efficiently than randomForest, and should be used in preference to randomForest with large datasets.\nThe default number of trees, both for the randomForest function randomForest() and for the ranger function ranger(), is 500. Assuming that the sample data can be treated as a random sample from the population to which results will be applied, the ‘out-of-bag’ error estimate provides an unbiased estimate of the error rate.\nBy contrast, the boosting approach that is implemented in the xgboost package starts by fitting one or perhaps a small number of trees. For each of the one or more trees, it then calculates the residuals, and fits a tree to the residuals. For details of the process by which each new tree is derived, designed to maximize the ‘gain’, see https://xgboost.readthedocs.io/en/stable/tutorials/model.html. The parameter eta, with values greater than 0 and at most 1, controls the “learning rate”. It sets a factor by which the contribution of each new tree is scaled when it is added to the current approximation. The default is eta=0.3. Smaller values allow finer control over the learning rate, and provide a way to make the model more robust to overfitting, while slowing computations.\nWhereas the defaults for random forest parameters generally do a good job, and extensive tuning is not required, xgboost parameters do typically require tuning.\nsuppressPackageStartupMessages(library(data.table))\ndata(agaricus.train, package='xgboost')\ntrain &lt;- agaricus.train\ndata(agaricus.test, package='xgboost')\ntest &lt;- agaricus.test"
  },
  {
    "objectID": "boostVSbag.html#a-random-forest-fit-to-the-data-using-ranger-functions",
    "href": "boostVSbag.html#a-random-forest-fit-to-the-data-using-ranger-functions",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "A random forest fit to the data, using ranger functions",
    "text": "A random forest fit to the data, using ranger functions\n\nlibrary(ranger)\nrf1 &lt;- ranger(y=train$label, x=as.matrix(train$data), importance='impurity')\n\n\nimp &lt;- importance(rf1)\nhist(imp)\n\n\n\n\n\n\n\n\nThe importance values are very widely spread. The number of columns (out of 126) that have some level of importance is 116. The 11 rated as having importance equal to zero either have all values the same, or (one column only) have just one value that differs from the rest.\nNow look at the predicted values. Values less than or equal to 0.5 will be treated as implying non-poisonous, with those greater than 0.5 implying poisonous of possibly poisonous:\n\npred &lt;- predict(rf1, data=test$data)$predictions\ntable(pred&gt;0.5, test$label)\n\n       \n          0   1\n  FALSE 835   0\n  TRUE    0 776\n\n\nNow look at the strength of the separation between non-poisonous and poisonous of possibly poisonous mushrooms:\n\nhist(pred, breaks=20)"
  },
  {
    "objectID": "boostVSbag.html#fit-using-xgboost-functions",
    "href": "boostVSbag.html#fit-using-xgboost-functions",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "Fit using xgboost functions",
    "text": "Fit using xgboost functions\nWe first do a simple fit, with no tuning, using the function xgboost(). (For more advanced features, including custom objective and evaluation functions and the facility for checking on performance on test data with each new round, the function xgb.test() will be required.)\n\nlibrary(xgboost)\nbst &lt;- xgboost(data = as.matrix(train$data), label = train$label,  \n               max_depth = 3, eta = 1, nrounds = 2,\n               nthread = 2, objective = \"binary:logistic\")\n\n[1] train-logloss:0.161178 \n[2] train-logloss:0.064728 \n\n## Now calculate a measure of the probability that an\n## observation belongs in the group with label=1, rather\n## than label=0.\npred &lt;- predict(bst, newdata=test$data)\ntable(pred&gt;0.5, test$label)\n\n       \n          0   1\n  FALSE 835   0\n  TRUE    0 776\n\nhist(pred, breaks=20)\n\n\n\n\n\n\n\n\nThe histogram of values of pred indicates that the great majority of observations are very clearly separated into one group rather than the other.\nOr, and preferably, if we use xgb.train(), we can do\n\ndtrain &lt;- xgb.DMatrix(train$data, label = train$label, nthread = 2)\ndtest &lt;- xgb.DMatrix(test$data, label = test$label, nthread = 2)\nwatchlist &lt;- list(eval = dtest, train = dtrain)\nparam &lt;- list(max_depth = 3, eta = 0.75, nthread = 2)\nbst &lt;- xgb.train(param, dtrain, nrounds = 4, watchlist, \n                 objective = \"binary:logistic\")\n\n[1] eval-logloss:0.232339   train-logloss:0.230181 \n[2] eval-logloss:0.110409   train-logloss:0.113869 \n[3] eval-logloss:0.054348   train-logloss:0.055521 \n[4] eval-logloss:0.029359   train-logloss:0.030794 \n\n\nWhat is unexpected here is that the root mean square error is, from the second round on, lower on the test data than on the training data, with the difference increasing with each successive round. This makes it doubtful whether the test data was genuinely a random sample from the total data. We therefore do a new random split of the total data into training and test subsets. Before proceeding, we will check for columns in the data that are constant. These are, in order to reduce the computational load, best removed:\n\nlab &lt;- c(train$label, test$label)\ndat &lt;- rbind(train$data,test$data)\n(rmcol &lt;- (1:ncol(dat))[apply(dat, 2, function(x)length(unique(x))==1)])\n\n [1]  33  35  38  57  59  88  89  97 103 104\n\ndat &lt;- dat[, -rmcol]\n\nNow create a new split into the training and test data, and hence new xgb.DMatrix objects.\n\nset.seed(67)\ntestrows &lt;- sample(1:nrow(dat), size=nrow(test$data))\nDtrain &lt;- xgb.DMatrix(dat[-testrows, ], label = lab[-testrows], \n                      nthread = 2)\nDtest &lt;- xgb.DMatrix(dat[testrows,], label = lab[testrows], \n                      nthread = 2)\nwatchlist &lt;- list(eval = Dtest, train = Dtrain)\n\n\nparam &lt;- list(max_depth = 3, eta = 0.75, nthread = 2)\nbst &lt;- xgb.train(param, Dtrain, nrounds = 60, watchlist, \n                 print_every_n = 3, objective = \"binary:logistic\")\n\n[1] eval-logloss:0.230585   train-logloss:0.230804 \n[4] eval-logloss:0.030782   train-logloss:0.026497 \n[7] eval-logloss:0.008330   train-logloss:0.005590 \n[10]    eval-logloss:0.003945   train-logloss:0.002763 \n[13]    eval-logloss:0.001929   train-logloss:0.001375 \n[16]    eval-logloss:0.001350   train-logloss:0.000942 \n[19]    eval-logloss:0.001020   train-logloss:0.000759 \n[22]    eval-logloss:0.000907   train-logloss:0.000667 \n[25]    eval-logloss:0.000814   train-logloss:0.000615 \n[28]    eval-logloss:0.000791   train-logloss:0.000563 \n[31]    eval-logloss:0.000751   train-logloss:0.000539 \n[34]    eval-logloss:0.000745   train-logloss:0.000530 \n[37]    eval-logloss:0.000726   train-logloss:0.000522 \n[40]    eval-logloss:0.000729   train-logloss:0.000515 \n[43]    eval-logloss:0.000717   train-logloss:0.000508 \n[46]    eval-logloss:0.000699   train-logloss:0.000503 \n[49]    eval-logloss:0.000697   train-logloss:0.000498 \n[52]    eval-logloss:0.000693   train-logloss:0.000493 \n[55]    eval-logloss:0.000686   train-logloss:0.000490 \n[58]    eval-logloss:0.000678   train-logloss:0.000486 \n[60]    eval-logloss:0.000679   train-logloss:0.000484 \n\n\nThus, around 58 rounds appear required, in order to minimize logloss. For purposes of distinguishing between the two classes of mushrooms, this is gross overkill. Just one round is enough to give a very clear separation. Try:\n\nbst1 &lt;- xgboost(Dtrain, nrounds = 1, eta=.75, \n                 objective = \"binary:logistic\")\n\n[1] train-logloss:0.204290 \n\nhist(predict(bst1, newdata=Dtest))\n\n\n\n\n\n\n\n\nNow look at importance measures (1) from the single round fit (bst1), and (2) from the 64 rounds fit (bst):\n\nimbst1 &lt;- xgb.importance(model=bst1)\nimbst &lt;- xgb.importance(model=bst)\n\"Importances identified\"\n\n[1] \"Importances identified\"\n\nc(\"Simgle round fit\"= dim(imbst1)[1], \"64 round fit\"= dim(imbst)[1])\n\nSimgle round fit     64 round fit \n               9               35 \n\n\nThe following plots the largest 10 importance values (the column is labeled Gain in the output) from the 64 round fit:\n\nxgb.plot.importance(imbst, top_n=10)\n\n\n\n\n\n\n\n\nJust 35 of the 116 columns have been used, with most giving only a very slight gain. Consider carefully what this means. The implication is that after accounting for effects that can be accounted for using these 35 columns, other columns add nothing extra. This happens because of the correlation structure. The first tree that is chosen sets the scene for what follows. The variety of trees that are chosen by ranger() gives an indication of how different that initial tree might be. Each new bootstrap sample simulates the taking of a new random sample from the population from which the original sample was taken.\nBy contrast, ranger() gives some level of importance to all features:\n\nlibrary(ranger)\nbag &lt;- ranger(y=lab, x=dat, importance='impurity')\nimbag &lt;- importance(bag)\nlength(imbag)\n\n[1] 116\n\nsummary(imbag)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   1.453   3.818  17.366  16.181 233.530 \n\n\nLook also at ranger predictions:\n\npred &lt;- predict(bag, data=test$data)$predictions\ntable(pred&gt;0.5, test$label)\n\n       \n          0   1\n  FALSE 835   0\n  TRUE    0 776\n\nhist(pred)\n\n\n\n\n\n\n\n\nNotice the very clear separation between values that round to 0 (not poisonous) and 1 (poisonous or possibly poisonous).\nWhat happens if we remove all the columns that were not given any level of importance in the xgboost.train() analysis, and then fit a random forest?\n\nrnam &lt;- unlist(imbst[,1])\ndatbst &lt;- dat[, rnam]\nrfSome &lt;- ranger(y=lab[-testrows], x=datbst[-testrows, ], importance='impurity')\npred &lt;- predict(rfSome, data=dat[testrows,])$predictions\ntable(pred&gt;0.5, lab[testrows])\n\n       \n          0   1\n  FALSE 840   0\n  TRUE    0 771\n\nhist(pred, breaks=20)"
  },
  {
    "objectID": "boostVSbag.html#a-more-conventional-tree-fit-using-rpartrpart",
    "href": "boostVSbag.html#a-more-conventional-tree-fit-using-rpartrpart",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "A more conventional tree – fit using rpart::rpart",
    "text": "A more conventional tree – fit using rpart::rpart\n\nlibrary(rpart)\ndatt &lt;- cbind(label=lab, as.data.frame(as.matrix(dat)))\nrp &lt;- rpart(label~., data=datt, method=\"class\", cp=0.001)\npr &lt;- predict(rp, type='vector')\ntable(pr, datt$label)\n\n   \npr     0    1\n  1 4208    0\n  2    0 3916"
  },
  {
    "objectID": "boostVSbag.html#the-diamonds-dataset-this-is-a-more-serious-challenge",
    "href": "boostVSbag.html#the-diamonds-dataset-this-is-a-more-serious-challenge",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "The diamonds dataset – this is a more serious challenge",
    "text": "The diamonds dataset – this is a more serious challenge\nFor the agaricus dataset, distinguishing the two classes of mushroom was an easy task – all three methods that were tried did an effective job. For a more realistic comparison of the methodologies, we will use the gplot2::diamonds dataset.\nThe website https://lorentzen.ch/index.php/2021/04/16/a-curious-fact-on-the-diamonds-dataset/ (Michael Mayer) points out that that more than 25% of the observations appear to be duplicates. For example, there are exactly six diamonds of 2.01 carat and a price of 16,778 USD that all have the same color, cut and clarity, with other measures showing different perspectives on the same data. Thus observe:\n\ndiamonds &lt;- ggplot2::diamonds\nid &lt;- apply(diamonds[,c(1:4,7)], 1, paste0, collapse='-')\nkeepFirst &lt;- !duplicated(id) ## all except the first\n## keepLast &lt;- rev(!duplicated(rev(id)))\ndiamondA &lt;- diamonds[keepFirst, ]       ## Retain only the first \nc(nrow(diamondA),nrow(diamondA)/4)      ## 39756, 9939\n\n[1] 39756  9939\n\n## diamondZ &lt;- diamonds[keepLast, ]     ## Retain only the last \ntable(keepFirst)/length(id)\n\nkeepFirst\n    FALSE      TRUE \n0.2629588 0.7370412"
  },
  {
    "objectID": "boostVSbag.html#keepfirst",
    "href": "boostVSbag.html#keepfirst",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "keepFirst",
    "text": "keepFirst"
  },
  {
    "objectID": "boostVSbag.html#false-true",
    "href": "boostVSbag.html#false-true",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "FALSE TRUE",
    "text": "FALSE TRUE"
  },
  {
    "objectID": "boostVSbag.html#section",
    "href": "boostVSbag.html#section",
    "title": "Boosting vs Bagging – a Comparison",
    "section": "0.2629588 0.7370412",
    "text": "0.2629588 0.7370412\nThe ranger package is an alternative to randomForest that is much more efficient for working with large datasets. Working with the dataset that retains only the first of the ‘duplicates’, one finds:\n\nset.seed(31)\nlibrary(ranger)\nY &lt;- diamondA[,\"price\", drop=T]\nsamp50pc &lt;- sample(1:nrow(diamondA), size=9939*2)\n(diamond50pc.rf &lt;- ranger(x=diamondA[samp50pc,-7], y=log(Y[samp50pc])))\n\nRanger result\n\nCall:\n ranger(x = diamondA[samp50pc, -7], y = log(Y[samp50pc])) \n\nType:                             Regression \nNumber of trees:                  500 \nSample size:                      19878 \nNumber of independent variables:  9 \nMtry:                             3 \nTarget node size:                 5 \nVariable importance mode:         none \nSplitrule:                        variance \nOOB prediction error (MSE):       0.0107198 \nR squared (OOB):                  0.9886767 \n\n## OOB prediction error (MSE):       0.0107198 \n## OOB prediction error (MSE):       0.01072289  ## Repeat  calculation\n\n\npred &lt;- predict(diamond50pc.rf,\n                data=diamondA[-samp50pc,-7])$predictions\nsum((pred-log(Y[-samp50pc]))^2)/length(pred)\n\n[1] 0.01141683\n\n\nAs expected this is very similar to the OOB mean square error.\n\nFit using xgboost::xgb.train()\nThe diamonds data includes some columns that are factors or ordered factors.\n\ndiamondA[1,]\n\n# A tibble: 1 × 10\n  carat cut   color clarity depth table price     x     y     z\n  &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.23 Ideal E     SI2      61.5    55   326  3.95  3.98  2.43\n\n\nObserve that color is an ordered factor – there is an order of preference from D (best) to J (worst). The xgboost functions xgboost() and xgb.DMatrix() require a model matrix as input, rather than a dataframe that can include factors and ordered factors in its columns. The function sparse.model.matrix() from the Matrix package can be used to create the needed model matrix. The function xgb.DMatrix() goes on to create an xgb.DMatrix object of the type needed for use of the function xgb.train().\n\nlibrary(Matrix)\nsparsem &lt;- sparse.model.matrix(price~., data=diamondA)[,-1]\n\nSpecifying price as the dependent variable ensures that the corresponding is excluded from the matrix that is created. Also, the initial column of 1’s serves no useful purpose for the tree-based calculations, and is removed.\n\nDtrain &lt;- xgb.DMatrix(as.matrix(sparsem[samp50pc, ]), \n                      label = log(Y[samp50pc]), nthread = 2)\nDtest &lt;- xgb.DMatrix(sparsem[-samp50pc,], \n                     label = log(Y[-samp50pc]), nthread = 2)\nwatchlist &lt;- list(eval = Dtest, train = Dtrain)\nparam &lt;- list(max_depth = 5, eta = 0.4, nthread = 2)\nbst &lt;- xgb.train(param, Dtrain, nrounds = 81, watchlist, \n                 print_every_n = 3)\n\n[1] eval-rmse:4.560709  train-rmse:4.563817 \n[4] eval-rmse:1.001089  train-rmse:1.000100 \n[7] eval-rmse:0.252409  train-rmse:0.248454 \n[10]    eval-rmse:0.132035  train-rmse:0.123515 \n[13]    eval-rmse:0.118572  train-rmse:0.108683 \n[16]    eval-rmse:0.115815  train-rmse:0.105641 \n[19]    eval-rmse:0.114015  train-rmse:0.102737 \n[22]    eval-rmse:0.112247  train-rmse:0.100619 \n[25]    eval-rmse:0.110841  train-rmse:0.098610 \n[28]    eval-rmse:0.109701  train-rmse:0.096937 \n[31]    eval-rmse:0.109257  train-rmse:0.096040 \n[34]    eval-rmse:0.108454  train-rmse:0.094530 \n[37]    eval-rmse:0.107567  train-rmse:0.093120 \n[40]    eval-rmse:0.106824  train-rmse:0.091683 \n[43]    eval-rmse:0.106709  train-rmse:0.091043 \n[46]    eval-rmse:0.105924  train-rmse:0.089707 \n[49]    eval-rmse:0.105779  train-rmse:0.089122 \n[52]    eval-rmse:0.105620  train-rmse:0.088774 \n[55]    eval-rmse:0.105428  train-rmse:0.088140 \n[58]    eval-rmse:0.104845  train-rmse:0.086958 \n[61]    eval-rmse:0.104801  train-rmse:0.086337 \n[64]    eval-rmse:0.104729  train-rmse:0.085900 \n[67]    eval-rmse:0.104302  train-rmse:0.084950 \n[70]    eval-rmse:0.104085  train-rmse:0.084115 \n[73]    eval-rmse:0.104079  train-rmse:0.083857 \n[76]    eval-rmse:0.103845  train-rmse:0.083048 \n[79]    eval-rmse:0.103723  train-rmse:0.082711 \n[81]    eval-rmse:0.103738  train-rmse:0.082495"
  },
  {
    "objectID": "bf.html",
    "href": "bf.html",
    "title": "Bayes Factors and Information Statistics",
    "section": "",
    "text": "Bayes Factors give a data analysis perspective that is different to that given by \\(p\\)-values, one that is in most cases closer to what the analyst would like to know. The value obtained depends, inevitably, on the choice of prior. But, what choice of prior makes good sense? Intuition, if it is to help, requires training. How do Bayes Factors with common choices of prior, stack up against the use of the information statistics BIC (Bayes Information Criterion) and AIC (Akaike Information Criterion) for use in model choice? Intuition, if it is to be useful at all, requires training. The discussion that follows may be helpful to this end.\nFor purposes of making a connection to the BIC and AIC, the focus will be on Bayes Factors as returned by functions in the BayesFactor package and, by BPpack::BF(). The BIC, in the large sample context for which it was developed, has a direct connection to a form of Bayes Factor. For the AIC, or the AICc that should replace it for use when the ratio of sample size to number of parameters estimated is less than perhaps 40, a more tenuous connection can be made. See Bayarri et al (2012) for commentary on the rational for the Jeffreys-Zellner-Siow family of priors that is used by BayesFactor functions.\nThe AIC penalty term is designed so that, in the large sample limit, the statistic will select the model with the lowest prediction error. The BIC penalty term is designed, in the large sample limit, to select the correct model. In practical use, this distinction may be somewhat artificial.\n\nAll summary statistics are random variables\n\n\n\n\n\n\n\n\nFigure 1: Boxplots are for 200 simulated \\(p\\)-values for a one-sided - one-sample \\(t\\)-test, for the specified effect sizes eff and - sample sizes n. The \\(p^{0.25}\\) scale on the \\(x\\)-axis is used - to reduce the extreme asymmetry in the distributions.\n\n\n\n\n\nNote first that all these statistics, as well as \\(p\\)-values, are random variables. The randomness is a particular issue when sample sizes are small and/or effect sizes are small. Figure 1 highlights this point. Code is:\n\n\nCode\neff2stat &lt;- function(eff=c(.2,.4,.8,1.2), n=c(40,160), numreps=200,\n                     FUN=function(x,N)pt(sqrt(N)*mean(x)/sd(x), df=N-1,\n                                         lower.tail=FALSE)){\n  simStat &lt;- function(eff=c(.2,.4,.8,1.2), N=10, nrep=200, FUN){\n    num &lt;- N*nrep*length(eff)\n    array(rnorm(num, mean=eff), dim=c(length(eff),nrep,N)) |&gt;\n      apply(2:1, FUN, N=N)\n  }\n  mat &lt;- matrix(nrow=numreps*length(eff),ncol=length(n))\n  for(j in 1:length(n)) mat[,j] &lt;-\n    as.vector(simStat(eff, N=n[j], numreps, FUN=FUN))  ## length(eff)*numep\n  data.frame(effsize=rep(rep(eff, each=numreps), length(n)),\n             N=rep(n, each=numreps*length(eff)), stat=as.vector(mat))\n}\n\n\n\n\nCode\nlibrary(lattice)\nset.seed(31)\nn &lt;- c (40,80)\ndf200 &lt;- eff2stat(eff=c(.2,.4,.8,1.2), n=n, numreps=200)\nlabx &lt;- c(0.001,0.01,0.05,0.2,0.4,0.8)\ngph &lt;- bwplot(factor(effsize) ~ I(stat^0.25) | factor(N), data=df200,\n              layout=c(2,1), xlab=\"P-value\", ylab=\"Effect size\",\n              scales=list(x=list(at=labx^0.25, labels =labx)))\nupdate(gph+latticeExtra::layer(panel.abline(v=labx[1:3]^0.25, col='lightgray')),\n       strip=strip.custom(factor.levels=paste0(\"n=\", n)),\n       par.settings=DAAG::DAAGtheme(color=F, col.points=\"gray50\"))\n\n\n\n\nThe BIC and AIC connection to Bayes Factors\nAs a starting point, comparisons will be for a one-sample \\(t\\)-statistic.\nGiven two models with the same outcome variable, with respective BIC (Bayesian Information Criterion) statistics \\(m_1\\) and \\(m_2\\), the quantity \\[\nb_{12} = exp((m_1-m_2)/2)\n\\] can be used as a relative preference statistic for \\(m_2\\) as against \\(m_1\\). If model 1 is nested in model 2 this becomes, under what is known as a Jeffreys Unit Information (JUI) prior that is centered on the maximum likelihood of the difference under the alternative, a Bayes Factor giving the probability of model 2 relative to model 1. In the case of a one-sample \\(t\\)-statistic, the BIC-derived Bayes Factor is \\[\n\\mbox{exp}(N*\\log(1+\\frac{t^2}{N-1})-\\log(N))/2),\n\\mbox{  where }N \\mbox{ is the sample size}\n\\]\nHow does this compare with Bayes Factors that are obtained with other choices of prior? Specifically, because the calculations can then be handled without Markov Chain Monte Carlo simulation, we look at results from functions in the Bayesfactor and BFpack packages.\n\nComparison with results from BayesFactor::ttestBF()\nFunctions in the BayesFactor package assume a Jeffreys-Zellner-Siow (JSZ) prior, which has a reasonable claim to be used as a default prior. Numerical quadrature is used to calculate the Bayes Factor, avoiding the need for Markov Chain Monte Carlo simulation. A Cauchy prior is assumed for the effect size, with the argument rscale giving the scale factor. The Jeffreys distribution has a similar role for the variance of the normal distributions that are assumed both under the null and under the alternative.\n\n\nCode\n# Functions that calculate Bayes Factors or relative preferences  \nt2BF &lt;- function(p=0.05, N, xrs=1/sqrt(2)){\n  t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n  BayesFactor::ttest.tstat(t=t, n1=N, rscale=xrs, simple=TRUE)}\nt2BFbic &lt;- function(p,N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n  exp((N*log(1+t^2/(N-1))-log(N))/2)}\nt2AIC &lt;- function(p,N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n  exp((N*log(1+t^2/(N-1))-2)/2)}\nt2AICc &lt;- function(p,N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n  exp((N*log(1+t^2/(N-1))-12/(N-3)+4/(N-2)-2)/2)}  ## Requires N &gt; 6\nt2eff &lt;- function(p=0.05, N)\n  eff &lt;- qt(p/2, df=N-1, lower.tail=FALSE)/sqrt(N)\n\n\n\n\nCode\npval &lt;- c(.05,.01,.001); np &lt;- length(pval)\nNval &lt;- c(3,4,5,10,20,40,80,160,360); nlen &lt;- length(Nval)\n## Bayes Factors, using BayesFactor::ttest.tstat()\nrs &lt;- c(1/sqrt(2), sqrt(2))\nbf &lt;- matrix(nrow=length(rs)+2,ncol=length(Nval))\ndimnames(bf) &lt;-\n  list(c('rscale=1/sqrt(2)', '       sqrt(2)', 'BIC','Effect size'),\n       paste0(c(\"n=\",rep(\"\",length(Nval)-1)),Nval))\nbfVal &lt;- setNames(rep(list(bf),length(pval)),\n                  paste0('p', substring(paste(pval),2)))\nfor(k in 1:length(pval)){p &lt;- pval[k]\n  for(i in 1:length(rs))for(j in 1:nlen)\n    bfVal[[k]][i,j] &lt;- t2BF(p=p, N=Nval[j], xrs=rs[i])\n  bfVal[[k]][length(rs)+1,] &lt;- outer(p, Nval, t2BFbic)\n  bfVal[[k]][length(rs)+2,] &lt;- outer(p, Nval, t2eff)\n  }\nlapply(bfVal, function(x)signif(x,2))\n\n\n$p.05\n                  n=3   4   5   10   20   40   80  160  360\nrscale=1/sqrt(2)  2.6 2.4 2.2 1.80 1.40 1.10 0.80 0.59 0.40\n       sqrt(2)    3.0 2.5 2.1 1.30 0.89 0.62 0.43 0.31 0.20\nBIC              19.0 9.6 6.6 3.00 1.80 1.20 0.79 0.55 0.36\nEffect size       2.5 1.6 1.2 0.72 0.47 0.32 0.22 0.16 0.10\n\n$p.01\n                   n=3    4    5   10   20   40  80  160  360\nrscale=1/sqrt(2)   6.4  7.0  7.0  6.2 5.10 4.10 3.1 2.30 1.60\n       sqrt(2)     9.8  9.6  8.7  5.6 3.70 2.50 1.8 1.20 0.82\nBIC              210.0 77.0 45.0 15.0 8.00 5.00 3.3 2.30 1.50\nEffect size        5.7  2.9  2.1  1.0 0.64 0.43 0.3 0.21 0.14\n\n$p.001\n                  n=3      4     5    10    20    40    80   160   360\nrscale=1/sqrt(2)   21   32.0  38.0  41.0 37.00 31.00 24.00 18.00 13.00\n       sqrt(2)     39   56.0  60.0  47.0 31.00 21.00 15.00 10.00  6.70\nBIC              6500 1600.0 750.0 180.0 77.00 44.00 28.00 19.00 12.00\nEffect size        18    6.5   3.9   1.5  0.87  0.56  0.38  0.27  0.17\n\n\nNote two points:\n- Not until \\(n\\)=80 is the BIC-based Bayes Factor in much the same ballpark as the that generated by the BayesFactor function. For smaller values of \\(n\\), it is overly large. The BIC statistic really is designed for use in a “large sample” context.\n- As \\(n\\) increases, the estimated effect size to which the Bayes Factor corresponds becomes ever smaller.\nNote then that BayesFactor::ttestBF() with the default setting of rscale, and the BIC-based Bayes Factor, are both using a prior whose scale is large relative to an ever smaller effect size.\n\n\nMatching the setting of rscale to the effect size\nObserve then the result from matching the scale for the prior to the effect size. The following checks this for \\(p\\)=0.05, making at the same time a comparison with AIC-based and BIC-based relative ‘preferences’.\n\n\nCode\nrs &lt;- c(0.5,1,4,16)\npval &lt;- 0.05\nBFrs &lt;- matrix(nrow=length(rs)+3, ncol=nlen)\ndimnames(BFrs) &lt;-\n  list(c(paste0(c(\"rscale=\",rep(\"       \",3)),rs,\"/sqrt(n)\"),\n         \"rscale=1/sqrt(2)\",\"BIC-based\",\"AIC-based\"), \n       paste0(c(\"n=\",rep(\"\",nlen-1)),Nval))\nfor(j in 1:nlen){\n  for(i in 1:length(rs))\n     BFrs[i,j] &lt;- t2BF(p=pval, N=Nval[j], xrs=rs[i]/sqrt(Nval[j]))\n  BFrs[length(rs)+1, j] &lt;- t2BF(p=pval, N=Nval[j], xrs=1/sqrt(2))\n  BFrs[length(rs)+2, j] &lt;- t2BFbic(p=pval, N=Nval[j])\n  BFrs[length(rs)+3, j] &lt;- t2AIC(p=pval, N=Nval[j])\n}\nprint(setNames(\"p=0.05\",\"\"), quote=F)\n\n\n       \np=0.05 \n\n\nCode\nround(BFrs,2)\n\n\n                     n=3    4    5   10   20   40   80  160  360\nrscale=0.5/sqrt(n)  1.84 1.77 1.71 1.59 1.53 1.50 1.48 1.48 1.47\n       1/sqrt(n)    2.38 2.19 2.06 1.81 1.70 1.65 1.62 1.61 1.60\n       4/sqrt(n)    3.02 2.28 1.92 1.41 1.22 1.15 1.11 1.10 1.09\n       16/sqrt(n)   1.48 0.91 0.70 0.46 0.39 0.36 0.35 0.34 0.34\nrscale=1/sqrt(2)    2.56 2.38 2.22 1.76 1.39 1.07 0.80 0.59 0.40\nBIC-based          18.96 9.57 6.56 3.00 1.78 1.16 0.79 0.55 0.36\nAIC-based          12.08 7.04 5.39 3.49 2.93 2.71 2.60 2.56 2.53\n\n\nThe BIC is designed, in effect, to look for effect sizes that are around one. If a small effect size is expected in a large sample context, use of ttestBF() or ttest.tstat() with a setting of rscale that matches the expected effect size, makes better sense than use of BIC().\nThere is a choice of prior that allows the AIC-based preference measure to be interpreted as a Bayes Factor. See Burnham & Anderson (2004). Relative preference values that are larger than from the BayesFactor functions at all settings of rscale suggests a tendency to choose an overly complex model.\nFor \\(p\\)=0.01 we find:\n\n\nCode\nrs &lt;- c(0.5,1,4,16)\npval &lt;- 0.01\nBFrs &lt;- matrix(nrow=length(rs)+3, ncol=nlen)\ndimnames(BFrs) &lt;-\n  list(c(paste0(c(\"rscale=\",rep(\"       \",3)),rs,\"/sqrt(n)\"),\"rscale=1/sqrt(2)\",\"BIC-based\",\"AIC-based\"), paste0(c(\"n=\",rep(\"\",nlen-1)),Nval))\nfor(j in 1:nlen){\n  for(i in 1:length(rs))\n     BFrs[i,j] &lt;- t2BF(p=pval, N=Nval[j], xrs=rs[i]/sqrt(Nval[j]))\n  BFrs[length(rs)+1, j] &lt;- t2BF(p=pval, N=Nval[j], xrs=1/sqrt(2))\n  BFrs[length(rs)+2, j] &lt;- t2BFbic(p=pval, N=Nval[j])\n  BFrs[length(rs)+3, j] &lt;- t2AIC(p=pval, N=Nval[j])\n}\n\n\n\n\nCode\nprint(setNames(\"p=0.01\",\"\"), quote=F)\n\n\n       \np=0.01 \n\n\nCode\nround(BFrs,2)\n\n\n                      n=3     4     5    10    20    40    80   160   360\nrscale=0.5/sqrt(n)   3.49  3.65  3.62  3.38  3.22  3.13  3.09  3.07  3.06\n       1/sqrt(n)     5.56  5.71  5.54  4.87  4.48  4.28  4.19  4.14  4.12\n       4/sqrt(n)    12.32 10.39  8.77  5.85  4.75  4.30  4.09  3.99  3.94\n       16/sqrt(n)   12.09  6.54  4.51  2.31  1.73  1.51  1.42  1.38  1.35\nrscale=1/sqrt(2)     6.38  7.03  7.05  6.20  5.12  4.08  3.12  2.32  1.60\nBIC-based          205.66 76.53 44.54 15.34  8.04  4.96  3.29  2.25  1.47\nAIC-based          131.05 56.31 36.64 17.84 13.23 11.54 10.81 10.47 10.29\n\n\n\n\n\nAIC and BIC – What \\(P\\)-value Corresponds to a Zero Bifference?\nWe ask “For what \\(p\\)-value does the statistic, in either case, rate the simpler model and the more complex model equally?”\n\n\nCode\neqAIC2p &lt;- function(n)2*pt(sqrt((n-1)*(exp(2/n)-1)),lower.tail=F,df=n-1)\neqBIC2p &lt;- function(n)2*pt(sqrt((n-1)*(exp(log(n)/n)-1)),lower.tail=F,df=n-1)\neqAICc2p &lt;- function(n){penalty &lt;- 12/(n-3)-4/(n-2)+2;\n  2*pt(sqrt((n-1)*(exp(penalty/n)-1)),lower.tail=F,df=n-1)}\neq2p &lt;- rbind(BIC=setNames(eqBIC2p(Nval[-1]), paste(Nval[-1])), \n              AIC=eqAIC2p(Nval[-1]), AICc=eqAICc2p(Nval[-1])) \nsignif(eq2p,2)\n\n\n          4     5    10    20   40    80   160   360\nBIC  0.3500 0.290 0.160 0.096 0.06 0.038 0.025 0.015\nAIC  0.2600 0.230 0.190 0.170 0.17 0.160 0.160 0.160\nAICc 0.0048 0.029 0.098 0.130 0.14 0.150 0.150 0.160\n\n\n\n\nUse of functions from the BFpack package\nWe investigate the Bayes Factors that are calculated using the Fractional Bayes Factor Approach. The details are not easy to describe simply. However the effect is that allowance must be made for the use of a fraction of the information in the data to determine the null. See Mulder et al (2021).\nWe compare\n\nBayes Factor with Jeffreys-Zellner-Siow prior centered on NULL\nFractional Bayes Factor from BFpack (BF.type=1), i.e., the prior is centered on the NULL\nFractional Bayes Factor from BFpack (BF.type=2), i.e., the prior is centered on the maximum likelihood estimate under the alternative.\nAlternative versus NULL, based on Bayesian Information Criterion (BIC)\n\n\n\nCode\nsuppressPackageStartupMessages(library(BayesFactor))\nsuppressPackageStartupMessages(library(BFpack))\nsuppressPackageStartupMessages(library(metRology))\npval &lt;- c(.05,.01,.001); np &lt;- length(pval)\nNval &lt;- c(3:5,10,20,40,80,160,320); nlen &lt;- length(Nval)\nbicVal &lt;- outer(pval, Nval, t2BFbic)\n# t2BF &lt;- function(p, N){t &lt;- qt(p/2, df=N-1, lower.tail=FALSE)\n#                       BayesFactor::ttest.tstat(t=t, n1=N, simple=TRUE)}\nBFval &lt;- packValNull &lt;- packValAlt &lt;- matrix(nrow=np,ncol=nlen)\ndimnames(packValNull) &lt;- dimnames(packValAlt) &lt;- dimnames(bicVal) &lt;- \n  dimnames(BFval) &lt;-\n  list(paste(pval), paste0(c(\"n=\",rep(\"\",nlen-1)),Nval))\nfor(i in 1:np)for(j in 1:nlen){\n  t &lt;- qt(pval[i]/2,Nval[j]-1,lower.tail=F)\n  d &lt;- rnorm(Nval[j])\n  d &lt;- d-mean(d)+t*sd(d)/sqrt(Nval[j])\n  tt &lt;- bain::t_test(d)\n  packValNull[i,j] &lt;- BF(tt,  hypothesis='mu=0',  \n    BF.type=1)[['BFmatrix_confirmatory']]['complement', 'mu=0']\n  packValAlt[i,j] &lt;- BF(tt,  hypothesis='mu=0',  \n    BF.type=2)[['BFmatrix_confirmatory']]['complement', 'mu=0']\n  BFval[i,j] &lt;- t2BF(pval[i], Nval[j])}\n\n\n\n\nCode\n## Fractional Bayes factor, center on point null\nprint(setNames(\"Fractional Bayes Factor, center prior on null\",\"\"), quote=F)\n\n\n                                              \nFractional Bayes Factor, center prior on null \n\n\nCode\nprint(packValNull, digits=3)\n\n\n        n=3     4     5    10    20     40     80   160   320\n0.05   2.04  2.19  2.13  1.66  1.20  0.856  0.607  0.43 0.304\n0.01   4.51  6.19  6.71  6.10  4.66  3.395  2.432  1.73 1.227\n0.001 14.24 28.34 36.64 42.93 35.65 26.849 19.520 13.98 9.951\n\n\n\n\nCode\n## Bayes Factor (Cauchy prior, `rscale=\"medium\")`\nprint(setNames(\"From `BayesFactor::ttestBF()`, center prior on null\",\"\"), quote=F)\n\n\n                                                    \nFrom `BayesFactor::ttestBF()`, center prior on null \n\n\nCode\nprint(BFval, digits=3)\n\n\n        n=3     4     5    10    20    40    80    160    320\n0.05   2.56  2.38  2.22  1.76  1.39  1.07  0.80  0.585  0.422\n0.01   6.38  7.03  7.05  6.20  5.12  4.08  3.12  2.321  1.688\n0.001 21.44 32.35 37.65 40.86 36.57 30.53 24.19 18.364 13.527\n\n\n\n\nCode\n## BIC-based to BFpack::BF() ratio\nprint(setNames(\"FBF, center prior on null: Ratio to BayesFactor result\",\"\"), quote=F)\n\n\n                                                       \nFBF, center prior on null: Ratio to BayesFactor result \n\n\nCode\nprint(packValNull/BFval, digits=3)\n\n\n        n=3     4     5    10    20    40    80   160   320\n0.05  0.797 0.919 0.958 0.939 0.864 0.800 0.759 0.734 0.721\n0.01  0.708 0.880 0.952 0.984 0.910 0.833 0.778 0.745 0.727\n0.001 0.664 0.876 0.973 1.050 0.975 0.879 0.807 0.761 0.736\n\n\n\nBFpack::BF() with BF.type=2 vs derived from BIC\n\n\nCode\n# Fractional Bayes factor, center on estimate under alternative\nprint(setNames(\"FBF, center on estimate under alternative\",\"\"), quote=F)\n\n\n                                          \nFBF, center on estimate under alternative \n\n\nCode\nprint(packValAlt, digits=3)\n\n\n         n=3       4      5    10    20     40     80    160    320\n0.05    20.9    9.57   6.22   2.6  1.48  0.946  0.638  0.441  0.308\n0.01   226.8   76.53  42.27  13.3  6.67  4.033  2.647  1.804  1.253\n0.001 7123.0 1606.11 715.79 151.9 63.95 35.565 22.407 14.973 10.295\n\n\n\n\nCode\n## From BIC\nprint(setNames(\"Derived from BIC\",\"\"), quote=F)\n\n\n                 \nDerived from BIC \n\n\nCode\nprint(bicVal, digits=3)\n\n\n       n=3       4      5    10    20    40     80   160    320\n0.05    19    9.57   6.56   3.0  1.78  1.16  0.792  0.55  0.385\n0.01   206   76.53  44.54  15.3  8.04  4.96  3.286  2.25  1.567\n0.001 6460 1606.11 754.24 175.7 77.10 43.73 27.819 18.68 12.872\n\n\n\n\nCode\n## BIC-based to BFpack::BF() ratio\nprint(setNames(\"FBF, center prior on alternative: Ratio to BIC\",\"\"), quote=F)\n\n\n                                               \nFBF, center prior on alternative: Ratio to BIC \n\n\nCode\nprint(packValAlt/bicVal, digits=3)\n\n\n      n=3 4     5    10    20    40    80   160 320\n0.05  1.1 1 0.949 0.865 0.829 0.813 0.805 0.802 0.8\n0.01  1.1 1 0.949 0.865 0.829 0.813 0.805 0.802 0.8\n0.001 1.1 1 0.949 0.865 0.829 0.813 0.805 0.802 0.8\n\n\nThe function BFpack::BF() is making allowance for the use of a fraction of the information in the data used to specify the prior distribution. The BIC based calculations do not make such an adjustment.\nAs for the use of the BIC to choose between a simpler and a more complex model, the calculated Bayes Factors are unreasonably large for small samples, while in larger samples the prior is tuned to detect effect sizes that are of similar (or larger) magnitude than the standard deviation.\nFigure 2 summarizes the comparisons made\n\n\n\n\n\n\n\n\nFigure 2: Results from different ways to calculate the Bayes Factor - for a result from a one-sample two-sided \\(t\\)-test where \\(p\\)=0.05\n\n\n\n\n\nCode is:\n\n\nCode\nlibrary(lattice)\nallVal &lt;- rbind(BFval, packValNull, bicVal, packValAlt)\nrownames(allVal) &lt;- paste0(\n  rep(c('BayesFactor', 'packValNull', 'BIC', 'packValAlt'), c(3,3,3,3)),\n  rep(c(\".05\",\".01\",\".001\"), 4))\ntdf &lt;- as.data.frame(t(allVal))\ntdf$n &lt;- Nval\nlabs &lt;- sort(c(2^(0:6),2^(0:6)*1.5))\nxyplot(BayesFactor.05+packValNull.05+BIC.05+packValAlt.05 ~ n,\n       data=tdf, type='l', auto.key=list(columns=2),\n       xlab=\"Sample size $n$\",\n       ylab=\"Bayes Factor (Choice of 4 possibilities)\",\n       scales=list(x=list(at=(0:8)*40),\n         y=list(log=T, at=labs, labels=paste(labs))),\n par.settings=simpleTheme(lty=c(1,1:3), lwd=2, col=rep(c('gray','black'), c(1,3))))\n\n\n\n\n\nBayes Factors for regression coefficients.\nWe will work with data simulated from a model of the form\n\\[\ny = a_1 x_1 + a_2 x_2 + \\epsilon,\n\\] where \\(\\epsilon\\) is normally distributed with mean 0 and variance \\(\\sigma^2\\).\nThe following function creates simulated data:\n\n\nCode\nsimReg &lt;- function(N=160, b1=1.2, b2=1.25, sd=40, num=20){\n    x1 &lt;- seq(from=1, to=min(num,N), length.out=N)\n    x2 &lt;- sample(x1)\n    df &lt;- data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(min(num,N),sd=sd))\n}\n\n\nIt can suitably be used thus:\n\n\nCode\nset.seed(19)\ndat &lt;- simReg(N = 160, b1 = 1.2, b2 = 1.25, sd = 40, num = 20)\ny.lm &lt;- lm(y ~ x1+x2, data=dat)\n## Check least squares fit\ncoef(summary(y.lm))\n\n\n             Estimate Std. Error    t value    Pr(&gt;|t|)\n(Intercept) -8.553311  9.8363109 -0.8695649 0.385865705\nx1           1.214061  0.6201538  1.9576765 0.052040933\nx2           1.863687  0.6201538  3.0052006 0.003090532\n\n\n\nFit BayesFactor model using lmBF()\nThis function returns the Bayes Factors for individual linear models against the intercept only model as the null. A comparison other than against the intercept only model require one call to lmBF() for each model, then using the ratio of the two Bayes Factors to compare the models. For obtaining multiple Bayes Factors that relate to the one model, the function regressionBF() that is demonstrated below may be preferred.\n\n\nCode\ny.lmBF12 &lt;- lmBF(y ~ x1+x2, data=dat, progress=FALSE)\ny.lmBF1 &lt;- lmBF(y ~ x1, data=dat, progress=FALSE)\ny.lmBF2 &lt;- lmBF(y ~ x2, data=dat, progress=FALSE)\nextractBF(y.lmBF1/y.lmBF12)  \n\n\n           bf        error                     time         code\nx1 0.07403776 3.405148e-05 Thu Oct 17 11:47:22 2024 d03060bea5ad\n\n\nCode\n  ## `extractBF(y.lmBF1/y.lmBF12)[1,1]` gives just the Bayes Factor.\n  ## `y.lmBF1/y.lmBF12` gives greater detail\n\n\nNote that in terms such as y.lmBF1/y.lmBF12, the ‘full’ model has to appear as the denominator. To obtain the Bayes Factors for y~x1+x2 against y~x1 (i.e. ‘Omit x2’), which is the Bayes Factor for x2 given x1, use the construction:\n\n\nCode\n1/extractBF(y.lmBF1/y.lmBF12)[1,1]  ## Or, `1/(y.lmBF1/y.lmBF12)` \n\n\n[1] 13.50662\n\n\nThe following shows the two Bayes Factors, for x2 given x1, and for x1 given x2, side by side:\n\n\nCode\nsapply(list('x2|x1'=y.lmBF1/y.lmBF12, 'x1|x2'=y.lmBF2/y.lmBF12),\n       function(x)1/extractBF(x)[1,1])\n\n\n    x2|x1     x1|x2 \n13.506622  1.286492 \n\n\n\n\nThe function regressionBF()\nThe function regressionBF() calculates Bayes Factors, either for all model terms (specify whichModels='all') or for all single term deletions (specify whichModels='top'), in either case against the intercept only model as the null. Again use extractBF() to get output in terse summary form:\n\n\nCode\ny.regBF &lt;- regressionBF(y ~ x1+x2, data=dat, progress=FALSE)\nextractBF(y.regBF)\n\n\n               bf        error                     time         code\nx1       0.896133 3.379620e-05 Thu Oct 17 11:47:22 2024 d0302fafe325\nx2       9.408323 2.142530e-05 Thu Oct 17 11:47:22 2024 d0304469aa98\nx1 + x2 12.103729 4.161742e-06 Thu Oct 17 11:47:22 2024 d030753f043e\n\n\nNow use regressionBF() with the argument whichModels='top'. Use extractBF() to omit the final line that shows 1.0 as the Bayes Factor for the full model against itself:\n\n\nCode\ntop.regBF &lt;- regressionBF(y ~ x1+x2, data=dat, progress=FALSE, \n  whichModels='top')\n  ## Type `top.regBF` to get detailed output\nextractBF(top.regBF)  ## Summary output\n\n\n           bf        error                     time         code\nx1 0.07403776 3.405148e-05 Thu Oct 17 11:47:22 2024 d0301c5d7df4\nx2 0.77730783 2.182576e-05 Thu Oct 17 11:47:22 2024 d0303df82cb8\n\n\nIt may seem more natural to work for the inverses of the Bayes Factors that are shown. These can be obtained in either of the following ways:\n\n\nCode\n1/extractBF(top.regBF)[,1]  |&gt; round(2)\n\n\n[1] 14.285714  1.282051\n\n\nreturned, here obtaining the Bayes Factors for x2 given x1, and for x1 given x2, as against the model y~x1+x2. This does however give very large Bayes Factors when \\(p\\)-values are very small. The very small Bayes Factors that are their inverses may be easier to work with.\n\n\nDataset to dataset variation with the one data generation mechanism\nWith the default settings, the simulated data and statistics in the fitted model show, even in medium size datasets such as here with \\(n\\)=80, large variation from one simulation to the next:\nTen simulations give \\(p\\)-values, estimated coefficients (expected values are b1=1.2 and b2=1.5), and Bayes Factors against the model that includes both x1 and x2, thus:\n\n\nCode\nmultsim &lt;- function(N=80, b1=1.25, b2=1.2, sd=4, num=20, nsim=10){\n    x1 &lt;- seq(from=1, to=min(num,N), length.out=N)/5\n    x2 &lt;- sample(x1)\nstats &lt;- matrix(nrow=8, ncol=nsim)\nfor(i in 1:nsim){\n  n &lt;- length(x1)\n  dat &lt;- data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(n,sd=sd))\n  y.lm &lt;- lm(y~x1+x2, data=dat)\n  c1c4 &lt;- coef(summary(y.lm))[2:3, c(1,4)]\n  lm1 &lt;- lm(y~x1, data=dat)\n  lm2 &lt;- lm(y~x2, data=dat)\n  lm12 &lt;- lm(y~x1+x2, data=dat)\nstats[1:6, i] &lt;- c(c1c4[,1], c1c4[,2], \n    1/extractBF(regressionBF(y ~ x1+x2, data=dat, whichModels='top'))$bf[2:1])\n  stats[7:8,i] &lt;- c(exp((BIC(lm2)-BIC(lm12))/2), exp((BIC(lm1)-BIC(lm12))/2))\n}\nrownames(stats) &lt;- c('b1','b2','p1','p2', 'bf1','bf2', 'bf1-BIC','bf2-BIC')\nstats\n}\nset.seed(17)\nstats &lt;- multsim()\nstats |&gt; round(3)\n\n\n          [,1]   [,2]   [,3]   [,4]    [,5]   [,6]    [,7]  [,8]   [,9]   [,10]\nb1       1.321  1.455  1.485  1.614   0.823  0.858   1.819 1.138  1.151   1.685\nb2       0.575  1.096  0.893  1.220   1.593  1.397   0.338 1.103  1.046   0.340\np1       0.001  0.000  0.001  0.001   0.043  0.067   0.000 0.005  0.004   0.000\np2       0.150  0.006  0.043  0.014   0.000  0.003   0.450 0.006  0.009   0.409\nbf1     29.247 92.644 35.210 27.357   1.473  1.162 244.210 9.637 10.350 270.895\nbf2      0.632  7.514  1.541  3.764 181.749 13.297   0.297 7.753  5.506   0.311\nbf1-BIC 24.894 95.848 31.273 23.816   0.955  0.649 285.874 7.356  7.907 321.521\nbf2-BIC  0.330  5.945  0.942  2.608 202.431 10.120   0.151 5.770  3.905   0.160\n\n\nObserve that the BIC-based factors are in most cases substantially less favorable than the Bayes Factors from regressionBF() to the model that has both of the explanatory variables. This remains the case even for a much larger sample size. Thus, try:\n\n\nCode\nstats &lt;- multsim(N=360, sd=8)  # Increase sd to avoid overly small p-values\nstats |&gt; round(3)\n\n\n\n\nRelative preference statistics from AIC and AICc\nNow create a simulated dataset, and calculate Bayes Factors for the coefficients (1) using Bayesfactor functions, and (2) derived from BIC statistics:\n\n\nCode\nsimDat &lt;-\nfunction(x1=rep(1:20,4)/5, x2=sample(rep(1:20,4)/5), \n                   b1=1.2, b2=1.5, sd=8){\n    n &lt;- length(x1)\n    data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(n,sd=sd))\n}\nlibrary(AICcmodavg)\nset.seed(31)\ndat31 &lt;- simDat()\ny0.lm &lt;- lm(y~0, data=dat31)\ny.lm &lt;- lm(y~x1+x2, data=dat31); bf12 &lt;- lmBF(y ~ x1+x2, data=dat31)\ny2.lm &lt;- lm(y~x2, data=dat31); bf2 &lt;- lmBF(y ~ x2, data=dat31)\ny1.lm &lt;- lm(y~x1, data=dat31); bf1 &lt;- lmBF(y ~ x1, data=dat31)\n## Regression summary\ncoef(summary(y.lm)) |&gt; signif(2)\n\n\n            Estimate Std. Error t value Pr(&gt;|t|)\n(Intercept)     1.20        2.2    0.55    0.590\nx1              0.99        0.7    1.40    0.160\nx2              1.80        0.7    2.50    0.014\n\n\nCode\n## Bayes Factors for x1 and x2, using functions from _Bayesfactor_\nBF &lt;- c(extractBF(bf12/bf2)$bf, extractBF(bf12/bf1)$bf)\n## Bayes Factors for x1 and x2, derived from BIC statistics\nBICbf &lt;- c(exp((BIC(y2.lm)-BIC(y.lm))/2), exp((BIC(y1.lm)-BIC(y.lm))/2))\n## Bayes Factors for x1 and x2, derived from AIC statistics\nAICrp &lt;- c(exp((AIC(y2.lm)-AIC(y.lm))/2), exp((AIC(y1.lm)-AIC(y.lm))/2))\n## Bayes Factors for x1 and x2, derived from AICc statistics\nAICcRP &lt;- c(exp((AICc(y2.lm)-AICc(y.lm))/2), exp((AICc(y1.lm)-AICc(y.lm))/2)) |&gt; round(2)\n## Bayes Factors for x1 and x2, derived from BIC ratio of ratios to null model\nstats &lt;- cbind(BF=BF, BICbf=BICbf, AICrp=AICrp, AICcRP=AICcRP)\n\n\nRelative preference statistics in favor of including both x1 and x2, are:\n\n\nCode\nstats |&gt; round(2)\n\n\n       BF BICbf AICrp AICcRP\n[1,] 0.70  0.31  1.04   0.93\n[2,] 4.45  2.65  8.73   7.83\n\n\nThe BIC-based statistics are substantially smaller, and the AIC and AICc-based statistics substantially larger, than those derived from calculations using BayesFactor::lmBF().\n\n\nChange in Bayes Factors with varying rscaleCont\nThe following code will be used for the calculations:\n\n\nCode\nsimReg &lt;- function(N=160, b1=1.2, b2=1.25, sd=40, num=20){\n    x1 &lt;- seq(from=1, to=min(num,N), length.out=N)\n    x2 &lt;- sample(x1)\n    df &lt;- data.frame(x1=x1, x2=x2, y=b1*x1+b2*x2+rnorm(min(num,N),sd=sd))\n}\ntuneReg &lt;- function(p=0.01, data){\n    reg &lt;- lm(y ~ x1+x2, data=data)\n    N &lt;- nrow(data)\n    tstats &lt;- coef(summary(reg))[,'t value']\n    tfix &lt;- qt(1-p/2, df=N-3, lower.tail=F)\n    y1 &lt;- fitted(reg)+resid(reg)*tstats[2]/tfix\n    y2 &lt;- fitted(reg)+resid(reg)*tstats[3]/tfix\n    cbind(dat, y1=y1, y2=y2)\n}\n\n\n\n\nCode\nrscaleCF &lt;- function(data=dat, rs=seq(from=0.04, to=0.16, by=0.05)){\n  colnam &lt;- as.character(substitute(rs))\n  if(colnam[1]==\"c\")colnam &lt;- colnam[-1] else colnam &lt;- paste(rs)\n  bfVal &lt;- matrix(nrow=3, ncol=length(colnam))\n  dimnames(bfVal) &lt;- list(\"BFvsInterceptOnly\"=c('x1','x2','x1+x2'), \n                          rscaleCont=colnam)\n  for(i in 1:length(rs)){\n  reg &lt;- regressionBF(y1~x1+x2, data=data, rscaleCont=rs[i])\n  bfVal[,i] &lt;- extractBF(reg)$bf\n  }\nbfVal\n}\n\n\n\n\nCode\nset.seed(53)\ndat &lt;- simReg(N=80, sd=28)\ndat80 &lt;- tuneReg(p=0.01, data=dat)\ncoef(summary(lm(y1~x1+x2, data=dat80)))[2:3,4, drop=F] |&gt; round(3)\nbfVal80a &lt;- rscaleCF(data=dat80, rs=c(.22, .31, sqrt(2)/4))\nbfVal80omit &lt;- apply(bfVal80a, 2, function(x)x[3]/x[-3])\ndimnames(bfVal80omit) &lt;- list('x1+x2 vs'=c('Omit x2','Omit x1'),\n  rscaleCont=dimnames(bfVal80a)[[2]])\nset.seed(53)\ndat &lt;- simReg(N=160, sd=40)\ndat160 &lt;- tuneReg(p=0.01, data=dat)\nbfVal160a&lt;- rscaleCF(data=dat160, rs=c(.26,.154,sqrt(2)/4))\nbfVal160omit &lt;- apply(bfVal160a, 2, function(x)x[3]/x[-3])\ndimnames(bfVal160omit) &lt;- list('x1+x2 vs'=c('Omit x2','Omit x1'),\n  rscaleCont=dimnames(bfVal160a)[[2]])\n\n\nIn the following, the first column shows the value of rscaleCont for the maximum Bayes Factor when x2 is omitted (i.e., only x1 left), and the second column when x1 is omitted.\n\n\nCode\nbfVal80omit |&gt; round(3)\n\n\n         rscaleCont\nx1+x2 vs   0.22  0.31 sqrt(2)/4\n  Omit x2 2.980 2.922     2.857\n  Omit x1 5.516 5.693     5.662\n\n\nFor `dat160’, we see\n\n\nCode\nbfVal160omit |&gt; round(3)\n\n\n         rscaleCont\nx1+x2 vs    0.26  0.154 sqrt(2)/4\n  Omit x2 24.741 22.994    24.242\n  Omit x1  5.136  5.329     4.772\n\n\nNotice that, in both cases, the Bayes Factors for the default setting of rscaleCont lie, for both variables, between that for the ‘Omit x2’ maximum and that for the ‘Omit x1’ maximum.\nIn this context, the smaller Bayes Factor is modestly increased, with the larger Bayes Factor slightly reduced. The full model (y1 ~ x1+x2) and the models obtained by leaving out x2 (x1 is retained) or x1 (x2 is retained), are both affected in much the same way by changes in rscaleConf.\n\n\nDifferent statistics offer different perspectives\nBayes Factors are at best a rough measure of model preference, to be used alongside other measures of model preference. The value obtained depends both on the choice of prior on where the prior is centered, and on the choice of scale for the prior. Different choices can lead to quite different Bayes Factors.\nNote also that when samples are small, different samples from the same population, if available, will give widely varying summary statistics. Refer back to Figure 1, which showed what could be expected for \\(p\\)-values.\nThe comparisons could usefully be extended to consider other choices of prior.\n\n\nNote – Finding rscaleConf value that makes Bayes Factor a maximum\n\n\nReferences\nBayarri, M. J., Berger, J. O., Forte, A., & García-Donato, G. (2012). Criteria for Bayesian model choice with application to variable selection.\nBurnham, K. P., & Anderson, D. R. (2004). Multimodel inference: understanding AIC and BIC in model selection. Sociological methods & research, 33(2), 261-304.\nMulder, J., Williams, D. R., Gu, X., Tomarken, A., Böing-Messing, F., Olsson-Collentine, A., Meijerink-Bosman, M., Menke, J., van Aert, R., Fox, J.-P., Hoijtink, H., Rosseel, Y., Wagenmakers, E.-J., & van Lissa, C. (2021). BFpack: Flexible Bayes Factor Testing of Scientific Theories in R. Journal of Statistical Software, 100(18), 1–63. https://doi.org/10.18637/jss.v100.i18"
  }
]